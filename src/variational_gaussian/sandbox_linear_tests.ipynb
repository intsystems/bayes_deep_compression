{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(10, 20)\n",
    "        self.linear_2 = nn.Linear(5, 10)\n",
    "\n",
    "        self.third_layer = nn.Sequential(nn.Conv1d(1, 1, 3, padding=1), nn.Linear(20, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_1(self.linear_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Sequential(nn.Conv2d(1, 1, 3, padding=1), nn.Linear(20, 10))\n",
    "len(list(a.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 0.1003, -0.3127, -0.2752],\n",
       "            [ 0.2209,  0.0025,  0.2514],\n",
       "            [-0.0948,  0.2127, -0.2904]]]], requires_grad=True)),\n",
       " ('0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0777], requires_grad=True)),\n",
       " ('1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0386,  0.1847,  0.0200,  0.0824,  0.0632, -0.1593, -0.0861, -0.0369,\n",
       "           -0.0978,  0.0437,  0.2033, -0.2067,  0.2038,  0.0943, -0.0493, -0.2016,\n",
       "           -0.0472, -0.0265,  0.1920,  0.0072],\n",
       "          [-0.0711,  0.0820, -0.1121,  0.1862, -0.1268,  0.0115, -0.1832,  0.0183,\n",
       "           -0.0252, -0.1815,  0.0781, -0.2101,  0.0484, -0.2213, -0.0135, -0.2053,\n",
       "           -0.1764,  0.0458, -0.0057, -0.0939],\n",
       "          [ 0.0474, -0.1719, -0.0569, -0.0930,  0.0250,  0.2138,  0.1263, -0.0348,\n",
       "            0.1050, -0.0655,  0.2144, -0.0643,  0.1849,  0.0166, -0.0249,  0.1120,\n",
       "            0.0393, -0.0422, -0.0755, -0.0328],\n",
       "          [-0.0033,  0.0134,  0.0326,  0.1710, -0.0171, -0.1089,  0.0342, -0.0147,\n",
       "           -0.1851, -0.0870,  0.0597,  0.0206,  0.2105, -0.0862, -0.0423, -0.2044,\n",
       "            0.1994, -0.1339, -0.2099,  0.1099],\n",
       "          [-0.0140, -0.0745,  0.1175,  0.0032,  0.1029,  0.1114, -0.0763, -0.0980,\n",
       "           -0.0451,  0.1672, -0.1995, -0.2036, -0.1367,  0.1632, -0.0890, -0.1008,\n",
       "           -0.1097,  0.0615, -0.0123, -0.0890],\n",
       "          [ 0.0172,  0.0993, -0.1892,  0.1894,  0.1473, -0.0447, -0.0138,  0.1949,\n",
       "           -0.2048, -0.0413, -0.1094, -0.0936, -0.0281, -0.0020,  0.0122, -0.1860,\n",
       "            0.1534, -0.0461, -0.1363, -0.1923],\n",
       "          [ 0.0400,  0.1808, -0.1346,  0.0028, -0.1079, -0.1922,  0.2141,  0.0430,\n",
       "            0.2220,  0.2045,  0.0614,  0.1316,  0.0389,  0.0520,  0.1352, -0.0796,\n",
       "            0.1956,  0.0320, -0.0625, -0.0862],\n",
       "          [ 0.0100, -0.1844, -0.2055, -0.0274, -0.1923,  0.1196,  0.1988, -0.0379,\n",
       "           -0.1233, -0.0339,  0.1459, -0.0465,  0.1421, -0.1783, -0.2041,  0.1870,\n",
       "           -0.2071, -0.0325, -0.2224,  0.0295],\n",
       "          [-0.1878, -0.0560, -0.1511,  0.1836,  0.2056, -0.0807,  0.1482,  0.1526,\n",
       "           -0.1755, -0.0180,  0.1003, -0.0044, -0.0556,  0.0614,  0.1648,  0.0729,\n",
       "            0.0648,  0.1232, -0.1539,  0.2149],\n",
       "          [-0.0777, -0.0710, -0.1845,  0.0837, -0.0041, -0.0396,  0.1761, -0.0021,\n",
       "            0.0041,  0.0364,  0.0989, -0.0547, -0.1942,  0.0943, -0.1507,  0.0868,\n",
       "            0.1404, -0.0454,  0.1087,  0.0681]], requires_grad=True)),\n",
       " ('1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1813, -0.2079,  0.0349, -0.0924, -0.0182, -0.2162,  0.1624, -0.1247,\n",
       "          -0.1399,  0.2011], requires_grad=True))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': Sequential(\n",
       "   (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): Linear(in_features=20, out_features=10, bias=True)\n",
       " ),\n",
       " '0': Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " '1': Linear(in_features=20, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(a.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': Linear(in_features=20, out_features=20, bias=True)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nn.Linear(20, 20)\n",
    "dict(b.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = myModule()\n",
    "params = net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-3.2434e-02, -2.1235e-01,  2.0307e-01, -3.8971e-02, -4.7705e-02,\n",
       "          2.9648e-02,  2.8886e-01,  2.2747e-01,  2.5189e-01, -1.3701e-01],\n",
       "        [-1.7861e-01, -2.7500e-01, -2.7414e-01,  1.1913e-01,  2.3626e-01,\n",
       "         -2.0756e-02, -2.1586e-01,  1.5784e-01,  2.5881e-01,  2.2508e-01],\n",
       "        [ 2.1662e-01,  1.9394e-01,  1.3046e-02, -1.6407e-01,  1.2347e-01,\n",
       "          1.8318e-01,  2.7386e-02,  2.5449e-01,  2.6245e-01, -1.0728e-01],\n",
       "        [-2.4045e-01,  3.0579e-01, -1.0416e-01, -2.2607e-01,  1.2463e-01,\n",
       "         -9.3524e-02, -5.9915e-02,  1.4448e-01,  2.6371e-01,  3.1516e-01],\n",
       "        [-2.2760e-02,  1.3410e-01,  2.9566e-01,  2.3258e-01,  8.2151e-02,\n",
       "          4.3427e-05,  2.9750e-01, -1.7836e-01,  1.1250e-01, -1.1434e-01],\n",
       "        [ 2.8166e-01,  2.3681e-02, -1.0265e-01, -6.9556e-02,  1.1485e-01,\n",
       "          8.7006e-02, -5.8378e-04,  1.8080e-02,  1.5271e-01,  1.6586e-01],\n",
       "        [ 3.0796e-01,  2.0883e-01, -2.5709e-01, -3.0728e-01,  1.1218e-01,\n",
       "          2.0578e-01,  5.6469e-02,  2.0468e-01,  1.4545e-01,  2.4671e-01],\n",
       "        [-1.7751e-01,  1.4635e-02, -1.1131e-01, -1.7638e-01,  1.2227e-01,\n",
       "         -1.7034e-01, -2.7557e-01, -7.6990e-02, -2.3179e-01,  4.5233e-03],\n",
       "        [ 3.8460e-02,  2.9501e-01,  1.8883e-01, -2.6099e-01,  2.0944e-01,\n",
       "         -6.0558e-03,  6.0424e-02, -2.0696e-01,  2.2272e-01,  1.9072e-01],\n",
       "        [ 1.5063e-01, -1.4874e-01,  1.7153e-01,  1.8977e-01,  7.7575e-02,\n",
       "          1.2019e-01,  1.3952e-01, -2.2577e-01,  9.1706e-02,  2.1416e-01],\n",
       "        [-2.2467e-02,  2.5674e-01, -1.1739e-01,  1.2073e-01,  2.7028e-01,\n",
       "          1.2009e-01,  1.9180e-01,  2.4311e-01, -1.3287e-01, -8.6226e-02],\n",
       "        [ 2.1673e-01,  7.4933e-02,  1.7082e-01, -2.3290e-01, -1.4957e-01,\n",
       "         -8.5537e-02,  1.8593e-01,  5.5720e-02, -1.8596e-01, -1.1304e-01],\n",
       "        [ 3.8607e-02,  2.1926e-01, -3.0931e-01,  4.3994e-02,  2.0106e-01,\n",
       "          9.8102e-02, -3.1579e-01,  7.2548e-02,  2.1653e-01,  1.7974e-01],\n",
       "        [ 9.3312e-02, -8.2294e-02, -3.0370e-01, -1.9287e-01,  1.8767e-01,\n",
       "          5.1423e-02, -1.7309e-01, -2.0417e-01, -3.0701e-01,  2.2394e-01],\n",
       "        [ 2.7063e-01, -2.8058e-01, -6.0948e-02,  3.1326e-01, -3.4552e-03,\n",
       "         -2.9832e-01,  2.7676e-01, -2.9031e-01, -4.8528e-02,  2.9529e-01],\n",
       "        [ 3.5308e-02, -3.5288e-02,  2.0122e-01, -1.6004e-01, -1.9712e-01,\n",
       "         -1.6385e-01, -2.8644e-01,  1.5993e-01,  6.4009e-02,  2.5110e-02],\n",
       "        [-6.9681e-02, -3.1371e-02,  2.0929e-01, -3.8388e-02,  2.1279e-01,\n",
       "          2.9584e-01, -2.6333e-01,  2.7654e-01, -3.0498e-01,  2.5055e-01],\n",
       "        [-2.8476e-01,  1.8538e-01,  7.7689e-02,  2.2647e-01,  5.1779e-02,\n",
       "          6.6782e-02, -1.8003e-01,  1.1558e-01, -1.0443e-01,  5.2843e-02],\n",
       "        [ 1.3336e-01,  1.6796e-01, -1.8312e-01, -5.3450e-02, -2.2350e-01,\n",
       "          9.4996e-02, -2.6949e-01,  1.7487e-01, -1.3452e-01,  9.9736e-02],\n",
       "        [-2.0866e-01,  3.0725e-01, -2.7483e-01,  5.0942e-02,  3.0893e-01,\n",
       "          2.4592e-01,  1.6074e-01, -2.4007e-01,  2.6522e-01,  2.1570e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'parameter name can\\'t contain \".\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# add mean and std as paramters\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m module\u001b[38;5;241m.\u001b[39mregister_parameter(\n\u001b[1;32m     55\u001b[0m     param_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     56\u001b[0m     nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrand_like(param, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m module\u001b[38;5;241m.\u001b[39mregister_parameter(\n\u001b[1;32m     59\u001b[0m     param_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     60\u001b[0m     nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrand_like(param, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# add forward pre hook to sample paramters for the module\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:571\u001b[0m, in \u001b[0;36mModule.register_parameter\u001b[0;34m(self, name, param)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter name should be a string. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter name can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter name can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be empty string \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'parameter name can\\'t contain \".\"'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from types import MethodType\n",
    "\n",
    "\n",
    "for module_name, module in net.named_modules():\n",
    "    if module_name == \"\":\n",
    "            continue\n",
    "    \n",
    "    if isinstance(module, nn.Linear):\n",
    "        named_params = dict(module.named_parameters())\n",
    "\n",
    "        for param_name, param in named_params.items():\n",
    "            # this param will be a storage for samples\n",
    "            param.requires_grad = False\n",
    "\n",
    "            # add mean and std as paramters\n",
    "            module.register_parameter(\n",
    "                param_name + \"_mean\", \n",
    "                nn.Parameter(torch.rand_like(param, requires_grad=True))\n",
    "            )\n",
    "            module.register_parameter(\n",
    "                param_name + \"_std\", \n",
    "                nn.Parameter(torch.rand_like(param, requires_grad=True))\n",
    "            )\n",
    "\n",
    "        # inital nn.Linear params are not needed\n",
    "        del module.weight\n",
    "        del module.bias\n",
    "        \n",
    "        # use local reparamterization trick for Linear layers\n",
    "        def new_forward(self, x):\n",
    "            # sample from weights posterior\n",
    "            output =  F.linear(x, self.weight_mean)\n",
    "            output += torch.randn(output.shape[-1]) * F.linear(x, self.weight_std)\n",
    "\n",
    "            # sample from bias posterior\n",
    "            output += self.bias_mean + torch.randn_like(self.bias_mean) * self.bias_std\n",
    "\n",
    "            return output\n",
    "\n",
    "        module.forward = MethodType(new_forward, module)\n",
    "\n",
    "        continue\n",
    "\n",
    "    # obtain all inital params of the submodule\n",
    "    named_params = dict(module.named_parameters())\n",
    "\n",
    "    for param_name, param in named_params.items():\n",
    "        # this param will be a storage for samples\n",
    "        param.requires_grad = False\n",
    "\n",
    "        # add mean and std as paramters\n",
    "        module.register_parameter(\n",
    "            param_name + \"_mean\", \n",
    "            nn.Parameter(torch.rand_like(param, requires_grad=True))\n",
    "        )\n",
    "        module.register_parameter(\n",
    "            param_name + \"_std\", \n",
    "            nn.Parameter(torch.rand_like(param, requires_grad=True))\n",
    "        )\n",
    "\n",
    "        # add forward pre hook to sample paramters for the module\n",
    "        def sample_param_prehook(cur_module: nn.Module, input, param_name) -> None:\n",
    "            param = cur_module.get_parameter(param_name)\n",
    "            param.copy_(\n",
    "                cur_module.get_parameter(param_name + \"_std\") * torch.randn_like(param) + \\\n",
    "                  cur_module.get_parameter(param_name + \"_mean\")\n",
    "            )\n",
    "            print(f\"Sampled for {param_name}\")\n",
    "\n",
    "        module.register_forward_pre_hook(partial(sample_param_prehook, param_name=param_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.weight_mean\n",
      "True\n",
      "linear_1.weight_std\n",
      "True\n",
      "linear_1.bias_mean\n",
      "True\n",
      "linear_1.bias_std\n",
      "True\n",
      "linear_2.weight_mean\n",
      "True\n",
      "linear_2.weight_std\n",
      "True\n",
      "linear_2.bias_mean\n",
      "True\n",
      "linear_2.bias_std\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, p in net.named_parameters():\n",
    "    print(name)\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.ones(5, 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "net._old_forward = net.forward\n",
    "\n",
    "def new_forward(self, x, num_samples: int = 1):\n",
    "    sampled_outputs = []\n",
    "    for i in range(num_samples):\n",
    "        sampled_outputs.append(self._old_forward(x))\n",
    "\n",
    "    return sampled_outputs\n",
    "\n",
    "net.forward = MethodType(new_forward, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 15.1424,   8.8401,   3.5889,  13.7657,  20.1540,  -2.6729,  23.2921,\n",
       "          15.9443,  -6.3283,  38.5617,   9.6503,  52.0065,   8.3783,  18.1263,\n",
       "          10.2334,  15.9154,  -9.4791,  -2.1770,  35.2312, -14.1118],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([ 3.5292,  9.0041, 23.1724,  2.5326, -6.5397,  8.5960,  8.2655,  4.8640,\n",
       "          7.3816, 12.2835,  1.8815,  8.0460, 11.9264,  1.7556,  5.3030, -3.9603,\n",
       "          7.2321,  9.5580, 10.9277,  6.2839], grad_fn=<AddBackward0>),\n",
       " tensor([23.8653, 22.8296, 34.2731,  0.3284,  7.2709, 11.7731, 35.7310, 25.2350,\n",
       "         21.2042, 46.7060, 17.2928, 32.7929,  4.2334,  4.2700, 41.7038,  0.9138,\n",
       "         16.6423, 30.0019, 26.2667, 39.0951], grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(torch.ones(5), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
