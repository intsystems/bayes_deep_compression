{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем путь к нашей библиотеке в переменную окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../src', '/home/sasha/BMM/bayes_deep_compression/examples', '/home/sasha/anaconda3/lib/python311.zip', '/home/sasha/anaconda3/lib/python3.11', '/home/sasha/anaconda3/lib/python3.11/lib-dynload', '', '/home/sasha/anaconda3/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем простой классификтор, который будет нашей базовой моделью, кторую мы хотим обучить и запрунить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module): \n",
    "    def __init__(self, classes: int = 10): \n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) \n",
    "        self.pool = nn.MaxPool2d(2, 2) \n",
    "        #self.dropout1 = nn.Dropout2d(0.25) \n",
    "        #self.dropout2 = nn.Dropout2d(0.5) \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128) \n",
    "        self.fc2 = nn.Linear(128, classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        x = self.pool(F.relu(self.conv1(x))) \n",
    "        #x = self.dropout1(x) \n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        #x = self.dropout2(x) \n",
    "        x = x.view(-1, 64 * 7 * 7) \n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работают распределения в нашей библиотеке(Их не обязательно импортировать для работы и обучения байесовской модели)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый вид это привычные нам распределения на числа. Импортируем тот, который используется у нас в модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayescomp.bayes.variational.distribution import LogUniformVarDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый вид это привычные нам распределения на числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayescomp.bayes.variational.net_distribution import VarBayesModuleNetDistribution #Необязательно импортировать для обучения, оно встроено в нашу байесовскую модель\n",
    "from bayescomp.bayes.base.net_distribution import BaseNetDistributionPruner #Также не обязательно для обучения, но нужен, если вы хотите запрунить модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем инициализировать веса распределения просто из параметров модели. При этом используется рекомендуемая начальная инициализация параметров распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogUniformVarDist(param_mus: torch.Size([2]), param_std_log: torch.Size([2]), scale_mus: torch.Size([2]), scale_alphas_log: torch.Size([2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.Parameter(torch.tensor([0.0, 1.0])) \n",
    "LogUniformVarDist.from_parameter(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это три модуля являются основными для байесовского обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayescomp.bayes.variational.net import LogUniformVarBayesModule, VarBayesModuleNet #Первым модулоем мы оборачиваем те слои модели, которые мы хотим сделать байесовыми, второй модуль это сама байесовская сеть\n",
    "from bayescomp.bayes.variational.optimization import LogUniformVarKLLoss #Это лосс байесовской модели, который отвечает за тип обучения. Всегда рекомендуется использовать специализированный лосс, но для большинства распределений его нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет MNIST, на котором мы хотим обучить наш классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посомтрим внимательнее как нужно создавать байесовскую сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом создадим нашу базовую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы часть слоев превратим в байесовски с помощью LogUniformVarBayesModule. И создадим список всех слоев nn.ModuleList([layer1, layer2, ...]), которые мы хотим обучить (в том чилсе слои, которые не являются байесовыми). Заметим, что можно обернуть и всю сеть целиком и передать список состоящий только из нее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes_model = BayesModule(module)\n",
    "var_module = LogUniformVarBayesModule(module)\n",
    "bayes_model = VarBayesModuleNet(module, nn.ModuleList([var_module])) #Первый аргумент базовая сеть, второй список всех слоев (где нужные из них являются байесовыми)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посомтрим на струкутру получившейся сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VarBayesModuleNet(\n",
      "  (module_list): ModuleList(\n",
      "    (0): LogUniformVarBayesModule(\n",
      "      (posterior_params): ParameterList(\n",
      "          (0): Object of type: ParameterDict\n",
      "          (1): Object of type: ParameterDict\n",
      "          (2): Object of type: ParameterDict\n",
      "          (3): Object of type: ParameterDict\n",
      "          (4): Object of type: ParameterDict\n",
      "          (5): Object of type: ParameterDict\n",
      "          (6): Object of type: ParameterDict\n",
      "          (7): Object of type: ParameterDict\n",
      "        (0): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n",
      "        )\n",
      "        (1): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 32]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 32]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 32]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        )\n",
      "        (2): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 64x32x3x3]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 64x32x3x3]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 64x32x3x3]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 64x32x3x3]\n",
      "        )\n",
      "        (3): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 64]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 64]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 64]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 64]\n",
      "        )\n",
      "        (4): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 128x3136]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 128x3136]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 128x3136]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 128x3136]\n",
      "        )\n",
      "        (5): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 128]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 128]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 128]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 128]\n",
      "        )\n",
      "        (6): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 10x128]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 10x128]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 10x128]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 10x128]\n",
      "        )\n",
      "        (7): ParameterDict(\n",
      "            (param_mus): Parameter containing: [torch.FloatTensor of size 10]\n",
      "            (param_std_log): Parameter containing: [torch.FloatTensor of size 10]\n",
      "            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 10]\n",
      "            (scale_mus): Parameter containing: [torch.FloatTensor of size 10]\n",
      "        )\n",
      "      )\n",
      "      (prior_params): ParameterList()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bayes_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У выбранной сети отсутвует prior на параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1.weight': None,\n",
       " 'conv1.bias': None,\n",
       " 'conv2.weight': None,\n",
       " 'conv2.bias': None,\n",
       " 'fc1.weight': None,\n",
       " 'fc1.bias': None,\n",
       " 'fc2.weight': None,\n",
       " 'fc2.bias': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_model.prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посомотрим как выглядит шаг обучения для сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(bayes_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом он ничем не отличается от обычного шага, нам только нужно парвильно агрегировать лоссы от нескольких семплов на одном шаге"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_dataset[10]\n",
    "y = bayes_model(torch.ones_like(image))\n",
    "kl_loss = LogUniformVarKLLoss()\n",
    "bayes_model.prior\n",
    "out = y.sum() + kl_loss(bayes_model.weights, bayes_model.posterior, bayes_model.prior)\n",
    "optimizer.zero_grad() \n",
    "out.backward() \n",
    "optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать распределение сетей можно просто из распределения на параметры и базовой сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_distributon = VarBayesModuleNetDistribution(bayes_model.base_module, bayes_model.posterior)\n",
    "#Это прунер, которые зануляет веса в зависимости от плотности распределения при 0\n",
    "net_distributon_pruner = BaseNetDistributionPruner(net_distributon)\n",
    "#Здесь мы устанавливаем средние веса модели  \n",
    "net_distributon.set_map_params()\n",
    "#Пруним на основе определенного порога\n",
    "net_distributon_pruner.prune(1.9)\n",
    "#get basic model for evaluation\n",
    "eval_model = net_distributon.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили модель с той же архитектурой что и изначальная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.3023,  0.1938,  0.2949],\n",
      "          [-0.3152, -0.1566, -0.2954],\n",
      "          [ 0.1122, -0.0164, -0.2698]]],\n",
      "\n",
      "\n",
      "        [[[-0.1391, -0.0838, -0.0929],\n",
      "          [-0.1514, -0.1413, -0.0208],\n",
      "          [-0.0616,  0.0643, -0.2400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1779, -0.0556,  0.0387],\n",
      "          [ 0.0298, -0.2650,  0.2053],\n",
      "          [ 0.1825,  0.0524,  0.0568]]],\n",
      "\n",
      "\n",
      "        [[[-0.1326,  0.0160, -0.2846],\n",
      "          [-0.2702,  0.3284,  0.0984],\n",
      "          [ 0.1598,  0.3254, -0.2004]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1090,  0.2313, -0.2271],\n",
      "          [-0.1360, -0.2276, -0.2871],\n",
      "          [-0.2184, -0.2782,  0.1676]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1886,  0.0239, -0.0954],\n",
      "          [-0.1763, -0.1735, -0.0617],\n",
      "          [-0.0442,  0.2030, -0.3038]]],\n",
      "\n",
      "\n",
      "        [[[-0.0125,  0.0237, -0.3104],\n",
      "          [ 0.2186, -0.1494, -0.2018],\n",
      "          [ 0.3172,  0.0777, -0.0151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1403,  0.2437,  0.3012],\n",
      "          [-0.2503, -0.1509,  0.2004],\n",
      "          [ 0.1842, -0.0677,  0.1355]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1765,  0.2946, -0.0750],\n",
      "          [-0.2974,  0.2443, -0.2778],\n",
      "          [ 0.1084, -0.1876, -0.2415]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2187, -0.2974, -0.2285],\n",
      "          [-0.0769, -0.0008, -0.2972],\n",
      "          [-0.0082, -0.0325,  0.3129]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3094, -0.0277, -0.1177],\n",
      "          [-0.2632, -0.3190,  0.2590],\n",
      "          [-0.2244, -0.2977, -0.0822]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2080,  0.0333, -0.0110],\n",
      "          [-0.0374, -0.0311, -0.2751],\n",
      "          [-0.1695,  0.1495, -0.0378]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1651, -0.1976, -0.3222],\n",
      "          [-0.0176, -0.2195,  0.2669],\n",
      "          [ 0.1613, -0.2372,  0.3215]]],\n",
      "\n",
      "\n",
      "        [[[-0.1514,  0.3222,  0.1876],\n",
      "          [-0.2503, -0.0947,  0.1198],\n",
      "          [-0.2005, -0.3018,  0.1469]]],\n",
      "\n",
      "\n",
      "        [[[-0.2665,  0.2325,  0.0547],\n",
      "          [ 0.3273,  0.0752,  0.3176],\n",
      "          [-0.0649, -0.0963,  0.3091]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1698, -0.2440, -0.0914],\n",
      "          [-0.1382,  0.0500, -0.2163],\n",
      "          [-0.2318, -0.2414,  0.3193]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3079, -0.1007,  0.3201],\n",
      "          [-0.0254, -0.2975, -0.0044],\n",
      "          [ 0.0028,  0.2028,  0.1971]]],\n",
      "\n",
      "\n",
      "        [[[-0.2793,  0.1843, -0.2960],\n",
      "          [-0.0477,  0.2884, -0.0363],\n",
      "          [-0.2232, -0.1665,  0.2663]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0395, -0.1153, -0.2216],\n",
      "          [ 0.1945, -0.0362,  0.0594],\n",
      "          [-0.1423,  0.0079, -0.0684]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3129, -0.1988,  0.0605],\n",
      "          [-0.2011, -0.1834,  0.0585],\n",
      "          [ 0.2515, -0.2476,  0.1238]]],\n",
      "\n",
      "\n",
      "        [[[-0.1700,  0.2340, -0.3041],\n",
      "          [-0.0441,  0.2095, -0.3244],\n",
      "          [ 0.0728,  0.2993,  0.2560]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2575,  0.1993, -0.1855],\n",
      "          [ 0.0269, -0.2285,  0.1616],\n",
      "          [ 0.2836, -0.2089,  0.2867]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1673,  0.1473,  0.3219],\n",
      "          [-0.1736, -0.0440, -0.2973],\n",
      "          [-0.0585,  0.0919,  0.0418]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0853, -0.0305,  0.0886],\n",
      "          [ 0.0217,  0.2749,  0.0004],\n",
      "          [-0.3035, -0.1323, -0.1065]]],\n",
      "\n",
      "\n",
      "        [[[-0.0347, -0.0683, -0.1135],\n",
      "          [-0.0777,  0.0344, -0.0211],\n",
      "          [ 0.0516,  0.0818,  0.0577]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0413,  0.0941,  0.0609],\n",
      "          [ 0.0132,  0.2746, -0.0029],\n",
      "          [ 0.2510, -0.2154,  0.0513]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3081,  0.0159, -0.0199],\n",
      "          [ 0.1207,  0.1839,  0.1433],\n",
      "          [-0.0966,  0.1352, -0.0973]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1429,  0.2632,  0.2769],\n",
      "          [-0.1799,  0.1762, -0.1334],\n",
      "          [ 0.1946, -0.2378, -0.0043]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2317, -0.2275,  0.1550],\n",
      "          [-0.0892, -0.2862,  0.2496],\n",
      "          [ 0.0761,  0.1411, -0.1468]]],\n",
      "\n",
      "\n",
      "        [[[-0.1346, -0.2428, -0.1063],\n",
      "          [ 0.2275, -0.2942, -0.2784],\n",
      "          [ 0.1986,  0.0480, -0.3259]]],\n",
      "\n",
      "\n",
      "        [[[-0.2209, -0.0137, -0.2504],\n",
      "          [ 0.3286, -0.2459,  0.1478],\n",
      "          [ 0.2358, -0.0675, -0.0084]]],\n",
      "\n",
      "\n",
      "        [[[-0.3254,  0.2206, -0.0185],\n",
      "          [-0.2704,  0.1132, -0.1457],\n",
      "          [-0.0577, -0.2160,  0.1405]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(eval_model.conv1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('module_list.0.posterior_params.0.param_mus',\n",
       "              tensor([[[[-0.3023,  0.1938,  0.2949],\n",
       "                        [-0.3152, -0.1566, -0.2954],\n",
       "                        [ 0.1122, -0.0164, -0.2698]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1391, -0.0838, -0.0929],\n",
       "                        [-0.1514, -0.1413, -0.0208],\n",
       "                        [-0.0616,  0.0643, -0.2400]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1779, -0.0556,  0.0387],\n",
       "                        [ 0.0298, -0.2650,  0.2053],\n",
       "                        [ 0.1825,  0.0524,  0.0568]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1326,  0.0160, -0.2846],\n",
       "                        [-0.2702,  0.3284,  0.0984],\n",
       "                        [ 0.1598,  0.3254, -0.2004]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1090,  0.2313, -0.2271],\n",
       "                        [-0.1360, -0.2276, -0.2871],\n",
       "                        [-0.2184, -0.2782,  0.1676]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1886,  0.0239, -0.0954],\n",
       "                        [-0.1763, -0.1735, -0.0617],\n",
       "                        [-0.0442,  0.2030, -0.3038]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0125,  0.0237, -0.3104],\n",
       "                        [ 0.2186, -0.1494, -0.2018],\n",
       "                        [ 0.3172,  0.0777, -0.0151]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1403,  0.2437,  0.3012],\n",
       "                        [-0.2503, -0.1509,  0.2004],\n",
       "                        [ 0.1842, -0.0677,  0.1355]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1765,  0.2946, -0.0750],\n",
       "                        [-0.2974,  0.2443, -0.2778],\n",
       "                        [ 0.1084, -0.1876, -0.2415]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2187, -0.2974, -0.2285],\n",
       "                        [-0.0769, -0.0008, -0.2972],\n",
       "                        [-0.0082, -0.0325,  0.3129]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3094, -0.0277, -0.1177],\n",
       "                        [-0.2632, -0.3190,  0.2590],\n",
       "                        [-0.2244, -0.2977, -0.0822]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2080,  0.0333, -0.0110],\n",
       "                        [-0.0374, -0.0311, -0.2751],\n",
       "                        [-0.1695,  0.1495, -0.0378]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1651, -0.1976, -0.3222],\n",
       "                        [-0.0176, -0.2195,  0.2669],\n",
       "                        [ 0.1613, -0.2372,  0.3215]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1514,  0.3222,  0.1876],\n",
       "                        [-0.2503, -0.0947,  0.1198],\n",
       "                        [-0.2005, -0.3018,  0.1469]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2665,  0.2325,  0.0547],\n",
       "                        [ 0.3273,  0.0752,  0.3176],\n",
       "                        [-0.0649, -0.0963,  0.3091]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1698, -0.2440, -0.0914],\n",
       "                        [-0.1382,  0.0500, -0.2163],\n",
       "                        [-0.2318, -0.2414,  0.3193]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3079, -0.1007,  0.3201],\n",
       "                        [-0.0254, -0.2975, -0.0044],\n",
       "                        [ 0.0028,  0.2028,  0.1971]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2793,  0.1843, -0.2960],\n",
       "                        [-0.0477,  0.2884, -0.0363],\n",
       "                        [-0.2232, -0.1665,  0.2663]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0395, -0.1153, -0.2216],\n",
       "                        [ 0.1945, -0.0362,  0.0594],\n",
       "                        [-0.1423,  0.0079, -0.0684]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3129, -0.1988,  0.0605],\n",
       "                        [-0.2011, -0.1834,  0.0585],\n",
       "                        [ 0.2515, -0.2476,  0.1238]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1700,  0.2340, -0.3041],\n",
       "                        [-0.0441,  0.2095, -0.3244],\n",
       "                        [ 0.0728,  0.2993,  0.2560]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2575,  0.1993, -0.1855],\n",
       "                        [ 0.0269, -0.2285,  0.1616],\n",
       "                        [ 0.2836, -0.2089,  0.2867]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1673,  0.1473,  0.3219],\n",
       "                        [-0.1736, -0.0440, -0.2973],\n",
       "                        [-0.0585,  0.0919,  0.0418]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0853, -0.0305,  0.0886],\n",
       "                        [ 0.0217,  0.2749,  0.0004],\n",
       "                        [-0.3035, -0.1323, -0.1065]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0347, -0.0683, -0.1135],\n",
       "                        [-0.0777,  0.0344, -0.0211],\n",
       "                        [ 0.0516,  0.0818,  0.0577]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0413,  0.0941,  0.0609],\n",
       "                        [ 0.0132,  0.2746, -0.0029],\n",
       "                        [ 0.2510, -0.2154,  0.0513]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3081,  0.0159, -0.0199],\n",
       "                        [ 0.1207,  0.1839,  0.1433],\n",
       "                        [-0.0966,  0.1352, -0.0973]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1429,  0.2632,  0.2769],\n",
       "                        [-0.1799,  0.1762, -0.1334],\n",
       "                        [ 0.1946, -0.2378, -0.0043]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2317, -0.2275,  0.1550],\n",
       "                        [-0.0892, -0.2862,  0.2496],\n",
       "                        [ 0.0761,  0.1411, -0.1468]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1346, -0.2428, -0.1063],\n",
       "                        [ 0.2275, -0.2942, -0.2784],\n",
       "                        [ 0.1986,  0.0480, -0.3259]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2209, -0.0137, -0.2504],\n",
       "                        [ 0.3286, -0.2459,  0.1478],\n",
       "                        [ 0.2358, -0.0675, -0.0084]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3254,  0.2206, -0.0185],\n",
       "                        [-0.2704,  0.1132, -0.1457],\n",
       "                        [-0.0577, -0.2160,  0.1405]]]])),\n",
       "             ('module_list.0.posterior_params.0.param_std_log',\n",
       "              tensor([[[[-4.6680, -5.1557, -6.0307],\n",
       "                        [-6.0358, -5.2458, -6.0857],\n",
       "                        [-5.3708, -6.9161, -5.1563]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.9296, -4.9510, -5.7666],\n",
       "                        [-6.9020, -5.3799, -4.8584],\n",
       "                        [-4.6198, -5.2275, -5.1176]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.6131, -5.8590, -5.4937],\n",
       "                        [-5.2921, -4.8411, -6.8054],\n",
       "                        [-5.1439, -5.2935, -4.6284]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8133, -4.8127, -4.9627],\n",
       "                        [-5.5984, -9.3892, -5.2002],\n",
       "                        [-8.1692, -5.0464, -5.0708]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9559, -4.9848, -5.2454],\n",
       "                        [-5.7516, -4.6670, -6.3891],\n",
       "                        [-4.9860, -5.7274, -5.0568]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9078, -4.8653, -4.6386],\n",
       "                        [-5.0833, -6.6092, -4.9537],\n",
       "                        [-8.6647, -5.8943, -5.8185]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.4234, -5.1696, -4.9497],\n",
       "                        [-5.3947, -5.9410, -5.1327],\n",
       "                        [-5.3328, -4.7214, -5.3860]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9772, -5.6437, -4.9258],\n",
       "                        [-4.8987, -4.7845, -5.1065],\n",
       "                        [-4.9554, -6.1154, -5.5085]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.5214, -5.1240, -4.6411],\n",
       "                        [-5.7218, -5.9526, -4.6987],\n",
       "                        [-4.6607, -5.3617, -6.0647]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3903, -4.9803, -4.7315],\n",
       "                        [-4.9417, -6.0390, -6.4542],\n",
       "                        [-6.8640, -4.6279, -5.2244]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.0609, -5.9081, -6.3559],\n",
       "                        [-4.9601, -5.8794, -5.6299],\n",
       "                        [-4.6907, -9.5356, -8.3889]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6243, -5.1204, -5.1603],\n",
       "                        [-5.3724, -5.2758, -5.0998],\n",
       "                        [-5.7654, -4.9847, -5.3205]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8268, -4.8260, -4.8077],\n",
       "                        [-4.7481, -5.9000, -6.2202],\n",
       "                        [-5.0941, -5.7782, -4.7529]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3226, -5.8882, -5.5868],\n",
       "                        [-5.1159, -6.1103, -4.8617],\n",
       "                        [-4.6260, -4.8302, -5.9241]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.4105, -4.7888, -5.0311],\n",
       "                        [-4.6302, -4.9047, -4.8874],\n",
       "                        [-5.3685, -4.6996, -7.0703]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7926, -5.8345, -5.6501],\n",
       "                        [-6.9654, -4.8340, -4.8692],\n",
       "                        [-5.1392, -4.6182, -4.7991]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7197, -4.7233, -5.3200],\n",
       "                        [-5.0657, -4.9606, -4.8746],\n",
       "                        [-5.1651, -5.4029, -5.0889]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.5693, -6.5850, -5.4891],\n",
       "                        [-6.8028, -7.4583, -6.3999],\n",
       "                        [-6.0847, -6.3965, -5.0318]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8923, -5.4236, -4.6524],\n",
       "                        [-5.4130, -5.2493, -5.7011],\n",
       "                        [-7.1075, -5.2891, -5.8058]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6538, -5.0068, -4.9454],\n",
       "                        [-5.0758, -5.6394, -5.3486],\n",
       "                        [-5.4596, -4.8116, -5.7854]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.0674, -4.7750, -4.9050],\n",
       "                        [-4.8290, -5.6911, -4.8679],\n",
       "                        [-4.9580, -5.2581, -4.7198]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8233, -6.2829, -5.4242],\n",
       "                        [-5.1367, -5.3996, -9.4015],\n",
       "                        [-4.8959, -4.6567, -4.7426]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.5696, -4.9665, -5.2467],\n",
       "                        [-5.8549, -4.9403, -6.5182],\n",
       "                        [-4.8915, -5.7343, -4.9790]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.0563, -7.4616, -5.9705],\n",
       "                        [-6.1223, -6.3774, -4.8091],\n",
       "                        [-6.2487, -5.0668, -5.1992]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.0294, -5.1717, -5.5512],\n",
       "                        [-5.2533, -4.8474, -6.0723],\n",
       "                        [-6.2747, -7.8376, -5.1174]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.9490, -5.2561, -5.0005],\n",
       "                        [-4.7649, -4.8641, -6.0398],\n",
       "                        [-5.2649, -4.6518, -4.7146]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8989, -5.7826, -5.4059],\n",
       "                        [-4.6713, -4.8062, -4.9156],\n",
       "                        [-4.6983, -4.9334, -5.7193]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0745, -6.0736, -5.2308],\n",
       "                        [-4.8546, -7.8321, -5.5927],\n",
       "                        [-6.7935, -4.6629, -5.4431]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7323, -6.0946, -5.8949],\n",
       "                        [-5.7073, -4.9492, -5.1514],\n",
       "                        [-6.1852, -5.4509, -4.6912]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2558, -4.6829, -5.7766],\n",
       "                        [-5.5031, -5.2102, -4.9500],\n",
       "                        [-5.0723, -6.4243, -4.6603]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8938, -6.6205, -4.9408],\n",
       "                        [-5.4625, -5.6948, -6.4895],\n",
       "                        [-6.7140, -6.2220, -6.2805]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8992, -5.6239, -5.7653],\n",
       "                        [-6.4117, -7.6336, -4.7730],\n",
       "                        [-4.9746, -5.1200, -6.5965]]]])),\n",
       "             ('module_list.0.posterior_params.0.scale_alphas_log',\n",
       "              tensor([[[[-3.4187, -3.7174, -2.3787],\n",
       "                        [-3.1351, -2.0745, -3.1980],\n",
       "                        [-3.0965, -3.0610, -2.9289]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5981, -3.9094, -3.8932],\n",
       "                        [-3.4802, -2.0848, -3.2224],\n",
       "                        [-2.1787, -3.0111, -2.1057]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4115, -3.3520, -2.5023],\n",
       "                        [-3.7368, -2.4095, -2.3616],\n",
       "                        [-2.9947, -2.2271, -2.5346]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0616, -2.8478, -2.1778],\n",
       "                        [-3.9550, -2.9207, -3.9338],\n",
       "                        [-2.2708, -2.9209, -2.4073]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2965, -3.7123, -3.0655],\n",
       "                        [-3.9480, -3.2759, -2.2590],\n",
       "                        [-2.3225, -3.5374, -3.7885]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9564, -2.0479, -3.4835],\n",
       "                        [-2.3118, -3.4397, -3.4326],\n",
       "                        [-2.5730, -3.1376, -2.6275]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.8859, -3.7732, -3.2062],\n",
       "                        [-2.8123, -3.8245, -3.3170],\n",
       "                        [-3.7474, -2.5673, -3.7350]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5144, -2.6990, -2.1187],\n",
       "                        [-3.8491, -2.8508, -3.0711],\n",
       "                        [-3.1067, -2.3642, -3.2846]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1253, -3.1962, -2.9400],\n",
       "                        [-3.9650, -2.0116, -3.6088],\n",
       "                        [-2.2610, -2.7882, -3.8300]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.6164, -2.7085, -2.8279],\n",
       "                        [-3.7149, -2.2112, -2.0330],\n",
       "                        [-3.1096, -3.4193, -3.2399]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1994, -3.5047, -3.9785],\n",
       "                        [-3.2239, -2.4354, -3.9956],\n",
       "                        [-3.4047, -3.9503, -2.3403]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5230, -3.0380, -2.2353],\n",
       "                        [-3.8027, -3.5320, -2.3908],\n",
       "                        [-2.1524, -3.4229, -2.5114]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3817, -2.2192, -3.8434],\n",
       "                        [-2.0427, -3.9225, -3.7810],\n",
       "                        [-2.2880, -2.7608, -2.9135]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5377, -3.9959, -2.9811],\n",
       "                        [-2.4883, -2.9125, -3.7554],\n",
       "                        [-3.9574, -2.1002, -2.1013]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3543, -2.2591, -3.6698],\n",
       "                        [-2.0830, -2.2600, -2.6263],\n",
       "                        [-2.7337, -3.9125, -2.7787]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5153, -3.4171, -2.7908],\n",
       "                        [-2.2856, -3.0273, -3.0652],\n",
       "                        [-3.4268, -3.4835, -3.0656]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9411, -2.1074, -2.5292],\n",
       "                        [-2.5760, -2.5730, -3.5708],\n",
       "                        [-2.7757, -3.6720, -3.7576]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1314, -2.0353, -2.8427],\n",
       "                        [-3.5730, -2.2904, -2.0232],\n",
       "                        [-3.8730, -2.2141, -3.1832]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5362, -2.6517, -3.4785],\n",
       "                        [-3.9051, -2.9574, -2.9500],\n",
       "                        [-2.8921, -2.2102, -2.6145]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6049, -3.7647, -2.4317],\n",
       "                        [-3.1637, -2.9858, -3.9307],\n",
       "                        [-2.2849, -3.3250, -2.2449]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6705, -3.8340, -2.3346],\n",
       "                        [-3.0310, -3.2411, -3.2318],\n",
       "                        [-3.9612, -2.0131, -3.2670]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4929, -3.1258, -3.8689],\n",
       "                        [-2.0020, -2.7222, -3.9923],\n",
       "                        [-3.5161, -3.5695, -2.0698]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1875, -2.4105, -2.1121],\n",
       "                        [-3.6588, -2.5250, -3.4611],\n",
       "                        [-2.0430, -2.0281, -2.4621]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4211, -2.9043, -3.4623],\n",
       "                        [-3.7632, -3.9642, -2.7183],\n",
       "                        [-2.0206, -2.5211, -3.7920]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7099, -3.1718, -3.9841],\n",
       "                        [-3.4555, -2.1799, -2.0657],\n",
       "                        [-3.4825, -2.8172, -3.4994]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4908, -2.0432, -2.2794],\n",
       "                        [-3.5052, -2.3631, -3.3350],\n",
       "                        [-3.8384, -3.3252, -3.0592]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0244, -2.9003, -2.2598],\n",
       "                        [-2.3511, -2.8186, -2.7978],\n",
       "                        [-2.2300, -3.5461, -3.2987]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3413, -3.5496, -2.6986],\n",
       "                        [-3.0460, -3.4901, -3.5558],\n",
       "                        [-3.5047, -2.1815, -3.4678]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5114, -2.3073, -2.5462],\n",
       "                        [-3.2874, -3.3658, -3.3263],\n",
       "                        [-3.9706, -2.3847, -2.0992]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2025, -2.9855, -3.8720],\n",
       "                        [-3.2507, -3.1225, -2.5146],\n",
       "                        [-3.9729, -2.0887, -2.4078]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5021, -2.3513, -2.3388],\n",
       "                        [-3.9824, -2.1225, -3.6460],\n",
       "                        [-2.9268, -2.8151, -2.7303]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4625, -2.1103, -3.4558],\n",
       "                        [-3.1533, -3.8654, -2.9948],\n",
       "                        [-2.6319, -2.6038, -2.7820]]]])),\n",
       "             ('module_list.0.posterior_params.0.scale_mus',\n",
       "              tensor([[[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]]])),\n",
       "             ('module_list.0.posterior_params.1.param_mus',\n",
       "              tensor([-5.6881e-02,  2.5755e-01, -5.7499e-02, -3.0782e-01,  5.6133e-02,\n",
       "                       1.4196e-01, -2.7147e-01,  6.2916e-02,  2.6742e-01, -1.2167e-01,\n",
       "                       2.6233e-01, -1.1228e-04, -8.7991e-02,  2.4257e-01,  1.5738e-01,\n",
       "                      -1.9730e-01, -3.6771e-02,  2.3806e-01, -2.5680e-01,  1.5844e-01,\n",
       "                       2.7671e-01, -2.8847e-01,  7.4341e-02,  1.8273e-01, -2.8194e-01,\n",
       "                       2.5949e-01, -2.9734e-01, -1.3813e-01, -2.3626e-01, -2.0522e-01,\n",
       "                      -3.4947e-02,  2.7957e-02])),\n",
       "             ('module_list.0.posterior_params.1.param_std_log',\n",
       "              tensor([-5.8531, -5.0251, -4.6695, -5.6580, -6.0908, -5.0468, -5.4194, -4.8829,\n",
       "                      -4.9358, -5.4649, -4.8578, -4.6518, -5.0004, -4.8242, -7.1573, -5.2299,\n",
       "                      -5.8643, -4.7494, -4.7539, -6.4383, -4.8113, -5.0502, -4.8234, -4.7153,\n",
       "                      -4.6958, -7.0433, -5.4695, -4.8326, -4.9841, -5.0226, -4.8043, -5.0743])),\n",
       "             ('module_list.0.posterior_params.1.scale_alphas_log',\n",
       "              tensor([-3.9720, -3.3769, -3.1610, -2.0548, -3.8005, -3.5241, -3.3899, -3.4581,\n",
       "                      -2.3674, -2.0493, -3.9427, -3.2582, -2.9190, -3.7603, -2.5605, -3.7906,\n",
       "                      -2.4988, -3.2822, -2.8320, -2.1995, -3.7139, -3.5474, -2.2760, -2.1320,\n",
       "                      -2.6326, -3.0290, -3.1089, -3.8023, -2.3189, -2.4084, -2.7251, -2.1182])),\n",
       "             ('module_list.0.posterior_params.1.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('module_list.0.posterior_params.2.param_mus',\n",
       "              tensor([[[[-7.7100e-03,  5.4790e-02, -4.6439e-02],\n",
       "                        [-2.5666e-02,  7.0856e-05,  4.1463e-02],\n",
       "                        [ 5.7703e-02, -3.2321e-02, -3.4684e-02]],\n",
       "              \n",
       "                       [[ 4.9002e-02, -3.1744e-02, -7.8825e-03],\n",
       "                        [-3.4527e-02,  2.0138e-02,  4.9787e-02],\n",
       "                        [ 5.3293e-02, -4.8873e-02, -4.2358e-02]],\n",
       "              \n",
       "                       [[ 2.8739e-02, -7.1017e-03, -2.5922e-02],\n",
       "                        [-3.3237e-02, -9.0375e-03,  3.8924e-02],\n",
       "                        [-4.8832e-02,  5.5395e-02, -1.2830e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2364e-02,  5.5431e-03, -3.4051e-02],\n",
       "                        [ 5.4685e-02, -3.8127e-02,  3.6562e-03],\n",
       "                        [ 5.1658e-02,  5.6627e-02,  2.4198e-02]],\n",
       "              \n",
       "                       [[-4.0373e-02,  8.4519e-03,  2.5526e-02],\n",
       "                        [-3.8454e-02,  7.0541e-03,  2.2421e-02],\n",
       "                        [ 4.2284e-02, -2.3315e-02,  3.1848e-02]],\n",
       "              \n",
       "                       [[-4.4469e-02,  1.6864e-02,  1.8774e-02],\n",
       "                        [ 4.0398e-02,  5.0153e-02, -2.7936e-02],\n",
       "                        [-1.1562e-02,  2.4845e-02, -3.6333e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.7107e-02, -4.6608e-02, -5.0606e-02],\n",
       "                        [ 5.6168e-02, -1.6773e-03,  4.2928e-02],\n",
       "                        [ 1.6456e-02, -2.0719e-04, -5.6311e-02]],\n",
       "              \n",
       "                       [[-1.7347e-02,  5.6713e-03, -7.0261e-03],\n",
       "                        [-4.1663e-02, -9.9240e-03, -2.2254e-02],\n",
       "                        [-2.1104e-02,  2.5374e-02, -3.4115e-02]],\n",
       "              \n",
       "                       [[-3.7840e-02,  2.0968e-02, -4.7591e-02],\n",
       "                        [-2.8589e-02, -3.2866e-02, -3.8424e-02],\n",
       "                        [-1.6116e-02,  1.3215e-02,  4.7036e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.1955e-02,  1.9305e-02, -2.6933e-02],\n",
       "                        [ 5.2013e-02, -2.2340e-04,  1.7213e-02],\n",
       "                        [-1.0361e-02, -3.6340e-02, -5.7661e-02]],\n",
       "              \n",
       "                       [[-2.3031e-03,  1.4640e-02,  7.7742e-04],\n",
       "                        [ 1.5956e-02, -4.1430e-02, -4.9954e-03],\n",
       "                        [ 3.7654e-02,  2.2213e-02, -2.7076e-02]],\n",
       "              \n",
       "                       [[ 2.3161e-02, -1.1589e-02,  1.9455e-02],\n",
       "                        [-1.3385e-02, -5.6369e-02,  3.8059e-02],\n",
       "                        [ 5.2567e-02,  4.7759e-02,  4.3005e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.7507e-02, -3.9272e-02,  5.2545e-02],\n",
       "                        [-3.1018e-02, -1.6988e-02,  5.5038e-02],\n",
       "                        [ 4.4960e-03,  9.7055e-03, -4.4981e-02]],\n",
       "              \n",
       "                       [[ 1.4212e-02,  1.2148e-02,  1.9770e-02],\n",
       "                        [ 1.6922e-02, -4.6928e-02,  2.9596e-03],\n",
       "                        [ 4.0252e-03, -3.9129e-02,  5.2690e-02]],\n",
       "              \n",
       "                       [[-3.7532e-03, -3.4104e-02, -5.4581e-02],\n",
       "                        [ 4.9478e-03,  1.1832e-02,  1.5692e-02],\n",
       "                        [-4.3561e-02, -2.7749e-02,  3.0280e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.5845e-03,  4.5895e-02, -4.0179e-02],\n",
       "                        [-2.6986e-02, -4.1516e-02, -8.0819e-03],\n",
       "                        [ 3.6541e-03, -4.2543e-02,  2.3785e-02]],\n",
       "              \n",
       "                       [[ 2.0291e-02,  9.1642e-03, -1.5169e-02],\n",
       "                        [-5.7352e-02,  3.2544e-03, -4.8554e-02],\n",
       "                        [-2.9224e-02, -5.5086e-03, -4.4527e-02]],\n",
       "              \n",
       "                       [[-1.4706e-02,  4.7383e-03, -5.0699e-02],\n",
       "                        [-4.6492e-02, -2.9252e-02, -3.0425e-02],\n",
       "                        [-4.3210e-02,  9.1058e-03,  1.4712e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 3.5956e-02,  2.2399e-03,  5.4483e-02],\n",
       "                        [ 7.6589e-03,  5.4506e-02, -3.1187e-02],\n",
       "                        [ 3.4773e-02,  2.2356e-02,  2.9660e-02]],\n",
       "              \n",
       "                       [[ 3.4219e-03,  1.4425e-02, -3.0753e-03],\n",
       "                        [ 5.0875e-02,  4.0691e-02,  1.8793e-02],\n",
       "                        [ 4.0225e-02, -5.7238e-04,  3.1252e-02]],\n",
       "              \n",
       "                       [[-5.1391e-02,  5.1525e-03, -5.5262e-03],\n",
       "                        [ 3.6265e-02, -2.1340e-02,  2.2140e-02],\n",
       "                        [-4.1796e-02,  4.1936e-02,  2.3722e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.6883e-02, -2.2848e-02, -3.2229e-02],\n",
       "                        [ 3.6033e-02, -1.2765e-02, -3.4013e-02],\n",
       "                        [-1.7630e-02,  4.9519e-02,  5.5844e-02]],\n",
       "              \n",
       "                       [[ 2.2459e-02, -4.1525e-04, -5.5119e-02],\n",
       "                        [ 8.3012e-03,  1.1867e-02, -8.7849e-03],\n",
       "                        [-1.1534e-02, -2.1331e-02, -3.3307e-02]],\n",
       "              \n",
       "                       [[-4.1118e-02, -2.0115e-02,  3.6809e-03],\n",
       "                        [ 9.4858e-03,  4.4473e-04, -3.7275e-02],\n",
       "                        [-2.3028e-02, -1.8307e-02, -5.3369e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3468e-02, -5.2856e-02,  1.5050e-02],\n",
       "                        [-1.6543e-02,  2.5859e-02,  1.6149e-02],\n",
       "                        [ 3.8698e-02,  4.5147e-02, -2.2739e-02]],\n",
       "              \n",
       "                       [[ 3.6351e-02, -1.1752e-02,  9.0186e-04],\n",
       "                        [ 5.6105e-02, -4.7348e-02, -9.0495e-03],\n",
       "                        [-2.1837e-02,  2.6221e-02, -4.4165e-02]],\n",
       "              \n",
       "                       [[-5.6114e-02, -3.8448e-02, -3.2051e-02],\n",
       "                        [-1.8787e-02, -5.4577e-02, -2.0123e-02],\n",
       "                        [-5.2164e-02,  3.7757e-02, -6.1961e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.7288e-02,  4.9713e-02,  4.6649e-02],\n",
       "                        [-2.4366e-02, -5.2146e-02, -4.2155e-02],\n",
       "                        [ 5.7554e-02, -4.7145e-03, -3.4380e-02]],\n",
       "              \n",
       "                       [[ 1.0833e-02, -2.7129e-02,  5.1384e-02],\n",
       "                        [-5.8356e-03, -8.7180e-03, -5.2810e-02],\n",
       "                        [-3.7678e-02,  3.5120e-02,  5.6944e-02]],\n",
       "              \n",
       "                       [[-2.7352e-02,  4.7641e-02, -2.2490e-03],\n",
       "                        [-3.1199e-02, -1.1621e-04, -1.3695e-02],\n",
       "                        [ 5.1445e-02,  7.7968e-03,  6.1409e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.5890e-02, -7.1815e-03, -3.6486e-02],\n",
       "                        [-4.0022e-02,  1.3700e-02,  4.5343e-02],\n",
       "                        [-1.7137e-02,  3.0123e-03, -1.2329e-03]],\n",
       "              \n",
       "                       [[ 3.4199e-04,  5.5280e-02,  4.9747e-02],\n",
       "                        [-1.8140e-02, -5.3980e-02,  1.0436e-02],\n",
       "                        [-4.6931e-03, -2.0930e-02, -1.9942e-02]],\n",
       "              \n",
       "                       [[ 4.9414e-02, -9.9996e-03,  2.9463e-02],\n",
       "                        [ 3.8377e-02,  1.7507e-03,  2.3896e-02],\n",
       "                        [-2.5887e-02,  2.8545e-03,  5.7909e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.7541e-02,  1.7774e-02,  1.0854e-02],\n",
       "                        [ 3.8050e-02, -4.6370e-02, -4.9693e-02],\n",
       "                        [ 2.8391e-02, -5.1057e-02, -4.7155e-02]],\n",
       "              \n",
       "                       [[-4.7780e-02,  4.7283e-02, -3.4375e-02],\n",
       "                        [ 5.4516e-02, -5.0223e-04,  2.9349e-02],\n",
       "                        [-3.7408e-02, -4.2198e-02,  4.7006e-03]],\n",
       "              \n",
       "                       [[-1.3772e-02,  1.3996e-02, -1.4861e-02],\n",
       "                        [ 1.7226e-02, -3.4662e-02, -9.6441e-03],\n",
       "                        [-4.3303e-02, -7.0372e-03, -6.3834e-03]]]])),\n",
       "             ('module_list.0.posterior_params.2.param_std_log',\n",
       "              tensor([[[[ -5.4104,  -4.9978,  -8.6057],\n",
       "                        [ -5.5410,  -5.3641,  -5.6445],\n",
       "                        [ -4.8368,  -6.1301,  -4.6584]],\n",
       "              \n",
       "                       [[ -5.1266,  -6.6334,  -5.3840],\n",
       "                        [ -7.8193,  -4.7209,  -4.8470],\n",
       "                        [ -6.4368,  -5.3148,  -6.6645]],\n",
       "              \n",
       "                       [[ -6.4395,  -6.4142,  -4.8790],\n",
       "                        [ -5.7961,  -5.1277,  -5.4331],\n",
       "                        [ -5.3921,  -6.2843,  -6.0879]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -5.1966,  -5.1421,  -5.4617],\n",
       "                        [ -4.7031,  -4.6894,  -4.8936],\n",
       "                        [ -7.6478,  -5.3568,  -5.0432]],\n",
       "              \n",
       "                       [[ -4.7332,  -5.1472,  -5.2536],\n",
       "                        [ -6.2649,  -5.0098,  -5.2781],\n",
       "                        [ -4.8065,  -6.2850,  -6.8422]],\n",
       "              \n",
       "                       [[ -4.9900,  -4.9414,  -4.7617],\n",
       "                        [ -6.1922,  -6.2038,  -5.8695],\n",
       "                        [ -6.0775,  -4.9624,  -5.7281]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -7.2408,  -6.3992,  -5.5610],\n",
       "                        [ -6.8896,  -5.5852,  -6.3439],\n",
       "                        [ -6.6148,  -4.6649,  -5.0876]],\n",
       "              \n",
       "                       [[ -4.8547,  -4.9660,  -8.7027],\n",
       "                        [ -5.4467,  -5.1763,  -4.7114],\n",
       "                        [ -4.6268,  -5.2963,  -5.6104]],\n",
       "              \n",
       "                       [[ -5.3600,  -5.3118,  -5.0091],\n",
       "                        [ -5.4969,  -5.0090,  -5.0104],\n",
       "                        [ -5.0822,  -5.0555,  -6.3731]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -4.6782,  -5.0509,  -5.0103],\n",
       "                        [ -4.9681,  -5.7592,  -5.2169],\n",
       "                        [ -4.7667,  -4.6440,  -6.5099]],\n",
       "              \n",
       "                       [[ -5.6445,  -7.8781,  -5.1786],\n",
       "                        [ -8.1867,  -6.2985,  -5.4559],\n",
       "                        [ -5.0019,  -4.7381,  -4.8871]],\n",
       "              \n",
       "                       [[ -4.7108,  -5.4525,  -5.3355],\n",
       "                        [ -5.2683,  -5.8155,  -5.8280],\n",
       "                        [ -6.7590,  -6.0488,  -6.4925]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -4.6907,  -5.7881,  -5.5594],\n",
       "                        [ -4.7776,  -5.0134,  -4.6359],\n",
       "                        [ -5.2801,  -5.1021,  -4.9513]],\n",
       "              \n",
       "                       [[ -7.3718,  -5.5058,  -5.4042],\n",
       "                        [ -5.7779,  -5.0867,  -5.4612],\n",
       "                        [ -5.1246,  -4.9519,  -5.6977]],\n",
       "              \n",
       "                       [[ -5.1837,  -4.8839,  -7.5473],\n",
       "                        [ -5.0652,  -5.3055,  -5.4602],\n",
       "                        [ -5.3764,  -6.2016,  -6.8509]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -4.8055,  -9.2720,  -5.6671],\n",
       "                        [ -6.3970,  -6.1226,  -5.2287],\n",
       "                        [ -6.8128,  -5.0915,  -4.6921]],\n",
       "              \n",
       "                       [[ -4.6948,  -5.1463,  -5.0574],\n",
       "                        [ -6.1934,  -5.2626,  -5.4259],\n",
       "                        [ -5.0383,  -4.6811,  -4.9122]],\n",
       "              \n",
       "                       [[ -5.7202,  -5.0158,  -5.6039],\n",
       "                        [ -5.4865,  -4.6806,  -5.3936],\n",
       "                        [ -4.9192,  -5.4019,  -4.8026]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ -4.8065,  -4.6364,  -5.9098],\n",
       "                        [ -5.1485,  -7.5170,  -5.5308],\n",
       "                        [ -4.6733,  -5.5754,  -6.2896]],\n",
       "              \n",
       "                       [[ -7.6320,  -4.6801,  -4.7657],\n",
       "                        [ -4.9276,  -5.1949,  -4.8800],\n",
       "                        [ -5.1586,  -5.0395,  -5.6997]],\n",
       "              \n",
       "                       [[ -4.7407,  -4.6271,  -4.6581],\n",
       "                        [ -4.8081,  -4.9950,  -5.2914],\n",
       "                        [ -5.8292,  -6.0302,  -4.9355]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -4.7621,  -6.5300,  -5.2816],\n",
       "                        [ -4.7669,  -5.5216,  -5.6226],\n",
       "                        [ -4.6431,  -5.2053,  -4.7768]],\n",
       "              \n",
       "                       [[ -5.2672,  -4.6458,  -5.6260],\n",
       "                        [ -5.0704,  -5.5947,  -4.7157],\n",
       "                        [-10.2410,  -7.7036,  -5.1846]],\n",
       "              \n",
       "                       [[ -5.8608,  -4.7753,  -4.8339],\n",
       "                        [ -5.3976,  -4.9830,  -5.5042],\n",
       "                        [ -5.0678,  -5.6684,  -5.0389]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -4.9607,  -4.6108,  -4.9775],\n",
       "                        [ -7.7357,  -5.0077,  -4.6865],\n",
       "                        [ -4.7754,  -6.4242,  -6.4673]],\n",
       "              \n",
       "                       [[ -5.0494,  -4.7677,  -7.3062],\n",
       "                        [ -4.9133,  -5.2170,  -6.2128],\n",
       "                        [ -5.0076,  -4.6315,  -8.6533]],\n",
       "              \n",
       "                       [[ -7.3104,  -5.9673,  -4.9282],\n",
       "                        [ -6.5305,  -5.4826,  -4.7867],\n",
       "                        [ -4.8943,  -5.3702,  -4.8722]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -4.7898,  -6.7627,  -5.4909],\n",
       "                        [ -5.1917,  -7.0362,  -4.7019],\n",
       "                        [ -5.0209,  -4.9642,  -4.6499]],\n",
       "              \n",
       "                       [[ -5.7185,  -6.9583,  -4.6716],\n",
       "                        [ -4.9676,  -7.4500,  -6.6719],\n",
       "                        [ -8.3092,  -5.1161,  -6.8875]],\n",
       "              \n",
       "                       [[ -6.2261,  -7.9961,  -4.8236],\n",
       "                        [ -5.4395,  -4.6242,  -4.7380],\n",
       "                        [ -4.7772,  -5.0955,  -5.1896]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -6.0302,  -5.1819,  -6.1809],\n",
       "                        [ -4.9349,  -5.5667,  -4.6051],\n",
       "                        [ -5.0230,  -4.6098,  -6.4777]],\n",
       "              \n",
       "                       [[ -4.6580,  -5.6923,  -4.6517],\n",
       "                        [ -4.9964,  -4.7639,  -8.1039],\n",
       "                        [ -5.4952,  -4.8853,  -4.7961]],\n",
       "              \n",
       "                       [[ -8.6251,  -5.2624,  -4.8002],\n",
       "                        [ -4.9107,  -6.9034,  -5.5238],\n",
       "                        [ -5.0566,  -4.6630,  -5.5966]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -5.7330,  -4.9659,  -5.2206],\n",
       "                        [ -6.1430,  -5.0759,  -4.8861],\n",
       "                        [ -6.3704,  -6.1748,  -7.2973]],\n",
       "              \n",
       "                       [[ -5.3204,  -4.8054,  -5.5253],\n",
       "                        [ -5.3475,  -8.0224,  -5.0655],\n",
       "                        [ -6.2963,  -4.8095,  -4.8542]],\n",
       "              \n",
       "                       [[ -4.7218,  -5.6109,  -4.7300],\n",
       "                        [ -6.1425,  -5.1648,  -5.0280],\n",
       "                        [ -7.1465,  -4.7719,  -5.5369]]]])),\n",
       "             ('module_list.0.posterior_params.2.scale_alphas_log',\n",
       "              tensor([[[[-2.5651, -2.7419, -3.5278],\n",
       "                        [-3.0839, -2.5479, -2.1193],\n",
       "                        [-3.1535, -3.2065, -2.6906]],\n",
       "              \n",
       "                       [[-2.0314, -2.3613, -3.0126],\n",
       "                        [-3.0940, -3.5424, -3.1908],\n",
       "                        [-3.8670, -3.0771, -3.0454]],\n",
       "              \n",
       "                       [[-2.2935, -2.1834, -3.6846],\n",
       "                        [-3.6370, -2.0811, -2.2337],\n",
       "                        [-2.4522, -3.2599, -2.3555]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.2979, -3.6483, -3.5111],\n",
       "                        [-3.2588, -3.3310, -3.0727],\n",
       "                        [-2.4452, -2.2223, -3.6400]],\n",
       "              \n",
       "                       [[-2.6860, -2.7594, -3.8629],\n",
       "                        [-2.6530, -3.2572, -3.7061],\n",
       "                        [-2.7411, -3.8606, -3.1207]],\n",
       "              \n",
       "                       [[-3.6129, -3.0020, -2.3459],\n",
       "                        [-3.0822, -2.5213, -2.0656],\n",
       "                        [-3.2128, -2.2671, -3.2052]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8543, -3.5376, -2.7081],\n",
       "                        [-3.1088, -2.4994, -3.3062],\n",
       "                        [-2.4490, -2.1128, -2.1287]],\n",
       "              \n",
       "                       [[-2.3360, -2.4865, -3.4616],\n",
       "                        [-3.5498, -2.5935, -2.8767],\n",
       "                        [-3.9802, -2.6312, -2.5105]],\n",
       "              \n",
       "                       [[-2.0669, -2.2181, -2.2497],\n",
       "                        [-3.0990, -3.3692, -2.3777],\n",
       "                        [-3.0440, -2.7206, -2.1542]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3624, -3.7036, -2.3691],\n",
       "                        [-3.0927, -3.5434, -2.0402],\n",
       "                        [-2.5422, -3.1002, -2.4366]],\n",
       "              \n",
       "                       [[-2.6236, -3.8597, -2.1411],\n",
       "                        [-2.1730, -3.5103, -2.2584],\n",
       "                        [-2.7806, -2.1457, -3.0277]],\n",
       "              \n",
       "                       [[-2.5723, -2.0781, -3.7810],\n",
       "                        [-3.4151, -3.5408, -2.0486],\n",
       "                        [-3.4879, -2.0673, -2.9052]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9648, -2.3590, -2.5193],\n",
       "                        [-2.1220, -2.7566, -3.9835],\n",
       "                        [-3.5533, -3.6094, -2.7020]],\n",
       "              \n",
       "                       [[-2.2191, -3.1740, -2.2530],\n",
       "                        [-2.3296, -3.1470, -3.8575],\n",
       "                        [-2.3254, -3.1779, -2.7257]],\n",
       "              \n",
       "                       [[-2.8613, -3.0244, -2.5403],\n",
       "                        [-3.3255, -3.0642, -3.3740],\n",
       "                        [-2.4216, -3.1601, -3.0668]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.7132, -2.3032, -2.4217],\n",
       "                        [-3.4354, -2.0411, -3.0858],\n",
       "                        [-3.0153, -3.6040, -3.0406]],\n",
       "              \n",
       "                       [[-3.3346, -3.5698, -2.3740],\n",
       "                        [-2.7562, -3.4489, -2.6199],\n",
       "                        [-2.1747, -3.3835, -3.2208]],\n",
       "              \n",
       "                       [[-3.6880, -3.4064, -3.4985],\n",
       "                        [-3.3169, -2.9990, -3.7681],\n",
       "                        [-3.4089, -3.5360, -3.7868]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.8750, -2.6284, -3.7562],\n",
       "                        [-2.5736, -2.3814, -3.2896],\n",
       "                        [-3.7208, -2.5928, -3.7039]],\n",
       "              \n",
       "                       [[-3.1718, -3.9350, -3.0486],\n",
       "                        [-3.4407, -2.7986, -2.7181],\n",
       "                        [-3.7697, -3.7439, -3.4146]],\n",
       "              \n",
       "                       [[-3.1779, -2.5221, -3.3334],\n",
       "                        [-3.3061, -3.6714, -2.3800],\n",
       "                        [-3.1225, -2.6066, -3.3849]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3843, -2.2058, -2.7789],\n",
       "                        [-2.2253, -2.1109, -3.4435],\n",
       "                        [-2.9683, -3.8334, -2.6312]],\n",
       "              \n",
       "                       [[-2.3164, -2.6402, -3.7229],\n",
       "                        [-3.1937, -3.3132, -2.4968],\n",
       "                        [-2.6240, -2.3896, -2.4467]],\n",
       "              \n",
       "                       [[-3.4800, -2.2377, -2.5435],\n",
       "                        [-2.9780, -2.8389, -2.5057],\n",
       "                        [-3.9736, -3.8490, -2.0625]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5547, -3.0333, -3.7432],\n",
       "                        [-2.0415, -3.0963, -2.2126],\n",
       "                        [-2.6741, -3.9799, -2.5448]],\n",
       "              \n",
       "                       [[-3.3684, -3.1576, -2.3836],\n",
       "                        [-2.9717, -2.0439, -3.7213],\n",
       "                        [-3.3196, -2.9403, -2.7848]],\n",
       "              \n",
       "                       [[-2.6005, -3.5631, -2.0025],\n",
       "                        [-3.1784, -2.7012, -2.1489],\n",
       "                        [-3.1660, -2.6460, -2.5723]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.4148, -2.2106, -2.4273],\n",
       "                        [-2.6520, -2.1728, -2.2496],\n",
       "                        [-3.4775, -3.0633, -3.1280]],\n",
       "              \n",
       "                       [[-2.0488, -3.6257, -3.0566],\n",
       "                        [-3.7811, -3.1934, -3.5403],\n",
       "                        [-3.8622, -2.8265, -2.3247]],\n",
       "              \n",
       "                       [[-3.3523, -3.2954, -3.3465],\n",
       "                        [-3.9367, -2.0951, -2.8117],\n",
       "                        [-3.8344, -2.6131, -3.5930]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0192, -2.7321, -3.7076],\n",
       "                        [-2.8065, -2.3403, -2.7735],\n",
       "                        [-2.2203, -2.7763, -2.8139]],\n",
       "              \n",
       "                       [[-3.8022, -2.5851, -2.2735],\n",
       "                        [-2.2481, -3.8494, -3.7944],\n",
       "                        [-2.4832, -3.4831, -2.1386]],\n",
       "              \n",
       "                       [[-2.7159, -2.2835, -3.3348],\n",
       "                        [-2.9014, -3.8916, -2.3636],\n",
       "                        [-2.1203, -3.6634, -2.4398]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.5673, -3.0012, -3.8460],\n",
       "                        [-3.4758, -3.7653, -2.0118],\n",
       "                        [-2.0891, -2.9877, -2.3519]],\n",
       "              \n",
       "                       [[-2.9740, -3.8680, -3.3323],\n",
       "                        [-3.3175, -2.2555, -3.4732],\n",
       "                        [-3.9593, -2.0924, -3.5181]],\n",
       "              \n",
       "                       [[-3.5667, -3.7302, -3.1632],\n",
       "                        [-2.1897, -2.4513, -3.2363],\n",
       "                        [-2.1165, -3.0536, -2.3101]]]])),\n",
       "             ('module_list.0.posterior_params.2.scale_mus',\n",
       "              tensor([[[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]]])),\n",
       "             ('module_list.0.posterior_params.3.param_mus',\n",
       "              tensor([ 0.0175, -0.0075,  0.0442, -0.0523,  0.0447,  0.0448,  0.0274,  0.0463,\n",
       "                       0.0074, -0.0524,  0.0070, -0.0225,  0.0004, -0.0530, -0.0079,  0.0258,\n",
       "                       0.0317,  0.0537,  0.0152, -0.0364, -0.0483,  0.0447,  0.0149,  0.0114,\n",
       "                      -0.0159,  0.0134,  0.0374, -0.0441, -0.0432,  0.0370,  0.0205,  0.0575,\n",
       "                       0.0333,  0.0362,  0.0057,  0.0574, -0.0477,  0.0557, -0.0166,  0.0510,\n",
       "                       0.0099,  0.0371, -0.0504,  0.0320,  0.0515,  0.0240,  0.0103,  0.0452,\n",
       "                       0.0265,  0.0073, -0.0266,  0.0193, -0.0318,  0.0420,  0.0005,  0.0214,\n",
       "                       0.0547, -0.0497, -0.0493,  0.0130, -0.0469, -0.0510, -0.0531, -0.0081])),\n",
       "             ('module_list.0.posterior_params.3.param_std_log',\n",
       "              tensor([-4.6773, -6.1726, -4.6882, -4.9873, -5.3467, -7.2078, -6.3968, -5.9758,\n",
       "                      -5.4038, -6.3960, -5.3985, -4.6320, -5.3869, -4.8623, -4.9121, -4.9363,\n",
       "                      -5.5539, -6.3417, -5.8659, -5.7632, -6.4558, -4.9935, -4.6283, -5.8718,\n",
       "                      -4.6585, -7.0770, -5.0169, -7.1327, -4.6727, -4.7127, -5.3287, -5.4914,\n",
       "                      -4.8592, -5.9273, -4.6707, -5.0561, -4.8049, -4.8804, -6.1399, -5.0882,\n",
       "                      -5.7801, -5.9745, -5.6524, -5.5508, -5.7423, -6.8843, -6.9704, -8.1110,\n",
       "                      -5.8109, -5.9531, -4.7024, -6.7915, -5.5347, -5.9438, -5.3814, -6.9990,\n",
       "                      -5.0976, -5.3466, -4.8104, -5.1079, -5.6689, -6.0047, -5.2952, -5.0970])),\n",
       "             ('module_list.0.posterior_params.3.scale_alphas_log',\n",
       "              tensor([-3.1738, -3.0784, -2.4871, -3.0219, -2.8572, -2.3968, -3.5660, -3.0801,\n",
       "                      -2.2082, -2.1469, -2.7161, -3.5088, -3.0030, -2.0700, -2.3132, -3.5840,\n",
       "                      -2.5336, -3.5013, -3.0859, -2.7998, -3.9310, -2.4742, -2.2515, -3.7222,\n",
       "                      -2.6094, -2.9756, -2.0963, -2.1953, -2.8759, -2.3720, -3.0047, -2.1983,\n",
       "                      -3.2377, -3.9396, -3.4534, -3.7109, -3.1937, -2.8690, -2.9031, -3.5238,\n",
       "                      -3.8824, -3.3756, -3.5311, -2.2332, -3.8197, -2.7067, -3.9622, -2.3386,\n",
       "                      -2.4010, -3.9145, -3.6471, -2.1165, -2.2047, -2.7885, -2.2987, -2.0475,\n",
       "                      -2.5157, -3.7477, -3.4118, -3.6898, -3.5554, -2.4396, -3.9712, -3.7816])),\n",
       "             ('module_list.0.posterior_params.3.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('module_list.0.posterior_params.4.param_mus',\n",
       "              tensor([[-1.2934e-02,  4.6069e-03, -1.5731e-02,  ..., -1.6821e-03,\n",
       "                       -1.5373e-02,  1.5965e-03],\n",
       "                      [ 1.4330e-02,  1.5377e-02, -6.6470e-03,  ..., -1.4631e-02,\n",
       "                        7.5257e-03,  1.2764e-02],\n",
       "                      [-5.0996e-05, -1.3151e-02, -2.1297e-03,  ..., -1.4281e-02,\n",
       "                       -4.2328e-03, -1.0496e-02],\n",
       "                      ...,\n",
       "                      [-5.6378e-03, -8.1883e-03, -2.5546e-03,  ...,  1.3585e-02,\n",
       "                        5.2078e-03,  4.6191e-04],\n",
       "                      [-1.1376e-02,  5.0245e-03,  3.7784e-04,  ...,  2.7126e-04,\n",
       "                        1.0592e-02, -9.2912e-04],\n",
       "                      [ 1.4872e-02,  1.0564e-03,  1.1583e-02,  ...,  5.1038e-03,\n",
       "                        5.3327e-03, -3.3570e-03]])),\n",
       "             ('module_list.0.posterior_params.4.param_std_log',\n",
       "              tensor([[-4.6359, -4.8121, -4.9456,  ..., -5.8884, -4.6772, -6.9920],\n",
       "                      [-6.8308, -4.8500, -4.6605,  ..., -4.9040, -4.7652, -6.3424],\n",
       "                      [-4.8811, -6.0618, -4.6506,  ..., -4.7695, -4.6690, -5.1588],\n",
       "                      ...,\n",
       "                      [-7.2252, -5.8683, -4.7476,  ..., -5.1372, -5.5091, -4.7808],\n",
       "                      [-5.5992, -5.0709, -4.8434,  ..., -5.4568, -5.7462, -4.9964],\n",
       "                      [-4.7166, -5.6478, -5.2482,  ..., -5.7334, -4.8581, -5.3240]])),\n",
       "             ('module_list.0.posterior_params.4.scale_alphas_log',\n",
       "              tensor([[-2.4488, -3.4212, -2.8364,  ..., -2.7037, -2.5206, -3.8757],\n",
       "                      [-2.3803, -2.6793, -3.4947,  ..., -3.9776, -3.9696, -2.1145],\n",
       "                      [-2.8276, -2.2622, -3.1709,  ..., -3.6444, -3.7385, -3.2620],\n",
       "                      ...,\n",
       "                      [-2.1763, -3.2857, -3.9804,  ..., -3.8020, -2.1040, -3.1916],\n",
       "                      [-2.3288, -2.6822, -3.6842,  ..., -3.0280, -3.9154, -2.3643],\n",
       "                      [-3.5653, -2.4831, -3.0220,  ..., -3.4718, -2.8972, -3.4631]])),\n",
       "             ('module_list.0.posterior_params.4.scale_mus',\n",
       "              tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('module_list.0.posterior_params.5.param_mus',\n",
       "              tensor([ 0.0025,  0.0030,  0.0092, -0.0038,  0.0140, -0.0145, -0.0115, -0.0067,\n",
       "                       0.0001, -0.0122,  0.0103,  0.0146,  0.0045, -0.0010, -0.0096,  0.0063,\n",
       "                       0.0041,  0.0040,  0.0123, -0.0141, -0.0011,  0.0020, -0.0057,  0.0042,\n",
       "                       0.0112,  0.0040,  0.0026, -0.0120, -0.0089,  0.0083,  0.0131,  0.0034,\n",
       "                      -0.0112, -0.0116, -0.0088,  0.0094, -0.0101, -0.0093,  0.0077,  0.0109,\n",
       "                       0.0123,  0.0098,  0.0003, -0.0024, -0.0063, -0.0117, -0.0081, -0.0084,\n",
       "                       0.0014, -0.0080, -0.0112, -0.0032,  0.0004, -0.0019, -0.0044, -0.0070,\n",
       "                       0.0014,  0.0098,  0.0053, -0.0100, -0.0145,  0.0113,  0.0117,  0.0163,\n",
       "                       0.0016,  0.0164, -0.0084,  0.0040,  0.0069,  0.0072, -0.0140, -0.0020,\n",
       "                      -0.0113, -0.0086,  0.0082, -0.0144, -0.0010,  0.0083, -0.0118,  0.0002,\n",
       "                       0.0156,  0.0025, -0.0082, -0.0158, -0.0115, -0.0058, -0.0091,  0.0054,\n",
       "                       0.0163,  0.0094,  0.0079, -0.0020, -0.0014, -0.0048, -0.0098, -0.0160,\n",
       "                       0.0054, -0.0072, -0.0035, -0.0043, -0.0091,  0.0004,  0.0081, -0.0106,\n",
       "                       0.0009,  0.0011, -0.0009,  0.0013, -0.0163, -0.0149, -0.0120,  0.0058,\n",
       "                      -0.0078,  0.0096, -0.0031,  0.0092, -0.0162,  0.0111,  0.0160, -0.0120,\n",
       "                       0.0168,  0.0074,  0.0096,  0.0048,  0.0109, -0.0097, -0.0130, -0.0090])),\n",
       "             ('module_list.0.posterior_params.5.param_std_log',\n",
       "              tensor([-5.3846, -5.3028, -5.4666, -4.8195, -7.6122, -4.7050, -4.6357, -5.7648,\n",
       "                      -6.4475, -5.6077, -5.2448, -5.9953, -4.8014, -6.1142, -4.7692, -5.1732,\n",
       "                      -4.8463, -4.8673, -4.9036, -5.8874, -4.8937, -5.8473, -5.4722, -7.0633,\n",
       "                      -5.0285, -6.1785, -5.0866, -4.6960, -5.1282, -6.1803, -4.6985, -5.0259,\n",
       "                      -4.7180, -6.7598, -6.3043, -6.0697, -7.7769, -6.0112, -6.0171, -4.6396,\n",
       "                      -5.8598, -4.6667, -5.8215, -5.0982, -6.2810, -6.1716, -5.2573, -5.8743,\n",
       "                      -5.5974, -5.2587, -5.0285, -5.2011, -5.0233, -5.6212, -4.7240, -6.5489,\n",
       "                      -4.6708, -5.7433, -4.6713, -5.1100, -4.8218, -4.7169, -4.9363, -5.8585,\n",
       "                      -5.5222, -5.0531, -6.0190, -5.2752, -5.2220, -4.9272, -4.6261, -4.7699,\n",
       "                      -5.2585, -4.8961, -5.7954, -5.8696, -4.6904, -6.2334, -5.5877, -5.1218,\n",
       "                      -4.8671, -7.8764, -4.7208, -5.6652, -4.8005, -7.6380, -6.4175, -5.3309,\n",
       "                      -4.8851, -5.6440, -6.1354, -5.3968, -6.2333, -6.2556, -5.4602, -5.5295,\n",
       "                      -7.3512, -4.8287, -5.1901, -6.3750, -5.1311, -4.7942, -4.7388, -4.8867,\n",
       "                      -6.9905, -5.3951, -5.2568, -4.8035, -5.2433, -6.6233, -6.2924, -5.5931,\n",
       "                      -4.7311, -4.9860, -5.1850, -5.6291, -4.7630, -6.0894, -4.7327, -5.1452,\n",
       "                      -6.0768, -4.8887, -4.7643, -5.9345, -6.2522, -6.0805, -5.0974, -5.5019])),\n",
       "             ('module_list.0.posterior_params.5.scale_alphas_log',\n",
       "              tensor([-2.8239, -3.6351, -3.7493, -2.3530, -3.0312, -3.4964, -2.0801, -3.7547,\n",
       "                      -3.9010, -3.8187, -3.2450, -2.8037, -3.4103, -3.0766, -2.7415, -3.9705,\n",
       "                      -2.2006, -2.9419, -3.8342, -2.9242, -3.8768, -3.3549, -2.4048, -2.1390,\n",
       "                      -2.0468, -3.5199, -2.8415, -2.9973, -3.5747, -3.2980, -3.5651, -2.9236,\n",
       "                      -2.0865, -3.5190, -3.1391, -3.5814, -2.7925, -3.5176, -3.5464, -3.3841,\n",
       "                      -2.9709, -3.8562, -3.9934, -3.3380, -2.7347, -3.9655, -2.1013, -2.0301,\n",
       "                      -3.9880, -2.0866, -2.8640, -3.9061, -3.9021, -3.3758, -3.7028, -2.3521,\n",
       "                      -3.3516, -3.5714, -3.6933, -3.1237, -3.1871, -2.8985, -3.4566, -2.8318,\n",
       "                      -2.9233, -3.3061, -3.9511, -3.7219, -3.2349, -2.9654, -2.6196, -2.3083,\n",
       "                      -2.4735, -2.8087, -2.8836, -2.2997, -3.4012, -2.4772, -3.3623, -2.1731,\n",
       "                      -2.9269, -3.9547, -3.3228, -2.8042, -3.8131, -3.8354, -3.7320, -2.7671,\n",
       "                      -2.3505, -3.6917, -2.9128, -2.0854, -2.3701, -3.9551, -2.1034, -3.4539,\n",
       "                      -2.7484, -2.6592, -3.2724, -2.7813, -3.7857, -2.7104, -2.1650, -2.8028,\n",
       "                      -2.2386, -3.6985, -3.0961, -2.6645, -3.5605, -3.5868, -2.8917, -2.3276,\n",
       "                      -2.5879, -3.7906, -3.4341, -3.5184, -3.8543, -3.9185, -2.1230, -2.2754,\n",
       "                      -3.7884, -3.6983, -3.7247, -2.5468, -3.7499, -3.2339, -3.5685, -3.4939])),\n",
       "             ('module_list.0.posterior_params.5.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('module_list.0.posterior_params.6.param_mus',\n",
       "              tensor([[-0.0432,  0.0673,  0.0641,  ..., -0.0305,  0.0251,  0.0043],\n",
       "                      [ 0.0577, -0.0009, -0.0237,  ..., -0.0649,  0.0477, -0.0689],\n",
       "                      [-0.0590,  0.0656, -0.0437,  ...,  0.0585, -0.0848,  0.0762],\n",
       "                      ...,\n",
       "                      [ 0.0264, -0.0321, -0.0694,  ..., -0.0188,  0.0396,  0.0584],\n",
       "                      [ 0.0009,  0.0542,  0.0013,  ...,  0.0681,  0.0856,  0.0200],\n",
       "                      [ 0.0771,  0.0215, -0.0493,  ...,  0.0818, -0.0450,  0.0214]])),\n",
       "             ('module_list.0.posterior_params.6.param_std_log',\n",
       "              tensor([[-5.1131, -4.9649, -9.4469,  ..., -4.8388, -5.9063, -4.7823],\n",
       "                      [-4.8934, -8.1244, -5.0997,  ..., -4.6722, -4.8615, -5.1756],\n",
       "                      [-5.8074, -4.8326, -5.1726,  ..., -5.6627, -7.7390, -5.5862],\n",
       "                      ...,\n",
       "                      [-4.9913, -5.9805, -5.4895,  ..., -5.4461, -4.7108, -5.9509],\n",
       "                      [-4.6525, -6.7020, -4.8345,  ..., -4.9422, -4.9393, -4.8533],\n",
       "                      [-4.7163, -5.0048, -4.9875,  ..., -5.3396, -5.9939, -4.8631]])),\n",
       "             ('module_list.0.posterior_params.6.scale_alphas_log',\n",
       "              tensor([[-2.8439, -3.6124, -2.8808,  ..., -2.1434, -3.7476, -3.6642],\n",
       "                      [-3.4235, -3.9836, -3.1441,  ..., -2.2819, -3.2358, -3.1570],\n",
       "                      [-3.4447, -2.2839, -3.0724,  ..., -3.7216, -3.0857, -2.6107],\n",
       "                      ...,\n",
       "                      [-3.5940, -2.1325, -2.1207,  ..., -2.2380, -2.8697, -3.7310],\n",
       "                      [-2.5165, -2.9487, -3.6339,  ..., -3.5334, -2.8400, -3.0699],\n",
       "                      [-3.4445, -2.5812, -3.0529,  ..., -3.4575, -3.8311, -3.8849]])),\n",
       "             ('module_list.0.posterior_params.6.scale_mus',\n",
       "              tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('module_list.0.posterior_params.7.param_mus',\n",
       "              tensor([ 0.0394,  0.0677, -0.0488,  0.0612, -0.0794,  0.0182,  0.0495, -0.0764,\n",
       "                      -0.0267, -0.0731])),\n",
       "             ('module_list.0.posterior_params.7.param_std_log',\n",
       "              tensor([-4.6459, -5.2529, -6.1177, -5.8416, -4.7799, -4.6955, -4.6498, -5.0164,\n",
       "                      -4.6333, -6.0870])),\n",
       "             ('module_list.0.posterior_params.7.scale_alphas_log',\n",
       "              tensor([-3.3607, -2.7567, -3.7331, -3.0308, -3.8869, -3.6477, -2.0743, -2.7621,\n",
       "                      -2.7696, -3.4189])),\n",
       "             ('module_list.0.posterior_params.7.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward делается по последнему сохраненному сэмплу. Заметим, что мы нигде не копируем данные, и модели не инкапсулируется. Поэтому, чтобы отвязать, их неободимо скопировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0364,  0.0278, -0.0481,  0.0617, -0.0657,  0.0238,  0.0492, -0.0762,\n",
      "         -0.0143, -0.0513]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0364,  0.0278, -0.0481,  0.0617, -0.0657,  0.0238,  0.0492, -0.0762,\n",
      "         -0.0143, -0.0513]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(bayes_model(torch.zeros_like(image)))\n",
    "#print(bayes_model(torch.zeros_like(image), sample = False))\n",
    "print(module(torch.zeros_like(image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы импортируем несколько модулей для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayescomp.bayes.variational.trainer import VarBayesTrainer, VarTrainerParams, Beta_Scheduler_Plato, CallbackLossAccuracy #Сам тренер, Параметры тренера, Планировщик beta(коэффициент сооьношения между обычным лоссом и байесовским), и callback для метрики точности\n",
    "from bayescomp.report.base import ReportChain #Это просто список callback\n",
    "from bayescomp.report.variational import VarBaseReport #Этот модуль callback просто выводит каждый шаг данные от тренера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = Beta_Scheduler_Plato()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77b7964d3434f12b37974d67604ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4000],Loss:30380.31640625, KL Loss: 3037819.75. FitLoss: 2.1177992820739746,Accuracy:0.3478250000000001,Validation Loss:30350.50390625,Validation Accuracy:0.713, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [2/4000],Loss:30327.248046875, KL Loss: 3032596.5. FitLoss: 1.2829042673110962,Accuracy:0.7069,Validation Loss:30297.421875,Validation Accuracy:0.776, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [3/4000],Loss:30274.4765625, KL Loss: 3027379.5. FitLoss: 0.6833617091178894,Accuracy:0.7846125000000004,Validation Loss:30244.9296875,Validation Accuracy:0.864, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [4/4000],Loss:30222.08203125, KL Loss: 3022158.75. FitLoss: 0.4941781163215637,Accuracy:0.8455999999999999,Validation Loss:30192.59765625,Validation Accuracy:0.895, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [5/4000],Loss:30169.732421875, KL Loss: 3016934.5. FitLoss: 0.39147379994392395,Accuracy:0.8830625000000001,Validation Loss:30140.27734375,Validation Accuracy:0.91, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [6/4000],Loss:30117.41015625, KL Loss: 3011706.5. FitLoss: 0.3444075882434845,Accuracy:0.8938625000000006,Validation Loss:30087.943359375,Validation Accuracy:0.921, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [7/4000],Loss:30065.05078125, KL Loss: 3006476.5. FitLoss: 0.28970837593078613,Accuracy:0.9121499999999999,Validation Loss:30035.603515625,Validation Accuracy:0.928, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [8/4000],Loss:30012.69921875, KL Loss: 3001243.75. FitLoss: 0.2602691650390625,Accuracy:0.9225375000000005,Validation Loss:29983.25,Validation Accuracy:0.933, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [9/4000],Loss:29960.3359375, KL Loss: 2996009.0. FitLoss: 0.24245715141296387,Accuracy:0.9256875000000004,Validation Loss:29930.861328125,Validation Accuracy:0.938, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [10/4000],Loss:29907.935546875, KL Loss: 2990772.0. FitLoss: 0.21635720133781433,Accuracy:0.9361624999999997,Validation Loss:29878.458984375,Validation Accuracy:0.946, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [11/4000],Loss:29855.5234375, KL Loss: 2985533.0. FitLoss: 0.19440095126628876,Accuracy:0.9434999999999997,Validation Loss:29826.037109375,Validation Accuracy:0.951, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [12/4000],Loss:29803.103515625, KL Loss: 2980291.75. FitLoss: 0.18647801876068115,Accuracy:0.9453499999999998,Validation Loss:29773.599609375,Validation Accuracy:0.953, Prune parameters: 0.0/421642,Beta: 0.01\n",
      "Epoch [13/4000],Loss:29750.654296875, KL Loss: 2975047.75. FitLoss: 0.1753680408000946,Accuracy:0.9489249999999997,Validation Loss:29721.140625,Validation Accuracy:0.957, Prune parameters: 890.0/421642,Beta: 0.01\n",
      "Epoch [14/4000],Loss:29698.181640625, KL Loss: 2969802.25. FitLoss: 0.16079740226268768,Accuracy:0.9538125000000001,Validation Loss:29668.671875,Validation Accuracy:0.958, Prune parameters: 2583.0/421642,Beta: 0.01\n",
      "Epoch [15/4000],Loss:29645.693359375, KL Loss: 2964554.25. FitLoss: 0.1505102664232254,Accuracy:0.9576500000000001,Validation Loss:29616.18359375,Validation Accuracy:0.954, Prune parameters: 4332.0/421642,Beta: 0.01\n",
      "Epoch [16/4000],Loss:29593.1796875, KL Loss: 2959303.5. FitLoss: 0.1439550817012787,Accuracy:0.9596000000000002,Validation Loss:29563.63671875,Validation Accuracy:0.96, Prune parameters: 6004.0/421642,Beta: 0.01\n",
      "Epoch [17/4000],Loss:29540.646484375, KL Loss: 2954051.0. FitLoss: 0.1394461691379547,Accuracy:0.9604250000000001,Validation Loss:29511.087890625,Validation Accuracy:0.965, Prune parameters: 7746.0/421642,Beta: 0.01\n",
      "Epoch [18/4000],Loss:29488.08203125, KL Loss: 2948795.5. FitLoss: 0.1282508671283722,Accuracy:0.9637374999999999,Validation Loss:29458.53125,Validation Accuracy:0.96, Prune parameters: 9428.0/421642,Beta: 0.01\n",
      "Epoch [19/4000],Loss:29435.50390625, KL Loss: 2943538.0. FitLoss: 0.12572011351585388,Accuracy:0.9643374999999997,Validation Loss:29405.93359375,Validation Accuracy:0.966, Prune parameters: 11082.0/421642,Beta: 0.01\n",
      "Epoch [20/4000],Loss:29382.8984375, KL Loss: 2938278.0. FitLoss: 0.12099774926900864,Accuracy:0.9663125000000002,Validation Loss:29353.33203125,Validation Accuracy:0.958, Prune parameters: 12749.0/421642,Beta: 0.01\n",
      "Epoch [21/4000],Loss:29330.27734375, KL Loss: 2933015.5. FitLoss: 0.12149674445390701,Accuracy:0.965475,Validation Loss:29300.673828125,Validation Accuracy:0.969, Prune parameters: 14400.0/421642,Beta: 0.01\n",
      "Epoch [22/4000],Loss:29277.623046875, KL Loss: 2927751.0. FitLoss: 0.11223123967647552,Accuracy:0.9685374999999995,Validation Loss:29248.02734375,Validation Accuracy:0.964, Prune parameters: 16115.0/421642,Beta: 0.01\n",
      "Epoch [23/4000],Loss:29224.95703125, KL Loss: 2922484.0. FitLoss: 0.11952126026153564,Accuracy:0.9663999999999998,Validation Loss:29195.337890625,Validation Accuracy:0.964, Prune parameters: 17820.0/421642,Beta: 0.01\n",
      "Epoch [24/4000],Loss:29172.25, KL Loss: 2917214.5. FitLoss: 0.11112736165523529,Accuracy:0.9689499999999998,Validation Loss:29142.6171875,Validation Accuracy:0.969, Prune parameters: 19600.0/421642,Beta: 0.01\n",
      "Epoch [25/4000],Loss:29119.5234375, KL Loss: 2911942.0. FitLoss: 0.1021934449672699,Accuracy:0.9713624999999999,Validation Loss:29089.87890625,Validation Accuracy:0.972, Prune parameters: 21307.0/421642,Beta: 0.01\n",
      "Epoch [26/4000],Loss:29066.7890625, KL Loss: 2906668.0. FitLoss: 0.1102665588259697,Accuracy:0.9684374999999996,Validation Loss:29037.11328125,Validation Accuracy:0.972, Prune parameters: 23030.0/421642,Beta: 0.01\n",
      "Epoch [27/4000],Loss:24483.796875, KL Loss: 2901395.0. FitLoss: 0.10515739023685455,Accuracy:0.9700875000000002,Validation Loss:3623.281982421875,Validation Accuracy:0.968, Prune parameters: 24719.0/421642,Beta: 0.00125\n",
      "Epoch [28/4000],Loss:905.736572265625, KL Loss: 2896969.0. FitLoss: 0.09832525253295898,Accuracy:0.9715749999999994,Validation Loss:56.65169906616211,Validation Accuracy:0.975, Prune parameters: 25737.0/421642,Beta: 1.953125e-05\n",
      "Epoch [29/4000],Loss:56.620235443115234, KL Loss: 2894730.5. FitLoss: 0.08252664655447006,Accuracy:0.9758624999999999,Validation Loss:56.61988830566406,Validation Accuracy:0.972, Prune parameters: 26187.0/421642,Beta: 1.953125e-05\n",
      "Epoch [30/4000],Loss:56.59083938598633, KL Loss: 2893741.5. FitLoss: 0.07245390862226486,Accuracy:0.9782249999999996,Validation Loss:56.600990295410156,Validation Accuracy:0.976, Prune parameters: 26403.0/421642,Beta: 1.953125e-05\n",
      "Epoch [31/4000],Loss:240.22271728515625, KL Loss: 2893302.0. FitLoss: 0.06221586838364601,Accuracy:0.9813249999999997,Validation Loss:1808.3074951171875,Validation Accuracy:0.979, Prune parameters: 26488.0/421642,Beta: 0.000625\n",
      "Epoch [32/4000],Loss:57621.75, KL Loss: 2892851.25. FitLoss: 0.05559420958161354,Accuracy:0.9833374999999996,Validation Loss:231292.109375,Validation Accuracy:0.983, Prune parameters: 27120.0/421642,Beta: 0.08\n",
      "Epoch [33/4000],Loss:230885.390625, KL Loss: 2886066.5. FitLoss: 0.08090046048164368,Accuracy:0.9783249999999997,Validation Loss:230267.640625,Validation Accuracy:0.967, Prune parameters: 31189.0/421642,Beta: 0.08\n",
      "Epoch [34/4000],Loss:193909.34375, KL Loss: 2871624.0. FitLoss: 0.21616584062576294,Accuracy:0.9568624999999997,Validation Loss:28632.150390625,Validation Accuracy:0.946, Prune parameters: 35947.0/421642,Beta: 0.01\n",
      "Epoch [35/4000],Loss:7126.74462890625, KL Loss: 2858535.5. FitLoss: 0.2860267758369446,Accuracy:0.9307749999999995,Validation Loss:111.71949005126953,Validation Accuracy:0.936, Prune parameters: 38910.0/421642,Beta: 3.90625e-05\n",
      "Epoch [36/4000],Loss:27.978914260864258, KL Loss: 2851870.75. FitLoss: 0.22353602945804596,Accuracy:0.9316249999999997,Validation Loss:0.6506418585777283,Validation Accuracy:0.943, Prune parameters: 40231.0/421642,Beta: 1.52587890625e-07\n",
      "Epoch [37/4000],Loss:0.2977147698402405, KL Loss: 2848947.5. FitLoss: 0.16863881051540375,Accuracy:0.9468000000000002,Validation Loss:0.21163706481456757,Validation Accuracy:0.954, Prune parameters: 40814.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [38/4000],Loss:0.1854047179222107, KL Loss: 2847670.25. FitLoss: 0.1310897320508957,Accuracy:0.9577749999999996,Validation Loss:0.18930912017822266,Validation Accuracy:0.964, Prune parameters: 41066.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [39/4000],Loss:0.17642854154109955, KL Loss: 2847112.5. FitLoss: 0.10854867845773697,Accuracy:0.9655124999999994,Validation Loss:0.23056495189666748,Validation Accuracy:0.966, Prune parameters: 41152.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [40/4000],Loss:0.20408250391483307, KL Loss: 2846869.25. FitLoss: 0.09548306465148926,Accuracy:0.9691749999999997,Validation Loss:0.2105483114719391,Validation Accuracy:0.974, Prune parameters: 41198.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [41/4000],Loss:0.18642884492874146, KL Loss: 2846764.0. FitLoss: 0.07783342897891998,Accuracy:0.9754624999999993,Validation Loss:0.20599550008773804,Validation Accuracy:0.976, Prune parameters: 41221.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [42/4000],Loss:0.17813825607299805, KL Loss: 2846718.0. FitLoss: 0.06954459100961685,Accuracy:0.9785499999999996,Validation Loss:0.19824206829071045,Validation Accuracy:0.976, Prune parameters: 41230.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [43/4000],Loss:0.18221062421798706, KL Loss: 2846698.75. FitLoss: 0.06004360318183899,Accuracy:0.9811999999999992,Validation Loss:0.5271052122116089,Validation Accuracy:0.972, Prune parameters: 41235.0/421642,Beta: 1.52587890625e-07\n",
      "Epoch [44/4000],Loss:13.902397155761719, KL Loss: 2846690.75. FitLoss: 0.05684417113661766,Accuracy:0.9818749999999994,Validation Loss:111.28253936767578,Validation Accuracy:0.979, Prune parameters: 41237.0/421642,Beta: 3.90625e-05\n",
      "Epoch [45/4000],Loss:3544.48388671875, KL Loss: 2846680.0. FitLoss: 0.05140850320458412,Accuracy:0.9835249999999995,Validation Loss:28466.30859375,Validation Accuracy:0.98, Prune parameters: 41258.0/421642,Beta: 0.01\n",
      "Epoch [46/4000],Loss:280892.90625, KL Loss: 2845250.5. FitLoss: 0.05340699106454849,Accuracy:0.9839374999999995,Validation Loss:454488.4375,Validation Accuracy:0.976, Prune parameters: 43135.0/421642,Beta: 0.16\n",
      "Epoch [47/4000],Loss:453415.8125, KL Loss: 2833847.75. FitLoss: 0.1903509646654129,Accuracy:0.9657500000000002,Validation Loss:225964.984375,Validation Accuracy:0.925, Prune parameters: 48159.0/421642,Beta: 0.08\n",
      "Epoch [48/4000],Loss:56233.5625, KL Loss: 2818349.5. FitLoss: 0.49637502431869507,Accuracy:0.8808625000000001,Validation Loss:879.172119140625,Validation Accuracy:0.903, Prune parameters: 52189.0/421642,Beta: 0.0003125\n",
      "Epoch [49/4000],Loss:219.14796447753906, KL Loss: 2809268.0. FitLoss: 0.37935906648635864,Accuracy:0.8857625000000002,Validation Loss:3.7115440368652344,Validation Accuracy:0.928, Prune parameters: 53995.0/421642,Beta: 1.220703125e-06\n",
      "Epoch [50/4000],Loss:1.1073362827301025, KL Loss: 2805273.0. FitLoss: 0.2509777545928955,Accuracy:0.9256874999999999,Validation Loss:0.26561614871025085,Validation Accuracy:0.934, Prune parameters: 54772.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [51/4000],Loss:0.24631056189537048, KL Loss: 2803532.5. FitLoss: 0.1928374320268631,Accuracy:0.9402874999999996,Validation Loss:0.22240963578224182,Validation Accuracy:0.956, Prune parameters: 55086.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [52/4000],Loss:0.20942750573158264, KL Loss: 2802775.0. FitLoss: 0.1559688001871109,Accuracy:0.9506,Validation Loss:0.19999544322490692,Validation Accuracy:0.963, Prune parameters: 55237.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [53/4000],Loss:0.22689056396484375, KL Loss: 2802445.0. FitLoss: 0.1333492398262024,Accuracy:0.9572624999999999,Validation Loss:0.23659569025039673,Validation Accuracy:0.965, Prune parameters: 55302.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [54/4000],Loss:0.22091954946517944, KL Loss: 2802301.75. FitLoss: 0.1140202060341835,Accuracy:0.9633750000000001,Validation Loss:0.2233748883008957,Validation Accuracy:0.973, Prune parameters: 55332.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [55/4000],Loss:0.20764070749282837, KL Loss: 2802240.0. FitLoss: 0.10074372589588165,Accuracy:0.9678874999999996,Validation Loss:0.21897046267986298,Validation Accuracy:0.971, Prune parameters: 55343.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [56/4000],Loss:0.20329315960407257, KL Loss: 2802213.5. FitLoss: 0.09639719873666763,Accuracy:0.969625,Validation Loss:0.21003493666648865,Validation Accuracy:0.974, Prune parameters: 55346.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [57/4000],Loss:0.19210536777973175, KL Loss: 2802202.5. FitLoss: 0.08520982414484024,Accuracy:0.9723249999999997,Validation Loss:0.20608210563659668,Validation Accuracy:0.975, Prune parameters: 55347.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [58/4000],Loss:0.1809510737657547, KL Loss: 2802198.0. FitLoss: 0.07405570149421692,Accuracy:0.9766874999999995,Validation Loss:0.2026793658733368,Validation Accuracy:0.975, Prune parameters: 55347.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [59/4000],Loss:0.1756707727909088, KL Loss: 2802196.25. FitLoss: 0.06877546012401581,Accuracy:0.9782499999999995,Validation Loss:0.2037978172302246,Validation Accuracy:0.972, Prune parameters: 55348.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [60/4000],Loss:0.1761106699705124, KL Loss: 2802196.0. FitLoss: 0.06921537220478058,Accuracy:0.9772999999999994,Validation Loss:0.19811448454856873,Validation Accuracy:0.979, Prune parameters: 55348.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [61/4000],Loss:0.17148706316947937, KL Loss: 2802197.0. FitLoss: 0.06459174305200577,Accuracy:0.9796624999999999,Validation Loss:0.19310879707336426,Validation Accuracy:0.98, Prune parameters: 55348.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [62/4000],Loss:0.1638994663953781, KL Loss: 2802197.75. FitLoss: 0.05700410529971123,Accuracy:0.9815999999999996,Validation Loss:0.2001221477985382,Validation Accuracy:0.97, Prune parameters: 55348.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [63/4000],Loss:0.17167192697525024, KL Loss: 2802199.0. FitLoss: 0.051414597779512405,Accuracy:0.9841124999999995,Validation Loss:0.5110198855400085,Validation Accuracy:0.981, Prune parameters: 55348.0/421642,Beta: 1.52587890625e-07\n",
      "Epoch [64/4000],Loss:13.674674987792969, KL Loss: 2802199.75. FitLoss: 0.045508772134780884,Accuracy:0.9857749999999996,Validation Loss:109.54413604736328,Validation Accuracy:0.98, Prune parameters: 55348.0/421642,Beta: 3.90625e-05\n",
      "Epoch [65/4000],Loss:3489.091796875, KL Loss: 2802195.25. FitLoss: 0.040618397295475006,Accuracy:0.9874374999999995,Validation Loss:28021.68359375,Validation Accuracy:0.982, Prune parameters: 55355.0/421642,Beta: 0.01\n",
      "Epoch [66/4000],Loss:276568.75, KL Loss: 2801223.5. FitLoss: 0.04084009677171707,Accuracy:0.9877624999999999,Validation Loss:447639.375,Validation Accuracy:0.978, Prune parameters: 56779.0/421642,Beta: 0.16\n",
      "Epoch [67/4000],Loss:446736.8125, KL Loss: 2792104.75. FitLoss: 0.12817969918251038,Accuracy:0.9730374999999997,Validation Loss:445428.90625,Validation Accuracy:0.958, Prune parameters: 61070.0/421642,Beta: 0.16\n",
      "Epoch [68/4000],Loss:220708.296875, KL Loss: 2777158.75. FitLoss: 0.425518274307251,Accuracy:0.9192500000000006,Validation Loss:6924.36669921875,Validation Accuracy:0.919, Prune parameters: 65641.0/421642,Beta: 0.0025\n",
      "Epoch [69/4000],Loss:1723.9029541015625, KL Loss: 2766000.0. FitLoss: 0.358375608921051,Accuracy:0.9041125000000001,Validation Loss:27.229761123657227,Validation Accuracy:0.94, Prune parameters: 67951.0/421642,Beta: 9.765625e-06\n",
      "Epoch [70/4000],Loss:6.961644649505615, KL Loss: 2760912.25. FitLoss: 0.24483570456504822,Accuracy:0.9261000000000003,Validation Loss:0.302947461605072,Validation Accuracy:0.944, Prune parameters: 68959.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [71/4000],Loss:0.24636872112751007, KL Loss: 2758701.5. FitLoss: 0.1871718019247055,Accuracy:0.9413124999999998,Validation Loss:0.21385838091373444,Validation Accuracy:0.955, Prune parameters: 69351.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [72/4000],Loss:0.20536574721336365, KL Loss: 2757741.25. FitLoss: 0.15276601910591125,Accuracy:0.9514249999999997,Validation Loss:0.19498851895332336,Validation Accuracy:0.969, Prune parameters: 69528.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [73/4000],Loss:0.2084367871284485, KL Loss: 2757324.5. FitLoss: 0.12954974174499512,Accuracy:0.9582374999999997,Validation Loss:0.23329004645347595,Validation Accuracy:0.965, Prune parameters: 69593.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [74/4000],Loss:0.21703949570655823, KL Loss: 2757144.0. FitLoss: 0.11186277866363525,Accuracy:0.964325,Validation Loss:0.21713215112686157,Validation Accuracy:0.973, Prune parameters: 69620.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [75/4000],Loss:0.20529431104660034, KL Loss: 2757065.75. FitLoss: 0.10012060403823853,Accuracy:0.9678500000000003,Validation Loss:0.21644212305545807,Validation Accuracy:0.968, Prune parameters: 69639.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [76/4000],Loss:0.19568631052970886, KL Loss: 2757032.5. FitLoss: 0.09051387757062912,Accuracy:0.9703624999999996,Validation Loss:0.21919596195220947,Validation Accuracy:0.971, Prune parameters: 69645.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [77/4000],Loss:0.1950211077928543, KL Loss: 2757018.0. FitLoss: 0.08984920382499695,Accuracy:0.9705124999999992,Validation Loss:0.20713648200035095,Validation Accuracy:0.972, Prune parameters: 69647.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [78/4000],Loss:0.18367737531661987, KL Loss: 2757012.5. FitLoss: 0.07850570976734161,Accuracy:0.9746749999999993,Validation Loss:0.19742976129055023,Validation Accuracy:0.976, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [79/4000],Loss:0.18063698709011078, KL Loss: 2757010.25. FitLoss: 0.07546539604663849,Accuracy:0.9743124999999994,Validation Loss:0.20173029601573944,Validation Accuracy:0.973, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [80/4000],Loss:0.1729581654071808, KL Loss: 2757009.75. FitLoss: 0.06778661161661148,Accuracy:0.9771124999999994,Validation Loss:0.20083516836166382,Validation Accuracy:0.975, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [81/4000],Loss:0.16714781522750854, KL Loss: 2757010.0. FitLoss: 0.061976224184036255,Accuracy:0.9801374999999997,Validation Loss:0.19852760434150696,Validation Accuracy:0.973, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [82/4000],Loss:0.16441290080547333, KL Loss: 2757011.0. FitLoss: 0.05924128368496895,Accuracy:0.9804874999999994,Validation Loss:0.19260276854038239,Validation Accuracy:0.98, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [83/4000],Loss:0.157108873128891, KL Loss: 2757011.25. FitLoss: 0.05193724483251572,Accuracy:0.9826624999999998,Validation Loss:0.18746154010295868,Validation Accuracy:0.98, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [84/4000],Loss:0.15408605337142944, KL Loss: 2757012.0. FitLoss: 0.04891440272331238,Accuracy:0.9843,Validation Loss:0.18781688809394836,Validation Accuracy:0.982, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [85/4000],Loss:0.14777441322803497, KL Loss: 2757012.5. FitLoss: 0.04260274022817612,Accuracy:0.9862124999999997,Validation Loss:0.18541160225868225,Validation Accuracy:0.98, Prune parameters: 69650.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [86/4000],Loss:1.7231332063674927, KL Loss: 2757013.5. FitLoss: 0.04038572311401367,Accuracy:0.9875249999999992,Validation Loss:13.544378280639648,Validation Accuracy:0.982, Prune parameters: 69650.0/421642,Beta: 4.8828125e-06\n",
      "Epoch [87/4000],Loss:429.1397399902344, KL Loss: 2757013.5. FitLoss: 0.03925337642431259,Accuracy:0.9874374999999995,Validation Loss:3446.339111328125,Validation Accuracy:0.982, Prune parameters: 69652.0/421642,Beta: 0.00125\n",
      "Epoch [88/4000],Loss:54704.94140625, KL Loss: 2756888.5. FitLoss: 0.03670114278793335,Accuracy:0.9886624999999997,Validation Loss:110251.8984375,Validation Accuracy:0.982, Prune parameters: 69872.0/421642,Beta: 0.04\n",
      "Epoch [89/4000],Loss:41122.97265625, KL Loss: 2755173.5. FitLoss: 0.0429149828851223,Accuracy:0.9869,Validation Loss:860.6678466796875,Validation Accuracy:0.985, Prune parameters: 70641.0/421642,Beta: 0.0003125\n",
      "Epoch [90/4000],Loss:214.34051513671875, KL Loss: 2753289.25. FitLoss: 0.047433048486709595,Accuracy:0.9863124999999998,Validation Loss:3.4422359466552734,Validation Accuracy:0.982, Prune parameters: 71025.0/421642,Beta: 1.220703125e-06\n",
      "Epoch [91/4000],Loss:0.8929052948951721, KL Loss: 2752433.5. FitLoss: 0.05287442356348038,Accuracy:0.9839999999999997,Validation Loss:0.13397282361984253,Validation Accuracy:0.982, Prune parameters: 71183.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [92/4000],Loss:0.13368144631385803, KL Loss: 2752062.75. FitLoss: 0.04838341102004051,Accuracy:0.9852000000000001,Validation Loss:0.18965837359428406,Validation Accuracy:0.979, Prune parameters: 71261.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [93/4000],Loss:0.1507258266210556, KL Loss: 2751901.75. FitLoss: 0.04574910178780556,Accuracy:0.9862999999999997,Validation Loss:0.1807771772146225,Validation Accuracy:0.979, Prune parameters: 71283.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [94/4000],Loss:0.15214590728282928, KL Loss: 2751832.75. FitLoss: 0.04717182368040085,Accuracy:0.9852249999999995,Validation Loss:0.182599276304245,Validation Accuracy:0.98, Prune parameters: 71295.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [95/4000],Loss:0.14651909470558167, KL Loss: 2751803.25. FitLoss: 0.04154614359140396,Accuracy:0.9871624999999995,Validation Loss:0.18089890480041504,Validation Accuracy:0.979, Prune parameters: 71300.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [96/4000],Loss:0.14260730147361755, KL Loss: 2751790.75. FitLoss: 0.037634823471307755,Accuracy:0.9884499999999997,Validation Loss:0.17810353636741638,Validation Accuracy:0.984, Prune parameters: 71303.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [97/4000],Loss:0.15092779695987701, KL Loss: 2751785.5. FitLoss: 0.03283398225903511,Accuracy:0.9902999999999997,Validation Loss:0.49915727972984314,Validation Accuracy:0.983, Prune parameters: 71305.0/421642,Beta: 1.52587890625e-07\n",
      "Epoch [98/4000],Loss:13.414241790771484, KL Loss: 2751783.5. FitLoss: 0.030288569629192352,Accuracy:0.99095,Validation Loss:107.5636215209961,Validation Accuracy:0.981, Prune parameters: 71305.0/421642,Beta: 3.90625e-05\n",
      "Epoch [99/4000],Loss:3426.306640625, KL Loss: 2751778.0. FitLoss: 0.03021513856947422,Accuracy:0.9908000000000003,Validation Loss:27517.53125,Validation Accuracy:0.985, Prune parameters: 71314.0/421642,Beta: 0.01\n",
      "Epoch [100/4000],Loss:271600.40625, KL Loss: 2750874.75. FitLoss: 0.029883217066526413,Accuracy:0.9913249999999998,Validation Loss:439613.6875,Validation Accuracy:0.986, Prune parameters: 72655.0/421642,Beta: 0.16\n",
      "Epoch [101/4000],Loss:438729.78125, KL Loss: 2742060.5. FitLoss: 0.09654019773006439,Accuracy:0.9804624999999991,Validation Loss:437429.21875,Validation Accuracy:0.972, Prune parameters: 77175.0/421642,Beta: 0.16\n",
      "Epoch [102/4000],Loss:320630.4375, KL Loss: 2726789.0. FitLoss: 0.3586614727973938,Accuracy:0.9281874999999997,Validation Loss:27180.576171875,Validation Accuracy:0.94, Prune parameters: 82261.0/421642,Beta: 0.01\n",
      "Epoch [103/4000],Loss:6765.39892578125, KL Loss: 2713480.5. FitLoss: 0.36120593547821045,Accuracy:0.9054874999999999,Validation Loss:106.08695220947266,Validation Accuracy:0.927, Prune parameters: 85131.0/421642,Beta: 3.90625e-05\n",
      "Epoch [104/4000],Loss:26.591808319091797, KL Loss: 2707031.5. FitLoss: 0.24588246643543243,Accuracy:0.9247249999999999,Validation Loss:0.6098611950874329,Validation Accuracy:0.949, Prune parameters: 86445.0/421642,Beta: 1.52587890625e-07\n",
      "Epoch [105/4000],Loss:0.30402207374572754, KL Loss: 2704232.0. FitLoss: 0.1815025955438614,Accuracy:0.9439999999999997,Validation Loss:0.21350452303886414,Validation Accuracy:0.96, Prune parameters: 86943.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [106/4000],Loss:0.19865866005420685, KL Loss: 2703018.5. FitLoss: 0.14710265398025513,Accuracy:0.9535375000000001,Validation Loss:0.19062668085098267,Validation Accuracy:0.965, Prune parameters: 87162.0/421642,Beta: 1.9073486328125e-08\n",
      "Epoch [107/4000],Loss:0.19478583335876465, KL Loss: 2702493.0. FitLoss: 0.1303539276123047,Accuracy:0.9578375000000001,Validation Loss:0.22315385937690735,Validation Accuracy:0.972, Prune parameters: 87264.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [108/4000],Loss:0.21567201614379883, KL Loss: 2702265.5. FitLoss: 0.11258876323699951,Accuracy:0.9633624999999999,Validation Loss:0.2242487221956253,Validation Accuracy:0.969, Prune parameters: 87314.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [109/4000],Loss:0.20259182155132294, KL Loss: 2702166.75. FitLoss: 0.09951233863830566,Accuracy:0.9666749999999997,Validation Loss:0.21022287011146545,Validation Accuracy:0.976, Prune parameters: 87332.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [110/4000],Loss:0.19312463700771332, KL Loss: 2702124.5. FitLoss: 0.09004676342010498,Accuracy:0.9704374999999995,Validation Loss:0.20763373374938965,Validation Accuracy:0.972, Prune parameters: 87336.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [111/4000],Loss:0.184549018740654, KL Loss: 2702106.25. FitLoss: 0.08147184550762177,Accuracy:0.9729374999999993,Validation Loss:0.19990618526935577,Validation Accuracy:0.976, Prune parameters: 87340.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [112/4000],Loss:0.1800352782011032, KL Loss: 2702099.0. FitLoss: 0.07695838809013367,Accuracy:0.9748499999999993,Validation Loss:0.19857710599899292,Validation Accuracy:0.974, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [113/4000],Loss:0.1726783961057663, KL Loss: 2702096.0. FitLoss: 0.06960160285234451,Accuracy:0.9769499999999995,Validation Loss:0.20127838850021362,Validation Accuracy:0.974, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [114/4000],Loss:0.16739027202129364, KL Loss: 2702094.75. FitLoss: 0.06431353837251663,Accuracy:0.9783999999999997,Validation Loss:0.19335117936134338,Validation Accuracy:0.977, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [115/4000],Loss:0.16488471627235413, KL Loss: 2702095.0. FitLoss: 0.06180797889828682,Accuracy:0.9800375000000001,Validation Loss:0.19179335236549377,Validation Accuracy:0.975, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [116/4000],Loss:0.1615561693906784, KL Loss: 2702095.5. FitLoss: 0.05847940593957901,Accuracy:0.9807249999999996,Validation Loss:0.18753203749656677,Validation Accuracy:0.978, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [117/4000],Loss:0.15845723450183868, KL Loss: 2702096.0. FitLoss: 0.055380452424287796,Accuracy:0.9812749999999992,Validation Loss:0.1839081346988678,Validation Accuracy:0.981, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [118/4000],Loss:0.15222255885601044, KL Loss: 2702096.25. FitLoss: 0.049145787954330444,Accuracy:0.9845374999999997,Validation Loss:0.1851346492767334,Validation Accuracy:0.982, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [119/4000],Loss:0.15396669507026672, KL Loss: 2702097.0. FitLoss: 0.050889886915683746,Accuracy:0.9826124999999998,Validation Loss:0.18649005889892578,Validation Accuracy:0.983, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [120/4000],Loss:0.15179137885570526, KL Loss: 2702097.5. FitLoss: 0.0487145371735096,Accuracy:0.9845124999999999,Validation Loss:0.18643437325954437,Validation Accuracy:0.978, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [121/4000],Loss:0.14448890089988708, KL Loss: 2702097.75. FitLoss: 0.04141205921769142,Accuracy:0.9863999999999994,Validation Loss:0.1813572645187378,Validation Accuracy:0.982, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [122/4000],Loss:0.1424279510974884, KL Loss: 2702098.25. FitLoss: 0.03935107961297035,Accuracy:0.9874624999999998,Validation Loss:0.1808464080095291,Validation Accuracy:0.983, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [123/4000],Loss:0.1395663470029831, KL Loss: 2702099.0. FitLoss: 0.03648944944143295,Accuracy:0.9883999999999993,Validation Loss:0.1808479130268097,Validation Accuracy:0.981, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [124/4000],Loss:0.13918305933475494, KL Loss: 2702099.5. FitLoss: 0.03610615432262421,Accuracy:0.9886249999999995,Validation Loss:0.17727482318878174,Validation Accuracy:0.985, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [125/4000],Loss:0.1354914754629135, KL Loss: 2702099.75. FitLoss: 0.032414548099040985,Accuracy:0.9895749999999996,Validation Loss:0.17618966102600098,Validation Accuracy:0.982, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [126/4000],Loss:0.13579727709293365, KL Loss: 2702100.0. FitLoss: 0.03272033855319023,Accuracy:0.989475,Validation Loss:0.1794431507587433,Validation Accuracy:0.982, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [127/4000],Loss:0.13562020659446716, KL Loss: 2702100.5. FitLoss: 0.03254324197769165,Accuracy:0.9892874999999999,Validation Loss:0.17537745833396912,Validation Accuracy:0.985, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [128/4000],Loss:0.13312488794326782, KL Loss: 2702101.0. FitLoss: 0.030047914013266563,Accuracy:0.9901999999999997,Validation Loss:0.17897000908851624,Validation Accuracy:0.985, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [129/4000],Loss:0.12995880842208862, KL Loss: 2702101.75. FitLoss: 0.026881814002990723,Accuracy:0.9915749999999995,Validation Loss:0.1719590723514557,Validation Accuracy:0.983, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [130/4000],Loss:0.12814345955848694, KL Loss: 2702102.0. FitLoss: 0.025066455826163292,Accuracy:0.9923499999999998,Validation Loss:0.17566309869289398,Validation Accuracy:0.984, Prune parameters: 87341.0/421642,Beta: 3.814697265625e-08\n",
      "Epoch [131/4000],Loss:0.46226823329925537, KL Loss: 2702102.25. FitLoss: 0.024190882220864296,Accuracy:0.9924249999999999,Validation Loss:3.371325969696045,Validation Accuracy:0.983, Prune parameters: 87341.0/421642,Beta: 1.220703125e-06\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=1000\n",
    "EPOCHS=4000\n",
    "LR = 1e-3 #5e-4\n",
    "# Split the training set into training and validation sets \n",
    "VAL_PERCENT = 0.2 # percentage of the data used for validation \n",
    "SAMPLES = 10\n",
    "BETA = 0.01 #5e-5\n",
    "BETA_FAC = 5e-1\n",
    "PRUNE = 1.9#1.99, 2.1\n",
    "PLATO_TOL = 20\n",
    "\n",
    "base_module = Classifier()\n",
    "var_module = LogUniformVarBayesModule(base_module)\n",
    "model = VarBayesModuleNet(base_module, nn.ModuleList([var_module]))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "fit_loss = nn.CrossEntropyLoss() \n",
    "kl_loss = LogUniformVarKLLoss()\n",
    "\n",
    "beta = Beta_Scheduler_Plato(BETA, BETA_FAC, PLATO_TOL)\n",
    "beta_KL = Beta_Scheduler_Plato(beta.beta, 1 / BETA_FAC, PLATO_TOL, ref = beta, threshold=1e-4)\n",
    "\n",
    "#Данная функция будет выполнятся после каждого шага тренера, соответсвенно нам требуется сделать шаг планировщика и изменить соотвествующий коэффициент\n",
    "def post_train_step(trainer: VarTrainerParams, train_result: VarBayesTrainer.TrainResult):\n",
    "    beta.step(train_result.fit_loss)\n",
    "    beta_KL.step(train_result.dist_loss)\n",
    "    trainer.params.beta = float(beta)\n",
    "    \n",
    "#print(model.base_module.state_dict().keys())\n",
    "val_size    = int(VAL_PERCENT * len(train_dataset)) \n",
    "train_size  = len(train_dataset) - val_size \n",
    "\n",
    "t_dataset, v_dataset = torch.utils.data.random_split(train_dataset,  \n",
    "                                                        [train_size,  \n",
    "                                                            val_size]) \n",
    "\n",
    "# Create DataLoaders for the training and validation sets \n",
    "train_loader = torch.utils.data.DataLoader(t_dataset,  \n",
    "                                        batch_size=BATCH_SIZE,  \n",
    "                                        shuffle=True, \n",
    "                                        pin_memory=True) \n",
    "eval_loader = torch.utils.data.DataLoader(v_dataset,  \n",
    "                                        batch_size=BATCH_SIZE,  \n",
    "                                        shuffle=False, \n",
    "                                        pin_memory=True) \n",
    "\n",
    "model.to(device) \n",
    "train_params = VarTrainerParams(EPOCHS, optimizer,fit_loss, kl_loss, SAMPLES, PRUNE, BETA, {'accuracy': CallbackLossAccuracy()})\n",
    "trainer = VarBayesTrainer(train_params, ReportChain([VarBaseReport()]), train_loader, eval_loader, [post_train_step])\n",
    "trainer.train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_module.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_bayes.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(409356., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.prune({'threshold': 1.9})\n",
    "print(model.prune_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(47343., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.prune([{'threshold': -2.2}])\n",
    "print(model.prune_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = Classifier()\n",
    "var_module = LogUniformVarBayesModule(module)\n",
    "model = VarBayesModuleNet(module, nn.ModuleList([var_module]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('module_list.0.posterior_params.0.param_mus',\n",
       "              tensor([[[[-0.2737, -0.0213, -0.1784],\n",
       "                        [-0.0449,  0.1339,  0.2884],\n",
       "                        [ 0.1757,  0.2454,  0.0318]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2829,  0.1125,  0.2672],\n",
       "                        [-0.0329,  0.1954,  0.2326],\n",
       "                        [-0.0857, -0.2534,  0.0893]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1525,  0.0669, -0.2533],\n",
       "                        [-0.1758,  0.1672,  0.1206],\n",
       "                        [ 0.0153,  0.1936, -0.3305]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0107,  0.3182,  0.2102],\n",
       "                        [-0.1029,  0.3132, -0.2125],\n",
       "                        [-0.2650,  0.0061,  0.2422]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1922,  0.1445, -0.1445],\n",
       "                        [-0.1862, -0.1703,  0.1814],\n",
       "                        [-0.3061, -0.1629,  0.1818]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2469, -0.2962, -0.1922],\n",
       "                        [-0.1502, -0.0190, -0.1009],\n",
       "                        [ 0.1418,  0.0435, -0.2017]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2999,  0.2396,  0.0861],\n",
       "                        [-0.1399,  0.0275, -0.3024],\n",
       "                        [-0.1098, -0.1394, -0.2101]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0273,  0.2970, -0.2747],\n",
       "                        [ 0.1127,  0.2224,  0.0496],\n",
       "                        [-0.2127,  0.1349, -0.2917]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0222,  0.2411,  0.0425],\n",
       "                        [ 0.0817,  0.1037,  0.2716],\n",
       "                        [ 0.0878, -0.2382, -0.0517]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0719,  0.1895,  0.1076],\n",
       "                        [-0.1426, -0.1827,  0.3090],\n",
       "                        [-0.1734,  0.0608, -0.1394]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2088,  0.1865,  0.1586],\n",
       "                        [-0.1817, -0.1410, -0.1049],\n",
       "                        [-0.1832, -0.3061, -0.3109]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2269,  0.1814, -0.3188],\n",
       "                        [ 0.2725, -0.0902, -0.2374],\n",
       "                        [-0.1628, -0.2985, -0.0739]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2382, -0.0886,  0.2284],\n",
       "                        [ 0.0932, -0.0300,  0.1199],\n",
       "                        [-0.3036,  0.0679,  0.3242]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2995, -0.0269,  0.2335],\n",
       "                        [-0.2868,  0.1297,  0.2263],\n",
       "                        [-0.1365,  0.0827, -0.1368]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2408, -0.1476,  0.1649],\n",
       "                        [ 0.1636, -0.0114, -0.1543],\n",
       "                        [ 0.1516,  0.2401, -0.3093]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0533,  0.1382,  0.0024],\n",
       "                        [-0.1546,  0.2536, -0.2555],\n",
       "                        [-0.1677, -0.0555,  0.0449]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2179,  0.0416,  0.1664],\n",
       "                        [ 0.1895,  0.1240, -0.2714],\n",
       "                        [-0.0561, -0.2904,  0.1815]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0931, -0.2447, -0.1122],\n",
       "                        [ 0.2836, -0.3144,  0.2693],\n",
       "                        [ 0.0475,  0.0702,  0.2088]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2935, -0.0657,  0.2059],\n",
       "                        [-0.2644,  0.2140,  0.0178],\n",
       "                        [-0.0679, -0.2655, -0.0174]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1912, -0.0838, -0.1078],\n",
       "                        [ 0.2290,  0.2124, -0.2783],\n",
       "                        [-0.0186,  0.2010, -0.1156]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2628, -0.2447,  0.0938],\n",
       "                        [-0.1858, -0.2919, -0.2134],\n",
       "                        [ 0.2410, -0.0449,  0.1174]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3145, -0.2647,  0.2348],\n",
       "                        [ 0.1423, -0.1619, -0.1704],\n",
       "                        [ 0.3221, -0.1983, -0.0801]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0133, -0.3084,  0.1228],\n",
       "                        [ 0.1514,  0.0796, -0.1055],\n",
       "                        [ 0.2269,  0.1001, -0.2295]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1243,  0.1812,  0.0307],\n",
       "                        [ 0.2902,  0.2480, -0.0722],\n",
       "                        [-0.1125,  0.2348,  0.0912]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2855,  0.0356,  0.2712],\n",
       "                        [ 0.3197, -0.0580,  0.0020],\n",
       "                        [-0.2933, -0.1005,  0.2072]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1050, -0.2946,  0.0811],\n",
       "                        [-0.0863,  0.2160, -0.2627],\n",
       "                        [-0.1286,  0.0596, -0.1345]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0546, -0.0362,  0.0594],\n",
       "                        [ 0.0646,  0.1745, -0.0352],\n",
       "                        [-0.1120, -0.0474,  0.0383]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1332, -0.2500, -0.1542],\n",
       "                        [-0.2412, -0.3166, -0.0210],\n",
       "                        [ 0.1060,  0.1158,  0.0057]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2678, -0.2166,  0.0712],\n",
       "                        [-0.0535,  0.1413,  0.2406],\n",
       "                        [-0.2855, -0.3059, -0.2075]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2055, -0.2482,  0.1404],\n",
       "                        [-0.2513, -0.1904,  0.1103],\n",
       "                        [-0.2845,  0.0184,  0.2082]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0582,  0.2715,  0.1728],\n",
       "                        [-0.1080,  0.1155,  0.1371],\n",
       "                        [ 0.2162, -0.0247, -0.2244]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1486,  0.0659,  0.1559],\n",
       "                        [-0.2081,  0.0449,  0.3216],\n",
       "                        [ 0.2564,  0.2344, -0.0866]]]])),\n",
       "             ('module_list.0.posterior_params.0.param_std_log',\n",
       "              tensor([[[[-7.6443, -6.8122, -6.2178],\n",
       "                        [-4.6537, -8.1042, -5.0853],\n",
       "                        [-6.9559, -4.9253, -4.6574]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.9750, -6.1978, -5.7079],\n",
       "                        [-5.8540, -7.7156, -6.2078],\n",
       "                        [-4.9556, -5.0383, -6.1048]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7616, -5.4980, -4.7636],\n",
       "                        [-4.9685, -8.0552, -6.3784],\n",
       "                        [-4.9320, -6.9730, -6.1062]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8593, -5.2212, -4.8359],\n",
       "                        [-4.9786, -7.6505, -5.0176],\n",
       "                        [-5.6760, -5.2133, -5.7229]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.5549, -5.6953, -7.5045],\n",
       "                        [-4.8243, -5.2325, -8.2682],\n",
       "                        [-4.7497, -5.2409, -6.0584]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2319, -5.1681, -5.0143],\n",
       "                        [-4.6465, -4.7084, -6.2273],\n",
       "                        [-4.7883, -5.2719, -4.9517]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.0582, -5.2466, -5.4416],\n",
       "                        [-5.1359, -5.2855, -5.4410],\n",
       "                        [-5.7903, -4.6311, -5.1965]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9802, -4.6867, -5.9103],\n",
       "                        [-4.9393, -7.2178, -5.2489],\n",
       "                        [-4.6243, -5.9567, -5.4351]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6969, -5.5580, -5.1405],\n",
       "                        [-4.7906, -5.6145, -7.0589],\n",
       "                        [-5.0275, -4.7744, -5.2552]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2480, -6.9430, -4.8825],\n",
       "                        [-5.6597, -6.6762, -4.9689],\n",
       "                        [-5.1752, -4.9931, -5.0197]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6701, -4.8942, -5.5445],\n",
       "                        [-5.9894, -5.5842, -4.8125],\n",
       "                        [-5.2390, -4.8248, -6.3544]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0899, -6.3075, -5.0728],\n",
       "                        [-4.7244, -4.7276, -5.5322],\n",
       "                        [-6.5785, -5.2176, -8.2081]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.8461, -6.0238, -5.0947],\n",
       "                        [-4.7004, -4.7245, -4.9359],\n",
       "                        [-5.4876, -4.8804, -5.2382]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7568, -4.7962, -4.8076],\n",
       "                        [-4.8334, -4.6812, -4.8790],\n",
       "                        [-5.2112, -4.6500, -5.2285]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.1571, -5.5234, -5.3730],\n",
       "                        [-6.3554, -4.7353, -5.0411],\n",
       "                        [-4.7423, -5.0554, -5.8877]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.4057, -5.4768, -5.8319],\n",
       "                        [-4.8379, -4.8591, -7.9367],\n",
       "                        [-6.0085, -4.6697, -5.8908]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.7667, -5.2830, -4.8108],\n",
       "                        [-4.7277, -5.0035, -4.8743],\n",
       "                        [-4.7109, -7.0394, -4.8987]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.5578, -5.4441, -4.7811],\n",
       "                        [-4.6132, -5.0880, -4.7208],\n",
       "                        [-5.2930, -4.7753, -5.3517]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.0392, -6.9003, -6.4838],\n",
       "                        [-6.2912, -4.9652, -6.3513],\n",
       "                        [-5.4759, -4.9867, -5.3265]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.6965, -5.1672, -4.6054],\n",
       "                        [-5.8090, -4.9806, -5.4375],\n",
       "                        [-5.2273, -5.3135, -5.2484]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8862, -4.9312, -5.9119],\n",
       "                        [-5.3062, -5.0569, -4.6850],\n",
       "                        [-5.4883, -6.1027, -5.0738]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3343, -6.1922, -6.2508],\n",
       "                        [-5.0133, -4.8441, -6.0976],\n",
       "                        [-7.5900, -5.7515, -4.6156]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.5193, -6.1760, -5.7592],\n",
       "                        [-5.5590, -4.8624, -5.6229],\n",
       "                        [-4.9285, -4.8645, -5.8739]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6902, -5.6837, -4.9300],\n",
       "                        [-5.4640, -6.1872, -5.1738],\n",
       "                        [-4.6249, -4.7705, -4.9058]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.1386, -4.6787, -4.8687],\n",
       "                        [-4.7995, -6.2731, -5.5295],\n",
       "                        [-7.9139, -5.6268, -4.8398]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.4830, -5.1139, -5.3729],\n",
       "                        [-4.6113, -5.2906, -5.0130],\n",
       "                        [-5.5503, -4.9968, -4.6078]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.3845, -6.3526, -5.5192],\n",
       "                        [-6.8718, -5.3505, -5.9623],\n",
       "                        [-5.2600, -5.0851, -5.2559]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.5084, -6.8383, -6.7434],\n",
       "                        [-5.1087, -5.0522, -5.0362],\n",
       "                        [-4.8434, -4.6105, -5.9471]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0541, -6.8497, -5.4263],\n",
       "                        [-5.1009, -4.6458, -5.4793],\n",
       "                        [-5.4213, -4.9003, -4.8682]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.4279, -8.2878, -5.5914],\n",
       "                        [-5.8017, -5.5931, -4.8151],\n",
       "                        [-4.7875, -5.1253, -4.8293]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6792, -5.2176, -6.3633],\n",
       "                        [-6.5483, -4.7496, -5.1030],\n",
       "                        [-4.7008, -4.6548, -4.8405]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9131, -5.5380, -5.7411],\n",
       "                        [-8.2987, -4.6276, -5.5294],\n",
       "                        [-5.5955, -4.9558, -8.0978]]]])),\n",
       "             ('module_list.0.posterior_params.0.scale_alphas_log',\n",
       "              tensor([[[[-3.9730, -3.8184, -2.7370],\n",
       "                        [-2.8423, -3.3155, -3.7674],\n",
       "                        [-2.8688, -3.4832, -3.3337]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4496, -3.1676, -2.3279],\n",
       "                        [-2.5704, -2.6994, -2.5631],\n",
       "                        [-2.3078, -3.7059, -3.1989]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3379, -3.3761, -2.0345],\n",
       "                        [-3.8821, -2.3943, -3.7338],\n",
       "                        [-3.1945, -2.8338, -3.7807]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3331, -3.7250, -3.6263],\n",
       "                        [-3.3903, -2.8828, -3.6775],\n",
       "                        [-3.0060, -2.4066, -2.8119]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3633, -2.0377, -2.0332],\n",
       "                        [-3.5051, -3.9339, -3.9972],\n",
       "                        [-2.8213, -2.7204, -2.4167]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4371, -3.7661, -2.8416],\n",
       "                        [-2.1897, -2.4125, -2.5461],\n",
       "                        [-3.4732, -3.3259, -2.3366]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2825, -3.0099, -2.5968],\n",
       "                        [-3.0321, -3.8588, -3.2249],\n",
       "                        [-3.7188, -3.9843, -3.5802]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.8530, -3.9986, -3.0276],\n",
       "                        [-2.0767, -3.6595, -2.9175],\n",
       "                        [-3.2203, -2.1420, -3.6336]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5784, -2.8119, -2.3119],\n",
       "                        [-3.2846, -3.0844, -2.5892],\n",
       "                        [-3.3545, -2.3341, -3.3552]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5695, -2.5832, -2.3026],\n",
       "                        [-3.3705, -3.8097, -3.8492],\n",
       "                        [-2.4600, -2.2967, -3.7054]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3626, -3.4220, -3.9690],\n",
       "                        [-2.0460, -2.1348, -3.8663],\n",
       "                        [-2.3202, -3.0671, -2.3580]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1191, -3.4663, -3.9155],\n",
       "                        [-2.9626, -2.6909, -2.0241],\n",
       "                        [-3.6772, -2.0887, -2.8112]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5335, -2.2046, -3.8070],\n",
       "                        [-2.0733, -3.4714, -3.3680],\n",
       "                        [-2.0920, -3.3321, -3.0712]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9456, -3.9397, -2.3680],\n",
       "                        [-2.0659, -3.5653, -2.3071],\n",
       "                        [-3.2517, -3.4542, -2.0715]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2097, -3.9403, -2.1460],\n",
       "                        [-3.0559, -3.4030, -2.5687],\n",
       "                        [-3.4692, -2.5501, -3.5228]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.8775, -3.4609, -3.5841],\n",
       "                        [-3.2468, -3.9733, -3.9215],\n",
       "                        [-3.4708, -3.8749, -3.1619]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4536, -3.0306, -3.1692],\n",
       "                        [-2.9309, -2.0621, -2.1662],\n",
       "                        [-2.1606, -2.5014, -2.8317]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.8540, -3.1003, -2.3888],\n",
       "                        [-2.9370, -2.7015, -3.5162],\n",
       "                        [-2.7293, -2.3345, -3.2506]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4800, -3.4780, -2.7654],\n",
       "                        [-2.0434, -2.3557, -2.0213],\n",
       "                        [-2.2703, -2.0352, -3.7971]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9834, -3.0913, -2.8502],\n",
       "                        [-2.8904, -2.8264, -3.4881],\n",
       "                        [-3.3516, -3.2357, -2.9829]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1380, -3.2730, -3.5269],\n",
       "                        [-2.6177, -3.5875, -3.6728],\n",
       "                        [-2.7636, -3.6273, -3.2167]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4872, -3.5043, -2.3076],\n",
       "                        [-2.8557, -3.3450, -2.6795],\n",
       "                        [-2.7737, -2.7499, -3.3472]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2195, -2.7689, -2.0130],\n",
       "                        [-3.5297, -3.6577, -2.2098],\n",
       "                        [-2.0077, -3.4508, -3.4853]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1046, -3.8315, -3.3988],\n",
       "                        [-2.0476, -3.0053, -3.8965],\n",
       "                        [-2.1855, -2.1632, -2.1847]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9850, -2.6428, -2.9310],\n",
       "                        [-2.1326, -3.2657, -2.4740],\n",
       "                        [-3.1584, -2.4350, -3.3533]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9450, -2.5128, -3.1556],\n",
       "                        [-3.2801, -3.5410, -2.8983],\n",
       "                        [-2.5519, -3.2341, -2.8721]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8494, -3.8954, -3.0426],\n",
       "                        [-3.2585, -3.2226, -3.6094],\n",
       "                        [-3.2983, -3.4633, -3.9118]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4175, -2.1018, -2.3832],\n",
       "                        [-2.6769, -2.5597, -3.1847],\n",
       "                        [-3.8278, -2.5293, -3.5093]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6108, -3.0801, -2.6812],\n",
       "                        [-2.7818, -3.4267, -3.2687],\n",
       "                        [-2.1624, -2.1442, -2.5664]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5382, -2.5531, -3.3821],\n",
       "                        [-2.7202, -3.1200, -3.5994],\n",
       "                        [-3.7090, -3.5544, -3.7490]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9206, -2.9339, -2.6854],\n",
       "                        [-2.8685, -3.9728, -2.4037],\n",
       "                        [-3.7632, -2.0636, -2.3169]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5363, -2.2065, -3.9187],\n",
       "                        [-2.5296, -2.8139, -2.1778],\n",
       "                        [-3.0076, -2.6238, -3.4175]]]])),\n",
       "             ('module_list.0.posterior_params.0.scale_mus',\n",
       "              tensor([[[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]]])),\n",
       "             ('module_list.0.posterior_params.1.param_mus',\n",
       "              tensor([-0.0917,  0.2394,  0.0686, -0.2604, -0.0798,  0.2396, -0.0338,  0.2310,\n",
       "                       0.0511, -0.0118,  0.1305,  0.2870,  0.1377,  0.0071, -0.0213, -0.0859,\n",
       "                       0.1393, -0.0984, -0.1080,  0.1917,  0.1436, -0.2344,  0.0637, -0.2897,\n",
       "                      -0.2618, -0.2384, -0.1295,  0.1794, -0.0685, -0.0023, -0.2036, -0.0473])),\n",
       "             ('module_list.0.posterior_params.1.param_std_log',\n",
       "              tensor([-5.6001, -5.0561, -7.3938, -4.6664, -5.3911, -4.6895, -5.8183, -4.6080,\n",
       "                      -4.9778, -5.0134, -5.1747, -6.2682, -5.6235, -4.6147, -4.6179, -5.5958,\n",
       "                      -7.3049, -5.8125, -5.3669, -6.1171, -5.4824, -4.6084, -4.8485, -5.1090,\n",
       "                      -4.6260, -4.8618, -4.6064, -5.7304, -4.9281, -6.2759, -6.1592, -4.6723])),\n",
       "             ('module_list.0.posterior_params.1.scale_alphas_log',\n",
       "              tensor([-3.8439, -3.2271, -3.3951, -2.1596, -3.5587, -3.3096, -3.8678, -3.2271,\n",
       "                      -3.7286, -2.2776, -3.4039, -2.1131, -3.8562, -2.1256, -2.6956, -2.0418,\n",
       "                      -3.5359, -3.6205, -3.8657, -2.2383, -2.6532, -2.0978, -2.3008, -3.0798,\n",
       "                      -3.8545, -2.1004, -2.2579, -3.7659, -2.8519, -2.2167, -2.4384, -3.4444])),\n",
       "             ('module_list.0.posterior_params.1.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('module_list.0.posterior_params.2.param_mus',\n",
       "              tensor([[[[-0.0196,  0.0454,  0.0391],\n",
       "                        [ 0.0089,  0.0444,  0.0319],\n",
       "                        [ 0.0151, -0.0048, -0.0067]],\n",
       "              \n",
       "                       [[ 0.0389,  0.0052, -0.0038],\n",
       "                        [ 0.0210, -0.0377,  0.0379],\n",
       "                        [-0.0082, -0.0270,  0.0123]],\n",
       "              \n",
       "                       [[-0.0195, -0.0450, -0.0348],\n",
       "                        [-0.0266,  0.0158,  0.0558],\n",
       "                        [ 0.0572, -0.0150, -0.0562]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0117, -0.0527,  0.0530],\n",
       "                        [-0.0441, -0.0011,  0.0099],\n",
       "                        [ 0.0460, -0.0206,  0.0311]],\n",
       "              \n",
       "                       [[ 0.0086, -0.0104,  0.0082],\n",
       "                        [-0.0060,  0.0010,  0.0508],\n",
       "                        [ 0.0234, -0.0204, -0.0198]],\n",
       "              \n",
       "                       [[ 0.0014,  0.0375, -0.0589],\n",
       "                        [-0.0500, -0.0523, -0.0287],\n",
       "                        [-0.0491,  0.0049,  0.0422]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0086, -0.0462,  0.0437],\n",
       "                        [-0.0584, -0.0168, -0.0261],\n",
       "                        [-0.0548, -0.0186, -0.0516]],\n",
       "              \n",
       "                       [[ 0.0359, -0.0341,  0.0119],\n",
       "                        [-0.0516, -0.0310, -0.0073],\n",
       "                        [-0.0050,  0.0106,  0.0388]],\n",
       "              \n",
       "                       [[ 0.0335,  0.0242, -0.0266],\n",
       "                        [-0.0136,  0.0261,  0.0277],\n",
       "                        [-0.0235, -0.0340, -0.0570]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0397, -0.0346,  0.0134],\n",
       "                        [-0.0087,  0.0414,  0.0012],\n",
       "                        [-0.0219,  0.0087,  0.0025]],\n",
       "              \n",
       "                       [[-0.0426,  0.0360, -0.0347],\n",
       "                        [-0.0527, -0.0319,  0.0490],\n",
       "                        [ 0.0425,  0.0571,  0.0575]],\n",
       "              \n",
       "                       [[ 0.0446,  0.0385,  0.0485],\n",
       "                        [ 0.0124,  0.0016, -0.0074],\n",
       "                        [ 0.0091, -0.0570, -0.0402]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0326, -0.0093, -0.0427],\n",
       "                        [-0.0027, -0.0183,  0.0407],\n",
       "                        [ 0.0067,  0.0192, -0.0295]],\n",
       "              \n",
       "                       [[ 0.0398, -0.0258, -0.0529],\n",
       "                        [ 0.0533, -0.0561,  0.0226],\n",
       "                        [-0.0508, -0.0089, -0.0217]],\n",
       "              \n",
       "                       [[-0.0574,  0.0053,  0.0086],\n",
       "                        [ 0.0176, -0.0122,  0.0389],\n",
       "                        [ 0.0315, -0.0262,  0.0497]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0496,  0.0429, -0.0389],\n",
       "                        [ 0.0564,  0.0134, -0.0487],\n",
       "                        [ 0.0151, -0.0558, -0.0526]],\n",
       "              \n",
       "                       [[ 0.0041,  0.0085,  0.0242],\n",
       "                        [-0.0357,  0.0532, -0.0053],\n",
       "                        [ 0.0320,  0.0389, -0.0084]],\n",
       "              \n",
       "                       [[-0.0232, -0.0328, -0.0265],\n",
       "                        [ 0.0073,  0.0322,  0.0154],\n",
       "                        [ 0.0334, -0.0364, -0.0399]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0362, -0.0152, -0.0356],\n",
       "                        [-0.0227,  0.0356,  0.0551],\n",
       "                        [ 0.0449,  0.0479,  0.0460]],\n",
       "              \n",
       "                       [[ 0.0109, -0.0216,  0.0328],\n",
       "                        [ 0.0278,  0.0500, -0.0082],\n",
       "                        [ 0.0543, -0.0203,  0.0153]],\n",
       "              \n",
       "                       [[ 0.0102,  0.0585, -0.0482],\n",
       "                        [ 0.0470,  0.0210,  0.0068],\n",
       "                        [ 0.0122,  0.0472, -0.0473]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0350,  0.0331,  0.0211],\n",
       "                        [ 0.0469,  0.0194, -0.0124],\n",
       "                        [ 0.0085,  0.0097,  0.0335]],\n",
       "              \n",
       "                       [[-0.0213, -0.0421,  0.0169],\n",
       "                        [ 0.0271,  0.0084, -0.0281],\n",
       "                        [-0.0009,  0.0395,  0.0552]],\n",
       "              \n",
       "                       [[-0.0405,  0.0506,  0.0431],\n",
       "                        [ 0.0378,  0.0429,  0.0452],\n",
       "                        [ 0.0226,  0.0213, -0.0085]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0589, -0.0413,  0.0375],\n",
       "                        [-0.0113,  0.0128, -0.0504],\n",
       "                        [ 0.0348,  0.0297, -0.0023]],\n",
       "              \n",
       "                       [[ 0.0142, -0.0146,  0.0395],\n",
       "                        [-0.0279,  0.0025, -0.0369],\n",
       "                        [-0.0193,  0.0065,  0.0184]],\n",
       "              \n",
       "                       [[ 0.0535, -0.0271,  0.0205],\n",
       "                        [ 0.0521,  0.0121,  0.0178],\n",
       "                        [-0.0015, -0.0246,  0.0102]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0374,  0.0082,  0.0361],\n",
       "                        [-0.0384, -0.0128,  0.0242],\n",
       "                        [ 0.0210,  0.0459, -0.0359]],\n",
       "              \n",
       "                       [[ 0.0085,  0.0549, -0.0122],\n",
       "                        [ 0.0470, -0.0425,  0.0118],\n",
       "                        [-0.0259, -0.0331,  0.0163]],\n",
       "              \n",
       "                       [[-0.0175,  0.0018,  0.0136],\n",
       "                        [ 0.0412, -0.0085, -0.0292],\n",
       "                        [ 0.0508, -0.0501,  0.0192]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0055, -0.0442, -0.0088],\n",
       "                        [-0.0253, -0.0387, -0.0068],\n",
       "                        [-0.0327,  0.0401,  0.0176]],\n",
       "              \n",
       "                       [[-0.0053, -0.0276,  0.0089],\n",
       "                        [ 0.0546,  0.0484, -0.0486],\n",
       "                        [ 0.0030,  0.0041, -0.0256]],\n",
       "              \n",
       "                       [[ 0.0418,  0.0024, -0.0539],\n",
       "                        [ 0.0261, -0.0357, -0.0540],\n",
       "                        [ 0.0082, -0.0161,  0.0249]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0420, -0.0219, -0.0447],\n",
       "                        [-0.0553, -0.0467, -0.0041],\n",
       "                        [-0.0133, -0.0239,  0.0284]],\n",
       "              \n",
       "                       [[-0.0090,  0.0331,  0.0218],\n",
       "                        [-0.0550, -0.0575, -0.0531],\n",
       "                        [ 0.0528, -0.0313,  0.0230]],\n",
       "              \n",
       "                       [[-0.0138,  0.0412, -0.0435],\n",
       "                        [ 0.0420, -0.0355,  0.0162],\n",
       "                        [ 0.0510,  0.0107,  0.0575]]]])),\n",
       "             ('module_list.0.posterior_params.2.param_std_log',\n",
       "              tensor([[[[ -4.8953,  -5.0285,  -6.2216],\n",
       "                        [ -4.9035,  -8.6040,  -6.4262],\n",
       "                        [ -5.3935,  -6.4831,  -5.3591]],\n",
       "              \n",
       "                       [[ -5.5549,  -5.1110,  -4.7856],\n",
       "                        [ -5.0981,  -4.8755,  -5.7412],\n",
       "                        [ -5.6913,  -5.1175,  -5.4186]],\n",
       "              \n",
       "                       [[ -7.5934,  -4.9929,  -4.8682],\n",
       "                        [ -6.5329,  -6.7392,  -5.7023],\n",
       "                        [ -4.6655,  -6.5716,  -5.0604]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -4.6578,  -4.9114,  -5.8949],\n",
       "                        [ -5.4794,  -5.1367,  -5.2141],\n",
       "                        [ -7.1503,  -5.0723,  -6.8911]],\n",
       "              \n",
       "                       [[ -7.7260,  -4.7098,  -4.7194],\n",
       "                        [ -4.8400,  -5.9135,  -5.9438],\n",
       "                        [ -4.6816,  -5.5583,  -4.6564]],\n",
       "              \n",
       "                       [[ -5.4169,  -4.8717,  -4.7754],\n",
       "                        [ -5.1123,  -4.8388,  -5.6561],\n",
       "                        [ -5.6101,  -5.9402,  -5.5524]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -5.3646,  -6.1044,  -4.8309],\n",
       "                        [ -4.9290,  -6.4702,  -4.7857],\n",
       "                        [ -4.7454,  -6.7668,  -4.7657]],\n",
       "              \n",
       "                       [[ -5.3774,  -6.1290,  -5.3385],\n",
       "                        [ -8.1817,  -5.3416,  -4.6654],\n",
       "                        [ -6.9278,  -5.2007,  -5.8790]],\n",
       "              \n",
       "                       [[ -6.2015,  -4.7918,  -7.0114],\n",
       "                        [ -6.6029,  -4.9880,  -5.1994],\n",
       "                        [ -4.7158,  -4.6187,  -4.7050]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -7.0537,  -5.3076,  -5.5863],\n",
       "                        [ -5.8178,  -5.1174,  -5.0456],\n",
       "                        [ -5.3676,  -5.2478,  -5.6625]],\n",
       "              \n",
       "                       [[ -5.3646,  -6.0331,  -4.8292],\n",
       "                        [ -5.1204,  -5.0109,  -5.4891],\n",
       "                        [ -6.4985,  -4.7923,  -4.8494]],\n",
       "              \n",
       "                       [[ -5.2417,  -5.3264,  -4.9075],\n",
       "                        [ -4.6669,  -5.7886,  -5.0299],\n",
       "                        [ -4.8249,  -4.7134,  -5.8615]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -6.3013,  -5.6837,  -6.1340],\n",
       "                        [ -4.8347,  -6.8893,  -6.6270],\n",
       "                        [ -5.4885,  -6.7957,  -4.8725]],\n",
       "              \n",
       "                       [[ -4.7680,  -7.2965,  -6.8039],\n",
       "                        [ -7.3785,  -4.9002,  -5.0066],\n",
       "                        [ -4.6913,  -5.5690,  -4.6084]],\n",
       "              \n",
       "                       [[ -4.8951,  -7.2483,  -4.8205],\n",
       "                        [ -5.4375,  -5.4348,  -5.8624],\n",
       "                        [ -5.6269,  -4.8600,  -8.1621]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -4.9090,  -5.1043,  -6.6300],\n",
       "                        [ -4.9572,  -5.2812,  -4.9467],\n",
       "                        [ -6.9966,  -5.9345,  -5.1252]],\n",
       "              \n",
       "                       [[ -4.6989,  -4.8439,  -5.2943],\n",
       "                        [ -5.1051,  -5.7968,  -5.8932],\n",
       "                        [-10.1077,  -4.7491,  -8.4951]],\n",
       "              \n",
       "                       [[ -5.6271,  -5.8009,  -5.6112],\n",
       "                        [ -5.6509,  -4.7704,  -6.9457],\n",
       "                        [ -6.5300,  -5.8772,  -6.3974]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ -6.2656,  -6.2033,  -4.9923],\n",
       "                        [ -4.8826,  -8.2525,  -5.1309],\n",
       "                        [ -4.8151,  -5.6355,  -5.0209]],\n",
       "              \n",
       "                       [[ -6.3848,  -6.0805,  -7.8335],\n",
       "                        [ -4.8021,  -5.2011,  -4.9359],\n",
       "                        [ -5.2328,  -6.5811,  -4.6836]],\n",
       "              \n",
       "                       [[ -4.6560,  -4.6885, -11.7446],\n",
       "                        [ -6.5880,  -4.7699,  -5.2749],\n",
       "                        [ -5.6483,  -4.7934,  -6.8433]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -5.2342,  -5.2849,  -6.0316],\n",
       "                        [ -5.3714,  -4.7595,  -5.4207],\n",
       "                        [ -6.7571,  -9.9357,  -7.0918]],\n",
       "              \n",
       "                       [[ -7.8484,  -7.9049,  -4.8852],\n",
       "                        [ -8.5928,  -6.9321,  -4.7064],\n",
       "                        [ -4.6875,  -4.8348,  -5.5023]],\n",
       "              \n",
       "                       [[ -4.7033,  -4.6839,  -5.3626],\n",
       "                        [ -5.8149,  -4.6163,  -4.7024],\n",
       "                        [ -5.2816,  -5.4297,  -4.8385]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -4.7027,  -4.6516,  -5.3516],\n",
       "                        [ -5.2600,  -4.8881,  -5.0138],\n",
       "                        [ -5.9239,  -5.4689,  -4.6260]],\n",
       "              \n",
       "                       [[ -4.9607,  -5.8558,  -5.9619],\n",
       "                        [ -5.1053,  -7.0853,  -5.5569],\n",
       "                        [ -4.6085,  -6.3642,  -4.8830]],\n",
       "              \n",
       "                       [[ -5.5688,  -4.9501,  -6.1348],\n",
       "                        [ -6.3153,  -5.5237,  -4.7659],\n",
       "                        [ -4.6286,  -5.5474,  -6.4980]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -5.5645,  -4.6941,  -5.2743],\n",
       "                        [ -5.4054,  -4.9133,  -6.4651],\n",
       "                        [ -7.0040,  -4.9656,  -7.0266]],\n",
       "              \n",
       "                       [[ -5.2028,  -5.1697,  -4.7442],\n",
       "                        [ -4.8667,  -5.2562,  -6.2058],\n",
       "                        [ -7.5951,  -4.6418,  -4.9033]],\n",
       "              \n",
       "                       [[ -4.6518,  -5.0600,  -5.0147],\n",
       "                        [ -6.5606,  -5.0663,  -5.1944],\n",
       "                        [ -7.4739,  -4.6704,  -4.6449]]],\n",
       "              \n",
       "              \n",
       "                      [[[ -5.6330,  -6.6073,  -5.0135],\n",
       "                        [ -4.9958,  -4.8325,  -4.6252],\n",
       "                        [ -4.9012,  -6.2909,  -6.0797]],\n",
       "              \n",
       "                       [[ -4.8079,  -7.8986,  -5.3821],\n",
       "                        [ -5.9537,  -7.2645,  -5.7079],\n",
       "                        [ -4.7860,  -5.0177,  -5.3445]],\n",
       "              \n",
       "                       [[ -4.9210,  -5.5095,  -4.9153],\n",
       "                        [ -9.2945,  -5.2109,  -5.0991],\n",
       "                        [ -4.6786,  -4.8999,  -4.7837]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ -5.0940,  -6.4246,  -6.0016],\n",
       "                        [ -5.9106,  -4.6236,  -9.3768],\n",
       "                        [ -4.8565,  -4.6549,  -4.8105]],\n",
       "              \n",
       "                       [[ -5.0449,  -5.1650,  -4.6892],\n",
       "                        [ -5.7240,  -5.5601,  -4.7530],\n",
       "                        [ -4.6144,  -5.6058,  -7.4718]],\n",
       "              \n",
       "                       [[ -6.1598,  -4.6782,  -5.3052],\n",
       "                        [ -5.1732,  -5.6023,  -5.3645],\n",
       "                        [ -5.2507,  -5.4567,  -4.8220]]]])),\n",
       "             ('module_list.0.posterior_params.2.scale_alphas_log',\n",
       "              tensor([[[[-2.9031, -3.5752, -3.1913],\n",
       "                        [-2.2432, -3.7676, -3.9219],\n",
       "                        [-2.2315, -3.3817, -3.0104]],\n",
       "              \n",
       "                       [[-3.4620, -2.1322, -2.3796],\n",
       "                        [-3.2906, -3.9242, -2.4920],\n",
       "                        [-2.6064, -2.2625, -3.8039]],\n",
       "              \n",
       "                       [[-3.0305, -3.3771, -3.4199],\n",
       "                        [-2.7889, -2.9304, -3.7615],\n",
       "                        [-2.1174, -2.1470, -3.9918]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.2688, -2.4659, -3.1240],\n",
       "                        [-3.7692, -3.6450, -3.6797],\n",
       "                        [-3.3195, -2.8639, -2.6135]],\n",
       "              \n",
       "                       [[-2.1157, -2.3645, -3.2899],\n",
       "                        [-3.4988, -3.0115, -2.7938],\n",
       "                        [-3.8293, -3.2961, -2.8436]],\n",
       "              \n",
       "                       [[-3.9779, -2.0617, -3.1237],\n",
       "                        [-3.9790, -2.6218, -2.4551],\n",
       "                        [-3.5406, -3.1608, -3.2890]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6755, -2.0339, -3.6471],\n",
       "                        [-3.1715, -3.2779, -2.7492],\n",
       "                        [-2.6883, -3.9882, -3.6644]],\n",
       "              \n",
       "                       [[-2.4057, -2.0133, -2.1821],\n",
       "                        [-2.7596, -2.0538, -2.6359],\n",
       "                        [-3.5137, -3.3185, -3.5885]],\n",
       "              \n",
       "                       [[-3.0351, -2.9763, -2.0979],\n",
       "                        [-3.3766, -3.9956, -2.0556],\n",
       "                        [-2.8010, -3.5787, -3.6768]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.9227, -3.6770, -3.7636],\n",
       "                        [-3.2002, -2.1039, -2.2279],\n",
       "                        [-3.7049, -2.7865, -2.1264]],\n",
       "              \n",
       "                       [[-3.0326, -2.2067, -2.2916],\n",
       "                        [-2.1105, -3.7049, -2.5112],\n",
       "                        [-3.1940, -2.1001, -3.4853]],\n",
       "              \n",
       "                       [[-2.8792, -3.0937, -2.7589],\n",
       "                        [-2.6047, -2.2957, -3.1234],\n",
       "                        [-3.8853, -2.5147, -2.4693]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4313, -3.4454, -2.5228],\n",
       "                        [-2.2018, -3.2355, -2.3762],\n",
       "                        [-3.8663, -3.0295, -3.5700]],\n",
       "              \n",
       "                       [[-3.5861, -2.0395, -2.0020],\n",
       "                        [-2.1038, -3.2346, -2.7971],\n",
       "                        [-2.3657, -2.5616, -3.5310]],\n",
       "              \n",
       "                       [[-3.3006, -3.3535, -3.3189],\n",
       "                        [-3.1244, -3.3003, -3.5692],\n",
       "                        [-2.7795, -2.1361, -2.3262]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1123, -3.5558, -2.4190],\n",
       "                        [-2.8286, -2.1191, -2.5490],\n",
       "                        [-2.3278, -2.8362, -3.7079]],\n",
       "              \n",
       "                       [[-2.0250, -2.0361, -2.2242],\n",
       "                        [-3.5920, -2.1569, -3.2739],\n",
       "                        [-2.3617, -3.3745, -2.4369]],\n",
       "              \n",
       "                       [[-3.4996, -2.8253, -2.5848],\n",
       "                        [-3.4868, -3.3970, -3.3454],\n",
       "                        [-3.3596, -3.8827, -3.4139]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.1325, -2.0101, -2.3009],\n",
       "                        [-3.3946, -3.7420, -2.2947],\n",
       "                        [-2.8810, -3.1959, -2.4976]],\n",
       "              \n",
       "                       [[-3.0886, -3.6569, -2.8172],\n",
       "                        [-3.7599, -3.9197, -2.1939],\n",
       "                        [-3.7000, -2.5793, -3.5384]],\n",
       "              \n",
       "                       [[-2.8714, -2.3903, -2.9222],\n",
       "                        [-2.1279, -3.4309, -2.6593],\n",
       "                        [-3.9820, -2.9647, -3.0653]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.4074, -3.5453, -2.9808],\n",
       "                        [-2.2863, -3.9489, -2.0954],\n",
       "                        [-3.8373, -3.9608, -3.8983]],\n",
       "              \n",
       "                       [[-3.3889, -2.3294, -3.8718],\n",
       "                        [-2.4368, -2.1073, -2.8512],\n",
       "                        [-3.6483, -3.7381, -3.0208]],\n",
       "              \n",
       "                       [[-2.0715, -3.7128, -2.4285],\n",
       "                        [-3.7453, -2.5793, -3.9045],\n",
       "                        [-2.2144, -2.8838, -3.2419]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8225, -2.4760, -2.3612],\n",
       "                        [-3.4040, -2.1312, -3.9279],\n",
       "                        [-3.2219, -3.0139, -2.9078]],\n",
       "              \n",
       "                       [[-3.9570, -3.6798, -3.8169],\n",
       "                        [-2.9281, -3.5607, -2.0121],\n",
       "                        [-2.0334, -3.3228, -3.2737]],\n",
       "              \n",
       "                       [[-3.0264, -2.6413, -2.7952],\n",
       "                        [-2.6720, -2.2991, -3.6808],\n",
       "                        [-2.9856, -3.2928, -3.7279]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.2259, -2.7306, -3.2844],\n",
       "                        [-3.2082, -2.3142, -3.7642],\n",
       "                        [-2.4837, -2.9002, -2.0545]],\n",
       "              \n",
       "                       [[-3.8918, -3.5356, -2.0444],\n",
       "                        [-2.3271, -2.4968, -3.9083],\n",
       "                        [-2.4074, -2.6570, -2.4153]],\n",
       "              \n",
       "                       [[-2.0498, -3.5175, -2.1168],\n",
       "                        [-3.9951, -2.7050, -2.6350],\n",
       "                        [-2.7071, -2.6911, -3.2756]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4691, -2.6418, -3.3070],\n",
       "                        [-2.0403, -3.3388, -3.3410],\n",
       "                        [-3.4311, -2.3567, -2.7274]],\n",
       "              \n",
       "                       [[-2.9643, -3.1868, -2.6083],\n",
       "                        [-3.9008, -2.3488, -2.8015],\n",
       "                        [-3.6829, -3.6533, -3.1903]],\n",
       "              \n",
       "                       [[-3.1084, -2.5719, -3.7105],\n",
       "                        [-2.5479, -3.8481, -2.2664],\n",
       "                        [-2.6867, -2.7838, -3.6353]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.0250, -2.3754, -2.1942],\n",
       "                        [-2.2754, -2.2488, -2.5304],\n",
       "                        [-3.6430, -2.9638, -3.1415]],\n",
       "              \n",
       "                       [[-3.1872, -2.3818, -3.4090],\n",
       "                        [-3.5182, -3.5750, -3.3125],\n",
       "                        [-2.8591, -2.2255, -3.4907]],\n",
       "              \n",
       "                       [[-2.1073, -3.1520, -3.7595],\n",
       "                        [-2.5730, -2.0243, -3.1251],\n",
       "                        [-3.6950, -3.2891, -3.7837]]]])),\n",
       "             ('module_list.0.posterior_params.2.scale_mus',\n",
       "              tensor([[[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]],\n",
       "              \n",
       "                       [[1., 1., 1.],\n",
       "                        [1., 1., 1.],\n",
       "                        [1., 1., 1.]]]])),\n",
       "             ('module_list.0.posterior_params.3.param_mus',\n",
       "              tensor([-0.0347, -0.0540, -0.0008,  0.0293, -0.0292,  0.0349, -0.0176,  0.0116,\n",
       "                       0.0173,  0.0239,  0.0535,  0.0285,  0.0451, -0.0420,  0.0108, -0.0409,\n",
       "                       0.0453, -0.0289, -0.0493, -0.0556, -0.0361, -0.0034, -0.0584,  0.0529,\n",
       "                       0.0162, -0.0444,  0.0243,  0.0210, -0.0163,  0.0040,  0.0075, -0.0225,\n",
       "                      -0.0476,  0.0339,  0.0409, -0.0459,  0.0447,  0.0547,  0.0345, -0.0486,\n",
       "                       0.0053, -0.0136,  0.0179, -0.0266,  0.0421, -0.0030, -0.0182, -0.0482,\n",
       "                      -0.0340, -0.0338,  0.0125, -0.0354,  0.0531, -0.0226, -0.0371, -0.0432,\n",
       "                       0.0315,  0.0339, -0.0145,  0.0430,  0.0406,  0.0375, -0.0415, -0.0373])),\n",
       "             ('module_list.0.posterior_params.3.param_std_log',\n",
       "              tensor([-5.4569, -4.6801, -4.7204, -5.3099, -5.5642, -7.9669, -5.1158, -5.7351,\n",
       "                      -4.8519, -5.2321, -6.6220, -5.4749, -5.7233, -5.7436, -4.8307, -5.3099,\n",
       "                      -5.5596, -6.0521, -5.2583, -4.7860, -4.8898, -5.6637, -4.7467, -4.6643,\n",
       "                      -5.3782, -6.6473, -4.6510, -5.2890, -4.7336, -4.6367, -4.8326, -7.7491,\n",
       "                      -5.7414, -5.3058, -5.7958, -4.8864, -4.6670, -5.3566, -6.6464, -6.9234,\n",
       "                      -6.2836, -4.6365, -7.2882, -6.8838, -4.6835, -5.1239, -7.1621, -5.2982,\n",
       "                      -5.1323, -6.3269, -5.4452, -6.2584, -6.0409, -5.0412, -4.7253, -4.7920,\n",
       "                      -4.8736, -4.7333, -4.9122, -4.8091, -6.4931, -5.7098, -8.9380, -4.7787])),\n",
       "             ('module_list.0.posterior_params.3.scale_alphas_log',\n",
       "              tensor([-2.0096, -3.1070, -2.4744, -2.8249, -2.1330, -2.1949, -3.7866, -3.2097,\n",
       "                      -3.8550, -3.1583, -3.0586, -2.9922, -2.6323, -2.1886, -2.5555, -2.2712,\n",
       "                      -2.9222, -2.3903, -2.9189, -2.6913, -2.1877, -2.3527, -2.1297, -3.1475,\n",
       "                      -2.6098, -2.0891, -3.4339, -2.4926, -3.6903, -2.8732, -2.1957, -3.5118,\n",
       "                      -2.2983, -3.3832, -3.4829, -2.7464, -3.3553, -3.5355, -2.4428, -3.0015,\n",
       "                      -2.7173, -2.5134, -3.9719, -2.8202, -2.6662, -3.1703, -2.6489, -2.1131,\n",
       "                      -2.0383, -3.8398, -2.6632, -2.2751, -2.4046, -2.9891, -2.8446, -3.0018,\n",
       "                      -2.0600, -3.5996, -3.0579, -2.5789, -3.7129, -2.4109, -2.5351, -3.2553])),\n",
       "             ('module_list.0.posterior_params.3.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('module_list.0.posterior_params.4.param_mus',\n",
       "              tensor([[ 0.0009,  0.0158,  0.0126,  ..., -0.0105, -0.0025, -0.0128],\n",
       "                      [ 0.0080, -0.0029, -0.0087,  ...,  0.0028, -0.0101,  0.0126],\n",
       "                      [ 0.0150, -0.0064, -0.0075,  ..., -0.0084, -0.0034, -0.0038],\n",
       "                      ...,\n",
       "                      [-0.0002, -0.0109,  0.0178,  ..., -0.0034, -0.0082, -0.0020],\n",
       "                      [-0.0095,  0.0160, -0.0090,  ..., -0.0084,  0.0039,  0.0121],\n",
       "                      [-0.0176, -0.0070,  0.0146,  ..., -0.0085,  0.0145, -0.0063]])),\n",
       "             ('module_list.0.posterior_params.4.param_std_log',\n",
       "              tensor([[-5.0882, -5.6143, -5.0647,  ..., -5.6104, -4.6591, -4.9445],\n",
       "                      [-4.6745, -5.9688, -5.0048,  ..., -6.3473, -4.6243, -6.2210],\n",
       "                      [-4.7615, -7.7854, -5.7147,  ..., -7.3079, -5.9162, -5.9307],\n",
       "                      ...,\n",
       "                      [-6.7976, -5.2108, -5.6782,  ..., -5.2774, -5.6684, -6.0092],\n",
       "                      [-6.6351, -4.8296, -5.0264,  ..., -7.7251, -5.2107, -4.8150],\n",
       "                      [-7.3702, -8.7021, -4.7729,  ..., -4.8881, -4.7526, -5.0925]])),\n",
       "             ('module_list.0.posterior_params.4.scale_alphas_log',\n",
       "              tensor([[-2.3414, -2.7704, -3.4864,  ..., -2.1757, -3.9189, -2.2035],\n",
       "                      [-2.3410, -2.2061, -3.1260,  ..., -3.4673, -2.0878, -3.4667],\n",
       "                      [-3.9802, -2.7468, -2.0136,  ..., -2.2748, -3.8034, -2.2377],\n",
       "                      ...,\n",
       "                      [-2.4080, -2.7311, -2.1504,  ..., -3.6135, -2.4462, -2.0026],\n",
       "                      [-3.4683, -2.5169, -2.6618,  ..., -2.9077, -2.7889, -2.0861],\n",
       "                      [-2.1884, -3.1091, -3.6772,  ..., -2.2174, -2.0252, -2.5327]])),\n",
       "             ('module_list.0.posterior_params.4.scale_mus',\n",
       "              tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('module_list.0.posterior_params.5.param_mus',\n",
       "              tensor([-0.0112,  0.0141,  0.0014,  0.0127, -0.0057, -0.0123, -0.0136,  0.0164,\n",
       "                       0.0140,  0.0152,  0.0077,  0.0159, -0.0018, -0.0005,  0.0025,  0.0036,\n",
       "                       0.0174, -0.0110,  0.0078, -0.0102,  0.0028,  0.0009,  0.0105,  0.0106,\n",
       "                       0.0002, -0.0018,  0.0092, -0.0014, -0.0002,  0.0019,  0.0001,  0.0019,\n",
       "                      -0.0122,  0.0073, -0.0102, -0.0153, -0.0162,  0.0075,  0.0051,  0.0153,\n",
       "                       0.0129, -0.0160, -0.0173, -0.0098, -0.0150,  0.0157, -0.0045,  0.0022,\n",
       "                      -0.0135,  0.0099,  0.0101,  0.0171,  0.0014, -0.0172,  0.0025, -0.0021,\n",
       "                       0.0169,  0.0017,  0.0117, -0.0016,  0.0121, -0.0066, -0.0090, -0.0044,\n",
       "                      -0.0091,  0.0011, -0.0143,  0.0033, -0.0132, -0.0091, -0.0091, -0.0157,\n",
       "                       0.0127,  0.0111, -0.0166,  0.0143,  0.0017, -0.0061, -0.0174,  0.0098,\n",
       "                       0.0034, -0.0063,  0.0041, -0.0007, -0.0088, -0.0068, -0.0123, -0.0048,\n",
       "                       0.0054, -0.0105, -0.0157,  0.0175, -0.0124,  0.0101,  0.0101,  0.0164,\n",
       "                       0.0117, -0.0088, -0.0032,  0.0164,  0.0047,  0.0089, -0.0158, -0.0005,\n",
       "                       0.0097, -0.0070,  0.0082,  0.0131, -0.0131, -0.0050,  0.0024, -0.0050,\n",
       "                       0.0152,  0.0010, -0.0035, -0.0169,  0.0012, -0.0119, -0.0085, -0.0127,\n",
       "                       0.0139,  0.0060,  0.0050, -0.0074, -0.0137,  0.0144,  0.0127, -0.0140])),\n",
       "             ('module_list.0.posterior_params.5.param_std_log',\n",
       "              tensor([ -8.5058,  -4.9983,  -4.7748,  -6.4515,  -8.2332, -10.5456,  -5.4021,\n",
       "                       -8.7938,  -6.3761,  -5.5005,  -7.5101,  -5.7537,  -4.8349,  -6.5463,\n",
       "                       -4.9158,  -6.0935,  -7.2662,  -5.4875,  -5.6636,  -4.8700,  -4.7151,\n",
       "                       -4.7584,  -4.8736,  -5.3312,  -4.9649,  -4.9046,  -4.9326,  -7.3845,\n",
       "                       -6.0946,  -5.0042,  -4.9009,  -4.6594,  -5.4126,  -4.7578,  -5.2708,\n",
       "                       -7.4743,  -5.6484,  -5.0898,  -5.7023,  -5.2178,  -6.5615,  -7.9420,\n",
       "                       -5.2271,  -5.7446,  -5.1716,  -5.5231,  -5.2031,  -6.4824,  -4.7606,\n",
       "                       -5.7502,  -5.2827,  -6.0781,  -5.8726,  -5.0141,  -4.9438,  -8.0827,\n",
       "                       -5.3131,  -5.7115,  -4.7213,  -4.6253,  -4.7276,  -4.6258,  -5.1091,\n",
       "                       -5.2092,  -5.5085,  -5.6953,  -4.6783,  -4.9021,  -4.7901,  -4.9724,\n",
       "                       -5.9262,  -6.1041,  -5.0713,  -4.6735,  -6.4833,  -5.2216,  -5.1605,\n",
       "                       -4.9854,  -5.7718,  -6.8357,  -4.9983,  -6.0955,  -5.8611,  -5.9829,\n",
       "                       -4.8589,  -5.1603,  -6.4991,  -5.6061,  -5.8304,  -5.4435,  -4.6777,\n",
       "                       -4.7764,  -5.0857,  -4.9599,  -4.7925,  -4.8101,  -4.6988,  -4.6460,\n",
       "                       -5.2655,  -6.4285,  -5.7866,  -5.3847,  -5.1084,  -5.3568,  -5.4592,\n",
       "                       -5.4557,  -7.1196,  -4.6417,  -6.3491,  -4.9110,  -5.1545,  -4.8412,\n",
       "                       -5.2638,  -6.0338,  -4.9269,  -7.4436,  -5.8754,  -4.9944,  -5.3159,\n",
       "                       -5.9607,  -6.5675,  -6.2111,  -5.1559,  -7.5439,  -4.7024,  -5.7944,\n",
       "                       -4.7159,  -5.0060])),\n",
       "             ('module_list.0.posterior_params.5.scale_alphas_log',\n",
       "              tensor([-2.9711, -2.3029, -3.2661, -3.8841, -3.9730, -2.9972, -3.0801, -2.8908,\n",
       "                      -3.2214, -2.5367, -3.7855, -3.5761, -3.2829, -2.4807, -3.7627, -3.3901,\n",
       "                      -3.2457, -2.9323, -3.2918, -3.2453, -3.4675, -2.9678, -3.9310, -2.9987,\n",
       "                      -2.2035, -2.3976, -3.5818, -3.1568, -2.2656, -3.4460, -3.9424, -3.2561,\n",
       "                      -2.0742, -3.8532, -3.1280, -2.8851, -2.0143, -2.0730, -2.5920, -3.7779,\n",
       "                      -2.5698, -3.7920, -3.6548, -2.3358, -3.9477, -3.6476, -3.8149, -2.5764,\n",
       "                      -2.8168, -3.6043, -3.2515, -3.9662, -3.3422, -2.4601, -2.8934, -2.9427,\n",
       "                      -3.4763, -2.3204, -2.2301, -2.4737, -3.8858, -3.7568, -3.4567, -3.1788,\n",
       "                      -2.4339, -2.0268, -2.8586, -3.4026, -2.9261, -2.2214, -3.0145, -3.9870,\n",
       "                      -2.0210, -3.2889, -2.6818, -2.6525, -2.6215, -2.5191, -3.3091, -2.5133,\n",
       "                      -2.1772, -2.7749, -3.2064, -2.7597, -2.0681, -2.4307, -2.3182, -3.3219,\n",
       "                      -3.7947, -3.7979, -2.5180, -2.9864, -2.1951, -3.9902, -2.8577, -3.3741,\n",
       "                      -3.5462, -3.3496, -2.4838, -2.2125, -3.5979, -2.9471, -3.3925, -3.8978,\n",
       "                      -3.0267, -2.5021, -2.3832, -3.2529, -2.6711, -3.8071, -3.5777, -3.3519,\n",
       "                      -2.8422, -2.2266, -2.3704, -2.8740, -2.7499, -2.9441, -2.7132, -3.9846,\n",
       "                      -2.8880, -3.0201, -3.8964, -2.4971, -3.7383, -2.5198, -2.3017, -3.7625])),\n",
       "             ('module_list.0.posterior_params.5.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('module_list.0.posterior_params.6.param_mus',\n",
       "              tensor([[ 0.0861,  0.0572,  0.0623,  ...,  0.0349, -0.0029,  0.0783],\n",
       "                      [ 0.0663, -0.0496, -0.0471,  ...,  0.0246, -0.0090,  0.0290],\n",
       "                      [ 0.0787, -0.0569, -0.0074,  ...,  0.0223,  0.0753,  0.0849],\n",
       "                      ...,\n",
       "                      [-0.0752, -0.0523,  0.0439,  ..., -0.0768, -0.0083, -0.0679],\n",
       "                      [-0.0070, -0.0547, -0.0404,  ...,  0.0185, -0.0464, -0.0533],\n",
       "                      [-0.0087, -0.0757, -0.0331,  ...,  0.0254, -0.0312,  0.0815]])),\n",
       "             ('module_list.0.posterior_params.6.param_std_log',\n",
       "              tensor([[-5.2017, -7.5709, -4.9455,  ..., -5.7424, -5.2740, -7.6515],\n",
       "                      [-5.7487, -5.2041, -5.0962,  ..., -6.7837, -5.8445, -6.4197],\n",
       "                      [-5.0128, -4.9754, -5.1209,  ..., -5.3098, -6.3397, -8.8992],\n",
       "                      ...,\n",
       "                      [-6.1798, -5.0020, -5.8882,  ..., -4.7352, -8.7045, -4.7601],\n",
       "                      [-6.9533, -5.3401, -4.6249,  ..., -4.6350, -4.9398, -4.7976],\n",
       "                      [-5.3153, -4.6354, -5.0823,  ..., -5.2240, -5.2665, -4.6325]])),\n",
       "             ('module_list.0.posterior_params.6.scale_alphas_log',\n",
       "              tensor([[-3.4896, -2.3144, -3.2581,  ..., -2.1586, -3.0065, -2.2661],\n",
       "                      [-2.4441, -2.0589, -2.5914,  ..., -3.1640, -3.8862, -2.6127],\n",
       "                      [-2.4363, -2.7432, -2.3645,  ..., -2.4624, -2.6623, -2.2037],\n",
       "                      ...,\n",
       "                      [-3.5581, -3.5343, -3.9614,  ..., -3.5155, -3.0719, -3.5733],\n",
       "                      [-3.6154, -3.6500, -2.4728,  ..., -2.0083, -3.4753, -2.4960],\n",
       "                      [-3.1972, -3.5865, -2.2273,  ..., -3.9335, -3.2613, -3.5673]])),\n",
       "             ('module_list.0.posterior_params.6.scale_mus',\n",
       "              tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('module_list.0.posterior_params.7.param_mus',\n",
       "              tensor([ 0.0045,  0.0808, -0.0811,  0.0353,  0.0298, -0.0510, -0.0097,  0.0499,\n",
       "                       0.0105, -0.0429])),\n",
       "             ('module_list.0.posterior_params.7.param_std_log',\n",
       "              tensor([-5.2095, -5.1445, -5.7701, -4.7362, -6.1606, -5.0685, -5.4482, -5.0233,\n",
       "                      -5.1172, -4.9747])),\n",
       "             ('module_list.0.posterior_params.7.scale_alphas_log',\n",
       "              tensor([-2.7219, -3.5050, -3.6166, -2.6818, -2.5404, -2.2111, -2.9576, -3.6258,\n",
       "                      -2.4424, -2.8427])),\n",
       "             ('module_list.0.posterior_params.7.scale_mus',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_bayes.pt'))\n",
    "image1, label1 = test_dataset[10]\n",
    "image2, label2 = test_dataset[11]\n",
    "model(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:16906.90234375, KL Loss: 1690681.375, FitLoss: 0.09073139727115631, Accuracy 0.98, Prune parameters: 221821.0/421642\n"
     ]
    }
   ],
   "source": [
    "val_loss = 0.0\n",
    "val_acc = 0.0\n",
    "PRUNE = 1.0\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,  \n",
    "                                         batch_size=BATCH_SIZE,  \n",
    "                                         shuffle=False, \n",
    "                                         pin_memory=True) \n",
    "kl_loss = LogUniformVarKLLoss()\n",
    "trainer.params.prune_threshold = PRUNE\n",
    "test_result = trainer.eval(model, test_loader)\n",
    "acc = test_result.custom_losses['val_accuracy']\n",
    "print(f'Loss:{test_result.val_loss}, KL Loss: {test_result.dist_loss}, FitLoss: {test_result.fit_loss}, Accuracy {acc}, Prune parameters: {test_result.cnt_prune_parameters}/{test_result.cnt_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device=device)\n",
    "model.prune({'threshold': 1.0})\n",
    "model.set_map_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 4.2992e-03, -5.4342e-01, -0.0000e+00],\n",
      "          [ 3.9089e-01, -0.0000e+00, -0.0000e+00],\n",
      "          [ 8.2518e-01,  3.0815e-01, -2.3478e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.8153e-02,  0.0000e+00, -0.0000e+00],\n",
      "          [ 4.4879e-01,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.4131e+00, -7.5729e-01, -0.0000e+00],\n",
      "          [ 2.0788e-01,  4.6619e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.6288e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  9.8380e-01,  3.4592e-01],\n",
      "          [-0.0000e+00,  4.0430e-01,  0.0000e+00],\n",
      "          [-8.4115e-01, -3.8792e-01, -1.5979e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0565e-01,  0.0000e+00,  2.3229e-01],\n",
      "          [ 0.0000e+00,  6.6020e-01,  0.0000e+00],\n",
      "          [-0.0000e+00, -0.0000e+00, -3.2411e-01]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000e+00,  3.8068e-01,  0.0000e+00],\n",
      "          [-1.7023e-03,  7.2274e-01,  1.6451e-01],\n",
      "          [-2.6313e-01,  0.0000e+00, -8.0280e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  7.1311e-01, -0.0000e+00],\n",
      "          [ 7.3480e-01,  0.0000e+00, -6.3528e-01],\n",
      "          [ 1.7638e-02, -0.0000e+00, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8662e-01,  4.1352e-01,  7.5745e-01],\n",
      "          [ 3.4204e-03, -2.4012e-03,  1.9629e-01],\n",
      "          [-0.0000e+00, -1.8996e+00, -5.4733e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7047e-01, -0.0000e+00, -4.2426e-02],\n",
      "          [-0.0000e+00,  8.9670e-01,  8.5076e-01],\n",
      "          [-4.0429e-01,  5.5609e-01, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  1.1400e-01],\n",
      "          [ 0.0000e+00,  4.4838e-01, -0.0000e+00],\n",
      "          [ 4.5566e-02, -0.0000e+00, -1.9310e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.7405e-02,  2.3569e-01, -0.0000e+00],\n",
      "          [ 4.6704e-01,  8.9131e-01,  0.0000e+00],\n",
      "          [-0.0000e+00, -1.1183e-02, -6.1903e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 5.3916e-02,  1.3328e-02, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4441e-01,  0.0000e+00, -2.3364e-01],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  7.9347e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.6268e-01,  0.0000e+00, -0.0000e+00],\n",
      "          [ 6.3382e-01,  3.4143e-01, -0.0000e+00],\n",
      "          [-0.0000e+00, -3.0772e-01, -8.3751e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0354e-01,  0.0000e+00, -0.0000e+00],\n",
      "          [ 4.2287e-01,  0.0000e+00,  3.6846e-01],\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.1328e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.0544e-01, -1.0880e+00, -1.3626e+00],\n",
      "          [ 0.0000e+00,  4.4564e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  6.4581e-01,  3.5768e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  3.1633e-01, -6.6071e-01],\n",
      "          [ 0.0000e+00,  2.3753e-01, -0.0000e+00],\n",
      "          [ 3.6975e-01, -5.6517e-03, -6.6312e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.1901e-01,  6.4522e-02,  2.1885e-01],\n",
      "          [-0.0000e+00,  6.1452e-01,  4.0866e-01],\n",
      "          [-1.2748e-01,  5.6207e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.3363e-01, -0.0000e+00, -0.0000e+00],\n",
      "          [ 2.1696e-01,  0.0000e+00,  6.1144e-02],\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.8670e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  6.6921e-01,  3.2235e-01],\n",
      "          [-0.0000e+00,  4.6664e-01,  1.8888e-01],\n",
      "          [-5.4447e-01, -0.0000e+00, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.0248e-01, -0.0000e+00,  1.2330e-01],\n",
      "          [-0.0000e+00,  0.0000e+00,  6.1526e-01],\n",
      "          [-1.3471e-01,  3.3910e-01,  2.8420e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  2.2249e-01,  0.0000e+00],\n",
      "          [ 1.4262e-01,  8.8915e-01,  0.0000e+00],\n",
      "          [-0.0000e+00,  0.0000e+00, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.1939e+00, -4.6484e-01, -0.0000e+00],\n",
      "          [-8.6274e-01,  1.4272e-01,  0.0000e+00],\n",
      "          [ 1.0309e-01,  4.9730e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.1762e-01, -1.4468e-01, -0.0000e+00],\n",
      "          [ 3.6268e-01,  5.3481e-01,  0.0000e+00],\n",
      "          [ 0.0000e+00,  5.2241e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000e+00,  0.0000e+00,  1.2648e-01],\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.5915e-01],\n",
      "          [-0.0000e+00,  0.0000e+00, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3550e-02,  0.0000e+00,  6.7948e-02],\n",
      "          [ 5.6981e-01,  0.0000e+00,  4.5842e-01],\n",
      "          [ 2.9938e-02,  1.7861e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  3.6278e-01, -2.5424e-01],\n",
      "          [ 9.4070e-01,  0.0000e+00, -2.1502e-02],\n",
      "          [ 9.6985e-03,  6.5121e-02, -0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  9.1819e-02],\n",
      "          [ 0.0000e+00, -0.0000e+00, -3.3740e-01],\n",
      "          [-0.0000e+00, -0.0000e+00, -2.1633e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3137e-02, -0.0000e+00, -0.0000e+00],\n",
      "          [ 5.4615e-01,  2.9908e-01, -0.0000e+00],\n",
      "          [ 9.4902e-01,  2.2312e-01, -3.2910e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.6456e-02, -3.2759e-01, -0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.7491e-01]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000e+00, -1.7607e-01, -7.8085e-02],\n",
      "          [-0.0000e+00,  1.0843e+00,  0.0000e+00],\n",
      "          [-7.0030e-02,  0.0000e+00,  1.0573e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  9.0907e-01],\n",
      "          [-1.3929e-01, -2.4492e-01,  0.0000e+00],\n",
      "          [-0.0000e+00, -0.0000e+00, -0.0000e+00]]]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.base_module.conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = test_dataset[100]\n",
    "plt.imshow(image.permute(1, 2, 0), cmap=\"gray\")\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2.1405], device='cuda:0'),\n",
       "indices=tensor([5], device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model(image.cuda()).data, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
