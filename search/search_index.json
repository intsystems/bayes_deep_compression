{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BayesComp","text":"<p>This python library is an extension of pytorch  for transforming ordinary neural networks into Bayesian. Why? Check out our post for motivation and approaches in this topic. You will find here foundamental ideas and practical considerations about Bayesian inference.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Ilgam Latypov</li> <li>Alexander Terentyev</li> <li>Kirill Semkin</li> <li>Nikita Mashalov</li> </ul>"},{"location":"#references","title":"References","text":"<p>Here are the works upon which this library is built</p> <ol> <li> <p>Graves, A. (2011). Practical Variational Inference for Neural Networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc.</p> </li> <li> <p>Christos Louizos, Karen Ullrich, &amp; Max Welling. (2017). Bayesian Compression for Deep Learning.</p> </li> <li> <p>Hippolyt Ritter, Aleksandar Botev, &amp; David Barber (2018). A Scalable Laplace Approximation for Neural Networks. In International Conference on Learning Representations.</p> </li> <li> <p>Yingzhen Li, &amp; Richard E. Turner. (2016). Renyi Divergence Variational Inference.</p> </li> </ol>"},{"location":"blog/","title":"Home","text":""},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/","title":"Bayesian approach in neural networks for model pruning","text":"","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#intro","title":"Intro","text":"<p>Using deep learning in solving complex, real-world problems has become quite an engineering routine. But we should never forget about the probabilistic sense inside models and loss minimization. So here, we are going to recall that probabilistic framework and extend it to the Bayesian framework. Such switching will give us pleasant perks but it is not always for free. The main applicition presented in the blog will be neural networks pruning but other directions will be mentioned too.</p> <p>So we will present 4 Bayesian techniques to envelope any task involving neural networks. If you get interested in the topic and want to use it on practice, you are welcome at our pytoch library.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#making-neural-network-bayesian","title":"Making neural network Bayesian","text":"<p>The usual ML problem setup consists in minimising the loss function \\(L\\) between the train targets \\(y\\) and parametrical model \\(f_{\\mathbf{w}}(\\mathbf{x})\\). We assume the model to be some neural network parameterized by \\(\\mathbf{w}\\). At the same time, the loss together with the model define data's distribution \\(p(y | \\mathbf{x}, \\mathbf{w})\\). Minimising the loss w.r.t. \\(\\mathbf{w}\\) is equivalent to finding maximum likelihood estimation \\(\\hat{\\mathbf{w}}\\) of model's parameters. Finally, to obtain target distribution on the new object \\(\\hat{\\mathbf{x}}\\) we just use learnt estimation and have \\(p(y | \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}})\\).</p> <p>The Bayesian approach complements the model with a prior distribution \\(p(\\mathbf{w} | \\Theta)\\) . It is generally parameterized by hyperparameters \\(\\Theta\\) (but prior can be a fixed distribution as well). This move kind of changes the game because now target distribution for a new object \\(\\hat{\\mathbf{x}}\\) is trickier</p> \\[    p(y | \\hat{\\mathbf{x}}, \\Theta) = \\int p(\\mathbf{w} | y, \\mathbf{x}, \\Theta) p(y | \\hat{\\mathbf{x}}, \\mathbf{w}) d\\mathbf{w} = \\mathbb{E}_{\\mathbf{w} \\sim p(\\mathbf{w} | y, \\mathbf{x}, \\Theta)} [p(y | \\hat{\\mathbf{x}}, \\mathbf{w})] , \\label{new_point}\\tag{1} \\] <p>where \\(p(\\mathbf{w} | y, \\mathbf{x}, \\Theta)\\) is a posterior distribution of the model's parameters based on the train data. Unfortunately, finding the posterior is typically intractable in case of the NNs. That leads to the intractability of the prediction. Another problem here is how to choose optimal hyperparameters \\(\\Theta\\) if we don't know them from some prior (expert) knowledge.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#why-bayes-may-be-useful","title":"Why Bayes may be useful?","text":"<p>In spite of the mentioned difficulties, Bayesian framework has a lot to give. You've might already heard that L2 regularization is equal to simple gaussian prior. Furthermore, nets pruning is possible using the very same gaussian or more sparsity-inducing priors (e.g. Laplace prior).</p> <p>Some other bayes features:</p> <ul> <li>The formula (\\(\\ref{new_point}\\)) implies assembling your model to evaluate prediction for a new data point. To see it, you may evaluate expectation with Monte_Carlo samples drawn from posterior. Major benefit here is using more information about true model state in the final prediction. It also prevents models from being over-confident, see illustrative picture under.</li> </ul> <p></p> <p>Confidence shift in distribution after incorporating prior knowledge</p> <ul> <li>Bayes can be used to perform model selection, for example see hidden state models and learning mixture of gaussians.</li> </ul> <p>We have provided only a handful of applications, but this might be enough to spark your interest. Now, we will go through methods that will overcome problems with bayessian framework and make it usable.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#variational-inference","title":"Variational inference","text":"<p>One of the ways to add a Bayesian layer of inference to your neural network - variational inference principle. It is built upon special function called evidence which is</p> \\[     p(y | \\mathbf{x}, \\Theta) = \\mathbb{E}_{p(\\mathbf{w} | y, \\mathbf{x}, \\Theta)} [p(y | \\mathbf{x}, \\mathbf{w})]. \\] <p>It's similar to (\\(\\ref{new_point}\\)) but it has training object \\(\\mathbf{x}\\) in the left. The function basically indicates how probable the given data is under varying hyperparameters. Maximising this function is a key to finding optimal \\(\\Theta\\). But it can also help with the posterior! Following the ELBO technic, introduce variational distribution \\(q(\\mathbf{w} | \\phi)\\) which is parameterized by \\(\\phi\\). This distribution is supposed to approximate the posterior \\(p(\\mathbf{w} | y, \\mathbf{x}, \\Theta)\\) (see picture under) and in theory can be anything we want. Then, it can be shown that the following expression is evidence lower bound</p> \\[    \\text{ELBO}(\\Theta, \\phi) = \\mathbb{E}_{\\mathbf{w} \\sim q(\\mathbf{w} | \\phi)} [p(y | \\mathbf{x}, \\mathbf{w})] + \\text{KL}(q(\\mathbf{w} | \\phi) || p(\\mathbf{w} | \\Theta)) \\] <p>or in terms of the loss and the model</p> \\[    \\text{ELBO}(\\Theta, \\phi) = \\mathbb{E}_{\\mathbf{w} \\sim q(\\mathbf{w} | \\phi)} [L(y, f_{\\mathbf{w}}(\\mathbf{x}))] + \\text{KL}(q(\\mathbf{w} | \\phi) || p(\\mathbf{w} | \\Theta)) \\] <p>where \\(\\text{KL}(\\cdot || \\cdot)\\) is a KL-divergence. Maximising it by \\(\\phi\\) and \\(\\Theta\\) gives us estimation of the optimal hyperparameters and the variational distribution.</p> <p></p> <p>ELBO optimization is equal to posterior fitting</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#using-variational-distribution","title":"Using variational distribution","text":"<p>If you are not interested in hyperparameters, you can just use trained \\(q(\\mathbf{w} | \\phi^*)\\) to get the desired prediction. The (\\(\\ref{new_point}\\)) can be estimated as</p> \\[    p(y | \\hat{\\mathbf{x}}, \\Theta) \\approx \\mathbb{E}_{\\mathbf{w} \\sim q(\\mathbf{w} | \\phi^*)} [p(y | \\hat{\\mathbf{x}}, \\mathbf{w})]. \\] <p>It is usually approximated futher with Monte-Carlo. On the other hand, you can find the MAP estimation: \\(\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax }} q(\\mathbf{w} | \\phi^*)\\), and simplify it to</p> \\[    p(y | \\hat{\\mathbf{x}}, \\Theta) \\approx p(y | \\hat{\\mathbf{x}}, \\mathbf{w}^*). \\] <p>This expression is equal to just using your model \\(f_{\\mathbf{w}}(\\mathbf{x})\\) with \\(\\mathbf{w} = \\mathbf{w}^*\\) for prediction.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#choosing-variational-distribution","title":"Choosing variational distribution","text":"<p>As it was mentioned, there are no limits on \\(q(\\mathbf{w} | \\phi)\\) except to be computable. But to make the whole thing practical and use gradient optimization it should comply with several requirements.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#reparameterization-trick","title":"Reparameterization trick","text":"<p>In order to estimate and compute the gradient of the expectation in \\(\\text{ELBO}(\\Theta, \\phi)\\) the \\(q\\) must \"separate\" the randomness from \\(\\mathbf{w}\\). Namely, introduce some deterministic function \\(h\\) parameterized by \\(\\phi\\) and some random variable \\(\\epsilon\\), usually with simple distribution \\(p(\\epsilon)\\) from which we can sample. Now, randomness of \\(\\mathbf{w}\\) is expressed through the randomness of \\(\\epsilon\\)</p> \\[    \\mathbf{w} \\sim q(\\mathbf{w} | \\phi) \\Leftrightarrow \\mathbf{w} = h(\\mathbf{w}, \\phi, \\epsilon), \\ \\epsilon \\sim p(\\epsilon). \\] <p>This trick enables us to estimate the expectation in ELBO and compute its gradients:</p> \\[\\begin{align}    \\mathbb{E}_{\\mathbf{w} \\sim q(\\mathbf{w} | \\phi)} [p(y | \\mathbf{x}, \\mathbf{w})] &amp;\\approx \\frac{1}{K} \\sum_{i = 1}^K p(y | \\mathbf{x}, \\mathbf{w}_i) = \\frac{1}{K} \\sum_{i = 1}^K L(y, f_{\\mathbf{w}_i}(\\mathbf{x})), \\\\    \\nabla \\mathbb{E}_{\\mathbf{w} \\sim q(\\mathbf{w} | \\phi)} [p(y | \\mathbf{x}, \\mathbf{w})] &amp;\\approx \\frac{1}{K} \\sum_{i = 1}^K \\nabla L(y, f_{\\mathbf{w}_i}(\\mathbf{x})). \\end{align}\\] <p>where \\(\\mathbf{w}_i = h(\\mathbf{w}, \\phi, \\epsilon_i)\\), \\(\\epsilon_i \\sim p(\\epsilon)\\) and \\(K\\) is the number of samples.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#kl-computation","title":"KL computation","text":"<p>To optimise \\(\\text{ELBO}(\\Theta, \\phi)\\) it is necessary to compute KL term between \\(q(\\mathbf{w} | \\phi)\\) and \\(p(\\mathbf{w} | \\Theta)\\)  plus its gradients. As the choice of distributions is arbitrary and task-dependent, we do not discuss it further. One general solution here can be Monte-Carlo estimation:</p> \\[    \\text{KL}(q(\\mathbf{w} | \\phi) || p(\\mathbf{w} | \\Theta)) \\approx \\frac{1}{M} \\sum_{i = 1}^M \\log \\frac{q(\\mathbf{w}_i | \\phi)}{p(\\mathbf{w}_i | \\Theta)}, \\] <p>where \\(\\mathbf{w}_i \\sim q(\\mathbf{w} | \\phi)\\). Several particular solutions will be given futher.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#posterior-approximation","title":"Posterior approximation","text":"<p>Ideally, the class of variational distributions parametrized by \\(\\phi\\) should contain \\(p(\\mathbf{w} | y, \\mathbf{x}, \\Theta)\\). If \\(q\\) is exactly posterior then the ELBO will be exactly the evidence (not just lower bound)!</p> <p>Practically, we don't know the exact posterior but we know it up to the normalisation. It is followed from thes Bayesian theorem:</p> \\[    p(\\mathbf{w} | y, \\mathbf{x}, \\Theta) \\propto p(y | \\mathbf{w}, \\mathbf{x}) p(\\mathbf{w} | \\Theta). \\] <p>This can be a hint for choosing variational distributions class. For example, if you know that \\(p(\\mathbf{w} | y, \\mathbf{x}, \\Theta)\\) have some special properties, make sure that functions from \\(q\\) class have them too (for example, multimodality, see picture under).</p> <p></p> <p>Unimodal variational distribution can't fit multimodal posterior</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#pruning-strategy","title":"Pruning strategy","text":"<p>Finally, let's learn variational pruning technic. Generally, it is based on the probability mass of the distribution \\(q\\) in the \\(w_i = 0\\) points. Here \\(w_i\\) represents each individual weight of the model. If probability of this point is high, the weight is considered to be zero.</p> <p>For example, if \\(q\\) is a factorized gaussian (Graves, 2011) meaning \\(w_i \\sim \\mathcal{N}(w_i | \\mu_i, \\sigma_i^2)\\), then \\(\\log q(w_i = 0) \\propto -\\dfrac{\\mu_i^2}{2 \\sigma_i^2}\\). Set upper threshold on this value and we obtain pruning rule for individual weights:</p> \\[    \\left| \\dfrac{\\mu_i}{\\sigma_i} \\right| &lt; \\lambda \\Rightarrow \\text{prune}. \\] <p>Similar rules can be derived for more tricky distributions like log-uniform or half-Cauchy (Christos Louizos et. al., 2017). The main practical issue here is whatever \\(q\\) you choose it should factorize weights into small groups. Otherwise it would be impossible to compute marginal distributions on individual weights in reasonable time.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#renyi-divergence","title":"Renyi-divergence","text":"<p>Another Bayesian approach (Yingzhen Li et. al., 2016) is actually an extension of the variational inference. The main idea is to substitute KL divergence with the Renyi divergence</p> \\[    D_{\\alpha} [p || q] = \\frac{1}{\\alpha - 1} \\log \\int p(\\mathbf{w})^{\\alpha} q(\\mathbf{w})^{1 - \\alpha} d \\mathbf{w}. \\] <p>Formally, \\(\\alpha \\ge 0\\). But it is possible to set \\(\\alpha &lt; 0\\) although it won't be divergence mathematically.</p> <p>Let's denote the new ELBO functional as \\(\\text{ELBO}_{\\alpha}\\). Also denote by \\(\\alpha_+\\) any \\(\\alpha \\in [0, 1]\\) and by \\(\\alpha_-\\) any \\(\\alpha &lt; 0\\). The great feature of the Reniu functional is flexibility. It is possible to show that varying \\(\\alpha\\) we can obtain an old \\(\\text{ELBO} = \\underset{\\alpha \\to 1}{\\lim} \\text{ELBO}_{\\alpha}\\) or even true evidence \\(p(y | x) = \\underset{\\alpha \\to 0}{\\lim} \\text{ELBO}_{\\alpha}\\). Generally, we have the following relations</p> \\[    \\text{ELBO} = \\underset{\\alpha \\to 1}{\\lim} \\text{ELBO}_{\\alpha} \\le \\text{ELBO}_{\\alpha_+} \\le p(y | x) = \\underset{\\alpha \\to 0}{\\lim} \\text{ELBO}_{\\alpha} \\le \\text{ELBO}_{\\alpha_-}. \\] <p>Visual example below demonstrates evolution of fitted variational distribution depending on \\(\\alpha\\). The task is Bayesian linear regression. Black line corresponds to the exact posterior. It is multivariate gaussian with covariant components. Chosen variational distribution family is factorized gaussians.</p> <p>We go from the  cyan circle  corresponding to \\(\\alpha \\to \\infty\\). It can be referred to as mode-seeking ELBO regime. This gaussian lies exactly inside true gaussian and is perfectly centred around the mode. Then, decreasing \\(\\alpha\\) to 1 we obtain  violet circle  corresponding to conventional ELBO. After that, we get more inflated  blue circle  with \\(\\alpha = 0.5\\) and optimal  green circle  with \\(\\alpha = 0\\). In this case, variational distribution is true marginal distribution of the parameters</p> \\[    q (\\mathbf{w}) = p(w_1 | y, x) \\cdot p(w_2 | y, x) \\] <p>and delivers true evidence \\(\\text{ELBO}_{0} = \\text{ELBO}\\).</p> <p>If \\(\\alpha\\) is decreased further, we switch to the so called mass-covering regime. The   red circle  corresponds to \\(\\alpha \\to -\\infty\\). As we can see it hasn't inflated over the whole plane but has become a perfect encapsulation of the true posterior. To conclude, fitting models for different \\(\\alpha\\) is a great way to control distribution of probability mass around the true posterior.</p> <p></p> <p>Evolution of variational distributions for different \\(\\alpha\\)</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#renyi-approximation","title":"Renyi approximation","text":"<p>Let's rewrite \\(\\text{ELBO}_{\\alpha}\\) in another way</p> \\[    \\text{ELBO}_{\\alpha} = \\frac{1}{1 - \\alpha} \\log \\mathbb{E}_{\\mathbf{w} \\sim q} [\\left( \\frac{p(y, \\mathbf{w} | x)}{q(\\mathbf{w})} \\right)^{1 - \\alpha}] \\] <p>Unfortunately, this loss is equally intractable for deep learning models as ELBO in variational inference. But key walkarounds towards feasibility stay the same: the reparametrization trick and monte-carlo estimation. Imagine we have \\(\\mathbf{w} = h(\\mathbf{w}, \\phi, \\epsilon), \\ \\epsilon \\sim p(\\epsilon)\\) and \\(K\\) samples from this distribution, then estimated loss is</p> \\[\\begin{multline}    \\text{ELBO}_{\\alpha} = \\frac{1}{1 - \\alpha} \\log \\mathbb{E}_{\\epsilon} [\\left( \\frac{p(y, \\mathbf{w} | x)}{q(\\mathbf{w})} \\right)^{1 - \\alpha}] \\approx \\\\ \\approx \\frac{1}{1 - \\alpha} \\log \\frac{1}{K} \\sum_{i = 1}^K [\\left( \\frac{p(y, \\mathbf{w}_i | x)}{q(\\mathbf{w}_i)} \\right)^{1 - \\alpha}] =: \\widehat{\\text{ELBO}}_{\\alpha}. \\end{multline}\\] <p>As \\(p(\\epsilon)\\) is usually standard normal it is easy to take gradient over this estimation.</p> <p>The only problem here is that the new loss is actually biased. But as authors of the approach proved this bias monotonously vanishes with more samples. Moreover, with fixed \\(K\\) you can play around with \\(\\alpha\\) as \\(\\widehat{\\text{ELBO}}_{\\alpha}\\) is non-decreasing on \\(\\alpha\\). Combining earlier facts we can obtain that either \\(\\widehat{\\text{ELBO}}_{\\alpha} &lt; p(y|x)\\) for all \\(\\alpha\\) or their exists optimal \\(\\alpha_K\\) such that \\(\\widehat{\\text{ELBO}}_{\\alpha_K} = p(y|x)\\)!</p> <p>The illustration below shows relations between \\(\\text{ELBO}_{\\alpha}\\) for diffrent \\(\\alpha\\)/fixed \\(K\\) and vice versa. It is denoted here as \\(\\mathcal{L}_{\\alpha}\\). True evidence is denoted as \\(\\log p(x)\\). Conventional elbo here is \\(\\mathcal{L}_{VI}\\).</p> <p></p> <p>Illustration of how Renyi loss is changed under different variations.</p> <p>So, regarding discussed guarantees the approach is possible to be realized on practise.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#kroneker-factorized-laplace","title":"Kroneker-factorized Laplace","text":"<p>The key advantage of the last Bayesian approach (Hippolyt Ritter et. al., 2018) is that it is applicable to trained NNs with definite layer structure. We assume each layer to be linear transform followed by activation function. Imagine we have no prior for now, only likelihood \\(\\log p(y | \\mathbf{w}, \\mathbf{x})\\). We can use second order approximation around likelihood maximum denoted by \\(\\mathbf{w}^*\\)</p> \\[    \\log p(y | \\mathbf{w}, \\mathbf{x}) \\approx \\log p(y | \\mathbf{w}^*, \\mathbf{x}) + (\\mathbf{w} - \\mathbf{w}^*)^{\\text{T}} H (\\mathbf{w} - \\mathbf{w}^*). \\] <p>Here \\(H\\) is a hessian of the likelihood in the maximum point that we don't know. However, we do know \\(\\mathbf{w}^*\\) (it is our trained model). The approximation gives us normal distribution for net's parameters</p> \\[     \\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w}^*, H^{-1}). \\] <p>Actually, the result would be the same even if we had some fixed prior in the beginning. In this case, the likelihood function is substituted for posterior \\(p(\\mathbf{w} | y, \\mathbf{x})\\). Now \\(\\mathbf{w}^*\\) is the MAP model, \\(H\\) is the hessian of the posterior. Simple example of this transition is incorporation of the L2-regularization.</p> <p>It must be said that laplace approximation is not a panacea. The picture below illustrates how far from reality it can be. But for now we assume that it's applicable.</p> <p></p> <p>Laplace approximation of an arbitary distribution.</p> <p>As you can see, it is not always quite accurate.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#pruning-strategy_1","title":"Pruning strategy","text":"<p>If we knew \\(H\\) and \\(\\mathbf{w}^*\\), pruning could be based on the probability mass in \\(w_i = 0\\).So it is similar to the pruning in the variational approach. The major concern here is again factorization into parameters groups, namely by NN's layers. The authors of the approach showed that if we assume layer's independence, the hessian will factorize into a block-diagonal matrix. Therefore we will have independent gaussians for each layer! Computing marginals \\(q(w_i)\\) will be absolutely feasible.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#hessian-factorization","title":"Hessian factorization","text":"<p>Let's see what is the hessian's structure on each layer. Denote hessian of the layer \\(\\lambda\\) as \\(H_{\\lambda}\\), then it can be shown that</p> \\[    \\mathbb{E}[H_{\\lambda}] = \\mathbb{E}[\\mathcal{Q}_{\\lambda}] \\otimes \\mathbb{E}[\\mathcal{H}_{\\lambda}], \\] <p>where \\(\\mathcal{Q}_{\\lambda} = a_{\\lambda-1}^{\\text{T}} a_{\\lambda-1}\\) is a covariance of the incoming activations \\(a_{\\lambda-1}\\) and \\(\\mathcal{H}_{\\lambda} = \\dfrac{\\partial^2 L}{\\partial h_{\\lambda} \\partial h_{\\lambda}}\\) is the hessian of the loss w.r.t. linear pre-activations \\(h_{\\lambda}\\).</p> <p>Expectations here can be estimated by Monte-Carlo. The \\(\\mathcal{H}_{\\lambda}\\) is actually quite heavy to compute but can be estimated using KFRA or KFAC algorithms (in terms of the implementation, these are most cumbersome).</p> <p>Ultimately, the distribution on layer weights is matrix normal</p> \\[    \\mathbf{w}_{\\lambda} \\sim \\mathcal{MN}(\\mathbf{w}^*_{\\lambda}, \\mathcal{Q}_{\\lambda}^{-1}, \\mathcal{H}_{\\lambda}). \\]","tags":["ML","DL","bayes","pruning"]},{"location":"blog/2024/11/23/bayesian-approach-in-neural-networks-for-model-pruning/#conclusion","title":"Conclusion","text":"<p>We have enough mathematics for now . I hope you have enjoyed the concept of Bayesian inference and now understand how it can be useful in applications. Some practical python libraries are Bayesian Neural Networks, pyro and our developing library bayescomp. You can find many examples of using bayes approach in their docs.</p>","tags":["ML","DL","bayes","pruning"]},{"location":"reference/methods/bayes/base/distribution/","title":"Base Parameter Distributions","text":""},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist","title":"<code>ParamDist</code>","text":"<p>               Bases: <code>Distribution</code>, <code>ABC</code></p> Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>class ParamDist(D.distribution.Distribution, ABC):\n    @classmethod\n    @abstractmethod\n    def from_parameter(self, p: nn.Parameter) -&gt; \"ParamDist\":\n        \"\"\"\n        Default initialization of ParamDist forom parameters of nn.Module\n\n        Args:\n            p (nn.Parameter): paramaters for which ParamDist should be created.\n        \"\"\"\n        ...\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n    @abstractmethod\n    def get_params(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"\n        Returns dictionary of parameters that should be registered as parameters at nn.Module.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def log_prob(self, weights: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Returns logarithm of probability density function of distibution evaluated at weights.\n\n        Args:\n            weights (torch.Tensor): the point at which probability should be evaluated.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def log_z_test(self):\n        \"\"\"\n        Returns parameter which is used to be compared with threshold to estimate\n        wether this parameter should be pruned. By default it is logarithm of z_test\n        or equivalent of it. log_z_test = log(abs(mean)) - log(variance)\n        \"\"\"\n        return torch.log(torch.abs(self.mean)) - torch.log(self.variance)\n\n    @abstractmethod\n    def rsample(self, sample_shape: _size = torch.Size()) -&gt; torch.Tensor:\n        \"\"\"\n        Returns parameters sampled using reparametrization trick, so they could be used for\n        gradient estimation\n\n        Returns:\n            torch.Tensor: sampled parameters\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def map(self) -&gt; torch.Tensor:\n        \"\"\"\n        Returns mode of the distibution. It has a sense of maximum aposteriori estimation\n        for bayessian nets.\n\n        Returns:\n            torch.Tensor: MAP parameters\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def mean(self) -&gt; torch.Tensor:\n        \"\"\"\n        Returns mean of the distibution. It has a sense of non-bias estimation\n        for bayessian nets.\n\n        Returns:\n            torch.Tensor: mean parameters\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def variance(self):\n        \"\"\"\n        Returns variance of the distibution. It has a sense of error estimation\n        for bayessian nets and assumed to be used in prunning.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.map","title":"<code>map: torch.Tensor</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns mode of the distibution. It has a sense of maximum aposteriori estimation for bayessian nets.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: MAP parameters</p>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.mean","title":"<code>mean: torch.Tensor</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns mean of the distibution. It has a sense of non-bias estimation for bayessian nets.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: mean parameters</p>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.from_parameter","title":"<code>from_parameter(p)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Default initialization of ParamDist forom parameters of nn.Module</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Parameter</code> <p>paramaters for which ParamDist should be created.</p> required Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_parameter(self, p: nn.Parameter) -&gt; \"ParamDist\":\n    \"\"\"\n    Default initialization of ParamDist forom parameters of nn.Module\n\n    Args:\n        p (nn.Parameter): paramaters for which ParamDist should be created.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.get_params","title":"<code>get_params()</code>  <code>abstractmethod</code>","text":"<p>Returns dictionary of parameters that should be registered as parameters at nn.Module.</p> Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>@abstractmethod\ndef get_params(self) -&gt; dict[str, nn.Parameter]:\n    \"\"\"\n    Returns dictionary of parameters that should be registered as parameters at nn.Module.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.log_prob","title":"<code>log_prob(weights)</code>  <code>abstractmethod</code>","text":"<p>Returns logarithm of probability density function of distibution evaluated at weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Tensor</code> <p>the point at which probability should be evaluated.</p> required Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>@abstractmethod\ndef log_prob(self, weights: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Returns logarithm of probability density function of distibution evaluated at weights.\n\n    Args:\n        weights (torch.Tensor): the point at which probability should be evaluated.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.log_z_test","title":"<code>log_z_test()</code>  <code>abstractmethod</code>","text":"<p>Returns parameter which is used to be compared with threshold to estimate wether this parameter should be pruned. By default it is logarithm of z_test or equivalent of it. log_z_test = log(abs(mean)) - log(variance)</p> Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>@abstractmethod\ndef log_z_test(self):\n    \"\"\"\n    Returns parameter which is used to be compared with threshold to estimate\n    wether this parameter should be pruned. By default it is logarithm of z_test\n    or equivalent of it. log_z_test = log(abs(mean)) - log(variance)\n    \"\"\"\n    return torch.log(torch.abs(self.mean)) - torch.log(self.variance)\n</code></pre>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.rsample","title":"<code>rsample(sample_shape=torch.Size())</code>  <code>abstractmethod</code>","text":"<p>Returns parameters sampled using reparametrization trick, so they could be used for gradient estimation</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: sampled parameters</p> Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>@abstractmethod\ndef rsample(self, sample_shape: _size = torch.Size()) -&gt; torch.Tensor:\n    \"\"\"\n    Returns parameters sampled using reparametrization trick, so they could be used for\n    gradient estimation\n\n    Returns:\n        torch.Tensor: sampled parameters\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/distribution/#methods.bayes.base.distribution.ParamDist.variance","title":"<code>variance()</code>  <code>abstractmethod</code>","text":"<p>Returns variance of the distibution. It has a sense of error estimation for bayessian nets and assumed to be used in prunning.</p> Source code in <code>src/methods/bayes/base/distribution.py</code> <pre><code>@abstractmethod\ndef variance(self):\n    \"\"\"\n    Returns variance of the distibution. It has a sense of error estimation\n    for bayessian nets and assumed to be used in prunning.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/loss/","title":"Base Bayessian Loss","text":""},{"location":"reference/methods/bayes/base/loss/#methods.bayes.base.optimization.BaseLoss","title":"<code>BaseLoss</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract class for Distribution losses. Your distribution loss should be computed using prior and posterior classes and parameters, sampled from posterior.</p> <p>In forward method loss should realize logic of loss for one sampled weights.</p> Source code in <code>src/methods/bayes/base/optimization.py</code> <pre><code>class BaseLoss(torch.nn.Module, ABC):\n    \"\"\"\n    Abstract class for Distribution losses. Your distribution loss should\n    be computed using prior and posterior classes and parameters, sampled from posterior.\n\n    In forward method loss should realize logic of loss for one sampled weights.\n    \"\"\"\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; torch.Tensor:\n        \"\"\"\n        This method computes loss for one sampled parameters.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/methods/bayes/base/loss/#methods.bayes.base.optimization.BaseLoss.forward","title":"<code>forward(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>This method computes loss for one sampled parameters.</p> Source code in <code>src/methods/bayes/base/optimization.py</code> <pre><code>@abstractmethod\ndef forward(self, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\"\n    This method computes loss for one sampled parameters.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/net/","title":"Base Bayessian NN","text":""},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet","title":"<code>BaseBayesNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>General envelope around arbitary nn.Module which is going to include nn.Modules and BayesModules as submodules.</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>class BaseBayesNet(nn.Module):\n    \"\"\"General envelope around arbitary nn.Module which is going to include nn.Modules and BayesModules\n    as submodules.\n    \"\"\"\n\n    def __init__(self, base_module: nn.Module, module_dict: dict[str, nn.Module]):\n        \"\"\"_summary_\n\n        Args:\n            base_module (nn.Module): custom Module which is going to have some BayesModule as submodules\n            module_dict (dict[str, nn.Module]): all submodules of the base_module supposed to be trained. This\n                may be nn.Module or BayesModule. Such division is required because base_module is not\n                registred as Module in this class.\n        \"\"\"\n        super().__init__()\n        # self.__dict__[\"base_module\"] = base_module\n        self.base_module = base_module\n        self.module_dict = module_dict\n        self.module_list = nn.ModuleList()\n        for module in module_dict.values():\n            self.module_list.append(module)\n\n    def sample(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"Sample new parameters of base_module from current posterior distribution\n\n        Returns:\n            dict[str, nn.Parameter]: new sampled paramters in form of dictionary\n        \"\"\"\n        param_sample_dict: dict[str, nn.Parameter] = {}\n        for module_name, module in self.module_dict.items():\n            if isinstance(module, BayesLayer):\n                parameter_dict = module.sample()\n                for parameter_name, p in parameter_dict.items():\n                    param_sample_dict[self.get_path(module_name, parameter_name)] = p\n        return param_sample_dict\n\n    @property\n    def weights(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"\n        Returns all weights in base_module\n\n        Returns:\n            dict[str, ParamDist]: dictionary where key - name of weight,\n                value - weight value\n        \"\"\"\n        weights: dict[str, nn.Parameter] = {}\n        for module_name, module in self.module_dict.items():\n            for parameter_name, p in module.weights.items():\n                weights[self.get_path(module_name, parameter_name)] = p\n        return weights\n\n    @property\n    def device(self):\n        \"\"\"\n        Return device of net\n        \"\"\"\n        return next(self.parameters()).device\n\n    def forward(self, *args, **kwargs):\n        return self.base_module(*args, **kwargs)\n\n    def flush_weights(self) -&gt; None:\n        \"\"\"\n        This method simply set as tensors all weights that will be calculated by this layer and,\n        so it will work properly when layer is initialized.\n        \"\"\"\n        for module in self.module_dict.values():\n            if isinstance(module, BayesLayer):\n                module.flush_weights()\n\n    def sample_model(self) -&gt; nn.Module:\n        \"\"\"Sample base model and return deepcopy of it.\n\n        Returns:\n\n        \"\"\"\n        self.sample()\n\n        for module in self.module_dict.values():\n            if isinstance(module, BayesLayer):\n                for param_name in module.net_distribution.weight_distribution:\n                    cur_param: torch.Tensor = getattr(module.base_module, param_name)\n                    setattr(module.base_module, param_name, nn.Parameter(cur_param))\n\n        model = copy.deepcopy(self.base_module)\n        self.flush_weights()\n        return model\n\n    def eval(self):\n        \"\"\"\n        Alias of self.base_module.eval()\n        \"\"\"\n        return self.base_module.eval()\n\n    def train(self):\n        \"\"\"\n        Alias of self.base_module.train()\n        \"\"\"\n        return self.base_module.train()\n\n    @property\n    def posterior(self) -&gt; dict[str, ParamDist]:\n        \"\"\"\n        Returns posterior distribution for each weight\n\n        Returns:\n            dict[str, ParamDist]: dictionary where key - name of weight,\n                value - postrior distribution of weight\n        \"\"\"\n        # self.params = {mus: , sigmas: }\n        posteriors: dict[str, ParamDist] = {}\n        for module_name, module in self.module_dict.items():\n            if isinstance(module, BayesLayer):\n                for parameter_name, parameter_posterior in module.posterior.items():\n                    posteriors[self.get_path(module_name, parameter_name)] = parameter_posterior\n        return posteriors\n    def get_path(self, module_name:str, parameter_name:str) -&gt; str:\n        \"\"\"\n        Args:\n            module_name (str):  module name\n            parameter_name (str):  parameter name\n        Returns:\n            str: path to weight\n        \"\"\"\n        if(module_name == \"\"):\n            return parameter_name\n        else:\n            return f\"{module_name}.{parameter_name}\"\n    @property\n    def prior(self) -&gt; dict[str, Optional[ParamDist]]:\n        \"\"\"\n        Returns prior distribution for each weight\n\n        Returns:\n            dict[str, ParamDist]: dictionary where key - name of weight,\n                value - prior distribution of weight\n        \"\"\"\n        # self.params = {mus: , sigmas: }\n        priors: dict[str, Optional[ParamDist]] = {}\n        for module_name, module in self.module_dict.items():\n            module_prior = None\n            if isinstance(module, BayesLayer):\n                module_prior = module.prior\n                priors.update(module_prior)\n                for parameter_name, parameter_prior in module.prior.items():\n                    priors[self.get_path(module_name, parameter_name)] = parameter_prior\n        return priors\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.device","title":"<code>device</code>  <code>property</code>","text":"<p>Return device of net</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.posterior","title":"<code>posterior: dict[str, ParamDist]</code>  <code>property</code>","text":"<p>Returns posterior distribution for each weight</p> <p>Returns:</p> Type Description <code>dict[str, ParamDist]</code> <p>dict[str, ParamDist]: dictionary where key - name of weight, value - postrior distribution of weight</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.prior","title":"<code>prior: dict[str, Optional[ParamDist]]</code>  <code>property</code>","text":"<p>Returns prior distribution for each weight</p> <p>Returns:</p> Type Description <code>dict[str, Optional[ParamDist]]</code> <p>dict[str, ParamDist]: dictionary where key - name of weight, value - prior distribution of weight</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.weights","title":"<code>weights: dict[str, nn.Parameter]</code>  <code>property</code>","text":"<p>Returns all weights in base_module</p> <p>Returns:</p> Type Description <code>dict[str, Parameter]</code> <p>dict[str, ParamDist]: dictionary where key - name of weight, value - weight value</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.__init__","title":"<code>__init__(base_module, module_dict)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>custom Module which is going to have some BayesModule as submodules</p> required <code>module_dict</code> <code>dict[str, Module]</code> <p>all submodules of the base_module supposed to be trained. This may be nn.Module or BayesModule. Such division is required because base_module is not registred as Module in this class.</p> required Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def __init__(self, base_module: nn.Module, module_dict: dict[str, nn.Module]):\n    \"\"\"_summary_\n\n    Args:\n        base_module (nn.Module): custom Module which is going to have some BayesModule as submodules\n        module_dict (dict[str, nn.Module]): all submodules of the base_module supposed to be trained. This\n            may be nn.Module or BayesModule. Such division is required because base_module is not\n            registred as Module in this class.\n    \"\"\"\n    super().__init__()\n    # self.__dict__[\"base_module\"] = base_module\n    self.base_module = base_module\n    self.module_dict = module_dict\n    self.module_list = nn.ModuleList()\n    for module in module_dict.values():\n        self.module_list.append(module)\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.eval","title":"<code>eval()</code>","text":"<p>Alias of self.base_module.eval()</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def eval(self):\n    \"\"\"\n    Alias of self.base_module.eval()\n    \"\"\"\n    return self.base_module.eval()\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.flush_weights","title":"<code>flush_weights()</code>","text":"<p>This method simply set as tensors all weights that will be calculated by this layer and, so it will work properly when layer is initialized.</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def flush_weights(self) -&gt; None:\n    \"\"\"\n    This method simply set as tensors all weights that will be calculated by this layer and,\n    so it will work properly when layer is initialized.\n    \"\"\"\n    for module in self.module_dict.values():\n        if isinstance(module, BayesLayer):\n            module.flush_weights()\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.get_path","title":"<code>get_path(module_name, parameter_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>module name</p> required <code>parameter_name</code> <code>str</code> <p>parameter name</p> required <p>Returns:     str: path to weight</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def get_path(self, module_name:str, parameter_name:str) -&gt; str:\n    \"\"\"\n    Args:\n        module_name (str):  module name\n        parameter_name (str):  parameter name\n    Returns:\n        str: path to weight\n    \"\"\"\n    if(module_name == \"\"):\n        return parameter_name\n    else:\n        return f\"{module_name}.{parameter_name}\"\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.sample","title":"<code>sample()</code>","text":"<p>Sample new parameters of base_module from current posterior distribution</p> <p>Returns:</p> Type Description <code>dict[str, Parameter]</code> <p>dict[str, nn.Parameter]: new sampled paramters in form of dictionary</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def sample(self) -&gt; dict[str, nn.Parameter]:\n    \"\"\"Sample new parameters of base_module from current posterior distribution\n\n    Returns:\n        dict[str, nn.Parameter]: new sampled paramters in form of dictionary\n    \"\"\"\n    param_sample_dict: dict[str, nn.Parameter] = {}\n    for module_name, module in self.module_dict.items():\n        if isinstance(module, BayesLayer):\n            parameter_dict = module.sample()\n            for parameter_name, p in parameter_dict.items():\n                param_sample_dict[self.get_path(module_name, parameter_name)] = p\n    return param_sample_dict\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.sample_model","title":"<code>sample_model()</code>","text":"<p>Sample base model and return deepcopy of it.</p> <p>Returns:</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def sample_model(self) -&gt; nn.Module:\n    \"\"\"Sample base model and return deepcopy of it.\n\n    Returns:\n\n    \"\"\"\n    self.sample()\n\n    for module in self.module_dict.values():\n        if isinstance(module, BayesLayer):\n            for param_name in module.net_distribution.weight_distribution:\n                cur_param: torch.Tensor = getattr(module.base_module, param_name)\n                setattr(module.base_module, param_name, nn.Parameter(cur_param))\n\n    model = copy.deepcopy(self.base_module)\n    self.flush_weights()\n    return model\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BaseBayesNet.train","title":"<code>train()</code>","text":"<p>Alias of self.base_module.train()</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def train(self):\n    \"\"\"\n    Alias of self.base_module.train()\n    \"\"\"\n    return self.base_module.train()\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer","title":"<code>BayesLayer</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract envelope around arbitrary nn.Module to substitute all its nn.Parameters  with ParamDist. It transform it into bayessian Module. New distribution is a  variational distribution which mimics the true posterior distribution.</p> <p>To specify bayes Module with custom posterior, please inherit this class and specify fields under. Attributes:     prior_distribution_cls: Type of prior distribution that is used in this layer     posterior_distribution_cls: Type of posterior distribution that is used in this layer     is_posterior_trainable: Is posterior trainable     is_posterior_trainable: Is prior trainable</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>class BayesLayer(nn.Module, ABC):\n    \"\"\"Abstract envelope around arbitrary nn.Module to substitute all its nn.Parameters\n     with ParamDist. It transform it into bayessian Module. New distribution is a\n     variational distribution which mimics the true posterior distribution.\n\n    To specify bayes Module with custom posterior, please inherit this class and\n    specify fields under.\n    Attributes:\n        prior_distribution_cls: Type of prior distribution that is used in this layer\n        posterior_distribution_cls: Type of posterior distribution that is used in this layer\n        is_posterior_trainable: Is posterior trainable\n        is_posterior_trainable: Is prior trainable\n    \"\"\"\n\n    prior_distribution_cls: Optional[ParamDist]\n    posterior_distribution_cls: type[ParamDist]\n    is_posterior_trainable: bool\n    is_prior_trainable: bool\n\n    def __init__(self, module: nn.Module) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            module (nn.Module): custom Module layer which is going to be converted to BayesLayer\n        \"\"\"\n        super().__init__()\n        posterior: dict[str, ParamDist] = {}\n        self.prior: dict[str, Optional[ParamDist]] = {}\n        \"\"\"Prior disctribution for each weight\"\"\"\n        i = 0\n        # Itereate to create posterior and prior dist for each parameter\n        for name, p in list(module.named_parameters()):\n            p.requires_grad = False\n            posterior[name] = self.posterior_distribution_cls.from_parameter(p)\n            self.prior[name] = None\n            if self.prior_distribution_cls is not None:\n                self.prior[name] = self.prior_distribution_cls.from_parameter(p)\n            i += 1\n        # BaseNetDistribution - \u044d\u0442\u043e \u043d\u0435 Module\n        self.net_distribution = BaseNetDistribution(\n            module, weight_distribution=posterior\n        )\n        \"\"\"Posterior net disctribution that is trained using data to fit to evaluate \n        probapility of each net consider the data\"\"\"\n        self.posterior_params = nn.ParameterList()\n        \"\"\"\n        key - weight_name, value - distribution_args: nn.ParameterDict\n        this step is needed to register nn.Parameters of the ParamDists inside this class\n        \"\"\"\n        for dist in self.posterior.values():\n            param_dict = nn.ParameterDict(dist.get_params())\n            for param in param_dict.values():\n                param.requires_grad = self.is_posterior_trainable\n            self.posterior_params.append(param_dict)\n        # equal steps for prior distribution\n        self.prior_params = nn.ParameterList()\n        \"\"\"\n        key - weight_name, value - distribution_args: nn.ParameterDict\n        this step is needed to register nn.Parameters of the ParamDists inside this class\n        \"\"\"\n        for dist in self.prior.values():\n            if isinstance(dist, ParamDist):\n                param_dict = nn.ParameterDict(dist.get_params())\n                for param in param_dict.values():\n                    param.requires_grad = self.is_prior_trainable\n                self.prior_params.append(param_dict)\n\n    @property\n    def posterior(self) -&gt; dict[str, ParamDist]:\n        \"\"\"\n        Returns posterior distribution for each weight\n\n        Returns:\n            dict[str, ParamDist]: dictionary where key - name of weight,\n                value - postrior distribution of weight\n        \"\"\"\n        return self.net_distribution.weight_distribution\n\n    @property\n    def base_module(self) -&gt; nn.Module:\n        \"\"\"Return base_module that stores last sample module\n        Return:\n            nn.Module: strored module\n        \"\"\"\n        return self.net_distribution.base_module\n\n    def sample(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"Sample new parameters from net distribution\n        Return:\n            dict[str, nn.Parameter]: new sampled parameters\"\"\"\n        return self.net_distribution.sample_params()\n\n    @property\n    def weights(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"\n        Returns all weights in base_module\n\n        Returns:\n            dict[str, ParamDist]: dictionary where key - name of weight,\n                value - weight value\n        \"\"\"\n        weights: dict[str, nn.Parameter] = {}\n        for param_name in self.net_distribution.weight_distribution:\n            cur_param: torch.Tensor = getattr(self.base_module, param_name)\n            weights[param_name] = cur_param\n        return weights\n\n    @property\n    def device(self):\n        \"\"\"\n        Return device of module\n        \"\"\"\n        return next(self.parameters()).device\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        Make forward of last sampled module\n        \"\"\"\n        return self.base_module(*args, **kwargs)\n\n    def eval(self):\n        \"\"\"\n        Alias of self.base_module.eval()\n        \"\"\"\n        self.base_module.eval()\n\n    def train(self):\n        \"\"\"\n        Alias of self.base_module.train()\n        \"\"\"\n        self.base_module.train()\n\n    @abstractmethod\n    def flush_weights(self) -&gt; None:\n        \"\"\"\n        This method simply set as tensors all weights that will be calculated by this layer and,\n        so it will work properly when layer is initialized.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.base_module","title":"<code>base_module: nn.Module</code>  <code>property</code>","text":"<p>Return base_module that stores last sample module Return:     nn.Module: strored module</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.device","title":"<code>device</code>  <code>property</code>","text":"<p>Return device of module</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.net_distribution","title":"<code>net_distribution = BaseNetDistribution(module, weight_distribution=posterior)</code>  <code>instance-attribute</code>","text":"<p>Posterior net disctribution that is trained using data to fit to evaluate  probapility of each net consider the data</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.posterior","title":"<code>posterior: dict[str, ParamDist]</code>  <code>property</code>","text":"<p>Returns posterior distribution for each weight</p> <p>Returns:</p> Type Description <code>dict[str, ParamDist]</code> <p>dict[str, ParamDist]: dictionary where key - name of weight, value - postrior distribution of weight</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.posterior_params","title":"<code>posterior_params = nn.ParameterList()</code>  <code>instance-attribute</code>","text":"<p>key - weight_name, value - distribution_args: nn.ParameterDict this step is needed to register nn.Parameters of the ParamDists inside this class</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.prior","title":"<code>prior: dict[str, Optional[ParamDist]] = {}</code>  <code>instance-attribute</code>","text":"<p>Prior disctribution for each weight</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.prior_params","title":"<code>prior_params = nn.ParameterList()</code>  <code>instance-attribute</code>","text":"<p>key - weight_name, value - distribution_args: nn.ParameterDict this step is needed to register nn.Parameters of the ParamDists inside this class</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.weights","title":"<code>weights: dict[str, nn.Parameter]</code>  <code>property</code>","text":"<p>Returns all weights in base_module</p> <p>Returns:</p> Type Description <code>dict[str, Parameter]</code> <p>dict[str, ParamDist]: dictionary where key - name of weight, value - weight value</p>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.__init__","title":"<code>__init__(module)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>custom Module layer which is going to be converted to BayesLayer</p> required Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def __init__(self, module: nn.Module) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        module (nn.Module): custom Module layer which is going to be converted to BayesLayer\n    \"\"\"\n    super().__init__()\n    posterior: dict[str, ParamDist] = {}\n    self.prior: dict[str, Optional[ParamDist]] = {}\n    \"\"\"Prior disctribution for each weight\"\"\"\n    i = 0\n    # Itereate to create posterior and prior dist for each parameter\n    for name, p in list(module.named_parameters()):\n        p.requires_grad = False\n        posterior[name] = self.posterior_distribution_cls.from_parameter(p)\n        self.prior[name] = None\n        if self.prior_distribution_cls is not None:\n            self.prior[name] = self.prior_distribution_cls.from_parameter(p)\n        i += 1\n    # BaseNetDistribution - \u044d\u0442\u043e \u043d\u0435 Module\n    self.net_distribution = BaseNetDistribution(\n        module, weight_distribution=posterior\n    )\n    \"\"\"Posterior net disctribution that is trained using data to fit to evaluate \n    probapility of each net consider the data\"\"\"\n    self.posterior_params = nn.ParameterList()\n    \"\"\"\n    key - weight_name, value - distribution_args: nn.ParameterDict\n    this step is needed to register nn.Parameters of the ParamDists inside this class\n    \"\"\"\n    for dist in self.posterior.values():\n        param_dict = nn.ParameterDict(dist.get_params())\n        for param in param_dict.values():\n            param.requires_grad = self.is_posterior_trainable\n        self.posterior_params.append(param_dict)\n    # equal steps for prior distribution\n    self.prior_params = nn.ParameterList()\n    \"\"\"\n    key - weight_name, value - distribution_args: nn.ParameterDict\n    this step is needed to register nn.Parameters of the ParamDists inside this class\n    \"\"\"\n    for dist in self.prior.values():\n        if isinstance(dist, ParamDist):\n            param_dict = nn.ParameterDict(dist.get_params())\n            for param in param_dict.values():\n                param.requires_grad = self.is_prior_trainable\n            self.prior_params.append(param_dict)\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.eval","title":"<code>eval()</code>","text":"<p>Alias of self.base_module.eval()</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def eval(self):\n    \"\"\"\n    Alias of self.base_module.eval()\n    \"\"\"\n    self.base_module.eval()\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.flush_weights","title":"<code>flush_weights()</code>  <code>abstractmethod</code>","text":"<p>This method simply set as tensors all weights that will be calculated by this layer and, so it will work properly when layer is initialized.</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>@abstractmethod\ndef flush_weights(self) -&gt; None:\n    \"\"\"\n    This method simply set as tensors all weights that will be calculated by this layer and,\n    so it will work properly when layer is initialized.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Make forward of last sampled module</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"\n    Make forward of last sampled module\n    \"\"\"\n    return self.base_module(*args, **kwargs)\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.sample","title":"<code>sample()</code>","text":"<p>Sample new parameters from net distribution Return:     dict[str, nn.Parameter]: new sampled parameters</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def sample(self) -&gt; dict[str, nn.Parameter]:\n    \"\"\"Sample new parameters from net distribution\n    Return:\n        dict[str, nn.Parameter]: new sampled parameters\"\"\"\n    return self.net_distribution.sample_params()\n</code></pre>"},{"location":"reference/methods/bayes/base/net/#methods.bayes.base.net.BayesLayer.train","title":"<code>train()</code>","text":"<p>Alias of self.base_module.train()</p> Source code in <code>src/methods/bayes/base/net.py</code> <pre><code>def train(self):\n    \"\"\"\n    Alias of self.base_module.train()\n    \"\"\"\n    self.base_module.train()\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/","title":"Base Network Distributions","text":""},{"location":"reference/methods/bayes/base/net_distribution/#basenetdistribution","title":"<code>BaseNetDistribution</code>","text":"<p>Base class for distribution of nets. This class sees nets as elements of distribution. It helps sample nets from this distribution or estimate statistics of distribution. For this purpose it have base module architecture and distribution of parameters for each of it weights.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>class BaseNetDistribution:\n    \"\"\"\n    Base class for distribution of nets. This class sees nets as elements of distribution.\n    It helps sample nets from this distribution or estimate statistics of distribution.\n    For this purpose it have base module architecture and distribution of parameters for\n    each of it weights.\n    \"\"\"\n\n    def __init__(\n        self, base_module: nn.Module, weight_distribution: dict[str, ParamDist]\n    ) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            base_module (nn.Module): custom module layer which is going to be converted to BayesModule\n            weight_distribution (dict[str, ParamDist]): posteror distribution for each parameter of moudule\n        \"\"\"\n        super().__init__()\n        self.base_module: nn.Module = base_module\n        \"\"\"Show default architecture of module for which should evalute parameters\"\"\"\n        self.weight_distribution: dict[str, ParamDist] = weight_distribution\n        \"\"\"Distribution of parameter for each named parameter of base_module\"\"\"\n\n    def detach_(self):\n        \"\"\"\n        Detach(Made deepcopy) base module from original module\n        \"\"\"\n        self.base_module = copy.deepcopy(self.base_module)\n\n    def sample_params(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"\n        Sample only model parameter from distribution and\n        return it.\n\n        Returns:\n            dict[str, nn.Parameter]: Return dict of sampled parameters, where\n                key is name of parameter, value is valeu of parameter\n        \"\"\"\n        param_sample_dict: dict[str, nn.Parameter] = {}\n        for param_name, param_posterior in self.weight_distribution.items():\n            param_sample = param_posterior.rsample()\n            param_sample_dict[param_name] = nn.Parameter(param_sample)\n            del_attr(self.base_module, param_name.split(\".\"))\n            set_attr(self.base_module, param_name.split(\".\"), param_sample)\n        return param_sample_dict\n\n    def sample_model(self) -&gt; nn.Module:\n        \"\"\"\n        Sample only model from distribution and\n        return it. Note that model is the same that in base_module.\n\n        Returns:\n            nn.Module: sampled base_module with sampled parameters\n        \"\"\"\n        self.sample_params()\n        return self.base_module\n\n    def set_map_params(self) -&gt; None:\n        \"\"\"\n        Set MAP estimation parameters of model from distribution and\n        set it. So it sets most probable model.\n        \"\"\"\n        for param_name, dist in self.weight_distribution.items():\n            pt = dist.map\n            # pt = torch.nn.Parameter(pt.to_sparse())\n            set_attr(self.base_module, param_name.split(\".\"), pt)\n\n    def set_mean_params(self) -&gt; None:\n        \"\"\"\n        Set unbaised estimation parameters of model from distribution and\n        set it. So it sets model that would be returned at means.\n        \"\"\"\n        for param_name, dist in self.weight_distribution.items():\n            pt = dist.mean\n            # pt = torch.nn.Parameter(pt.to_sparse())\n            set_attr(self.base_module, param_name.split(\".\"), pt)\n\n    def __replace_with_parameters(self) -&gt; None:\n        \"\"\"\n        Replace parameters of base_module wit nn.Parameters\n        \"\"\"\n        for param_name in self.weight_distribution.keys():\n            pt = get_attr(self.base_module, param_name.split(\".\"))\n            pt = nn.Parameter(pt)\n            set_attr(self.base_module, param_name.split(\".\"), pt)\n\n    def get_model(self) -&gt; nn.Module:\n        \"\"\"\n        Get model with last set parameters. (It still we be the same model as in base_module)\n        \"\"\"\n        self.__replace_with_parameters()\n        return self.base_module\n\n    def get_model_snapshot(self) -&gt; nn.Module:\n        \"\"\"\n        Get deepcopy of model with last set parameters.\n        \"\"\"\n        return copy.deepcopy(self.get_model())\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.base_module","title":"<code>base_module: nn.Module = base_module</code>  <code>instance-attribute</code>","text":"<p>Show default architecture of module for which should evalute parameters</p>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.weight_distribution","title":"<code>weight_distribution: dict[str, ParamDist] = weight_distribution</code>  <code>instance-attribute</code>","text":"<p>Distribution of parameter for each named parameter of base_module</p>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.__init__","title":"<code>__init__(base_module, weight_distribution)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>custom module layer which is going to be converted to BayesModule</p> required <code>weight_distribution</code> <code>dict[str, ParamDist]</code> <p>posteror distribution for each parameter of moudule</p> required Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def __init__(\n    self, base_module: nn.Module, weight_distribution: dict[str, ParamDist]\n) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        base_module (nn.Module): custom module layer which is going to be converted to BayesModule\n        weight_distribution (dict[str, ParamDist]): posteror distribution for each parameter of moudule\n    \"\"\"\n    super().__init__()\n    self.base_module: nn.Module = base_module\n    \"\"\"Show default architecture of module for which should evalute parameters\"\"\"\n    self.weight_distribution: dict[str, ParamDist] = weight_distribution\n    \"\"\"Distribution of parameter for each named parameter of base_module\"\"\"\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.__replace_with_parameters","title":"<code>__replace_with_parameters()</code>","text":"<p>Replace parameters of base_module wit nn.Parameters</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def __replace_with_parameters(self) -&gt; None:\n    \"\"\"\n    Replace parameters of base_module wit nn.Parameters\n    \"\"\"\n    for param_name in self.weight_distribution.keys():\n        pt = get_attr(self.base_module, param_name.split(\".\"))\n        pt = nn.Parameter(pt)\n        set_attr(self.base_module, param_name.split(\".\"), pt)\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.detach_","title":"<code>detach_()</code>","text":"<p>Detach(Made deepcopy) base module from original module</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def detach_(self):\n    \"\"\"\n    Detach(Made deepcopy) base module from original module\n    \"\"\"\n    self.base_module = copy.deepcopy(self.base_module)\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.get_model","title":"<code>get_model()</code>","text":"<p>Get model with last set parameters. (It still we be the same model as in base_module)</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def get_model(self) -&gt; nn.Module:\n    \"\"\"\n    Get model with last set parameters. (It still we be the same model as in base_module)\n    \"\"\"\n    self.__replace_with_parameters()\n    return self.base_module\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.get_model_snapshot","title":"<code>get_model_snapshot()</code>","text":"<p>Get deepcopy of model with last set parameters.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def get_model_snapshot(self) -&gt; nn.Module:\n    \"\"\"\n    Get deepcopy of model with last set parameters.\n    \"\"\"\n    return copy.deepcopy(self.get_model())\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.sample_model","title":"<code>sample_model()</code>","text":"<p>Sample only model from distribution and return it. Note that model is the same that in base_module.</p> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: sampled base_module with sampled parameters</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def sample_model(self) -&gt; nn.Module:\n    \"\"\"\n    Sample only model from distribution and\n    return it. Note that model is the same that in base_module.\n\n    Returns:\n        nn.Module: sampled base_module with sampled parameters\n    \"\"\"\n    self.sample_params()\n    return self.base_module\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.sample_params","title":"<code>sample_params()</code>","text":"<p>Sample only model parameter from distribution and return it.</p> <p>Returns:</p> Type Description <code>dict[str, Parameter]</code> <p>dict[str, nn.Parameter]: Return dict of sampled parameters, where key is name of parameter, value is valeu of parameter</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def sample_params(self) -&gt; dict[str, nn.Parameter]:\n    \"\"\"\n    Sample only model parameter from distribution and\n    return it.\n\n    Returns:\n        dict[str, nn.Parameter]: Return dict of sampled parameters, where\n            key is name of parameter, value is valeu of parameter\n    \"\"\"\n    param_sample_dict: dict[str, nn.Parameter] = {}\n    for param_name, param_posterior in self.weight_distribution.items():\n        param_sample = param_posterior.rsample()\n        param_sample_dict[param_name] = nn.Parameter(param_sample)\n        del_attr(self.base_module, param_name.split(\".\"))\n        set_attr(self.base_module, param_name.split(\".\"), param_sample)\n    return param_sample_dict\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.set_map_params","title":"<code>set_map_params()</code>","text":"<p>Set MAP estimation parameters of model from distribution and set it. So it sets most probable model.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def set_map_params(self) -&gt; None:\n    \"\"\"\n    Set MAP estimation parameters of model from distribution and\n    set it. So it sets most probable model.\n    \"\"\"\n    for param_name, dist in self.weight_distribution.items():\n        pt = dist.map\n        # pt = torch.nn.Parameter(pt.to_sparse())\n        set_attr(self.base_module, param_name.split(\".\"), pt)\n</code></pre>"},{"location":"reference/methods/bayes/base/net_distribution/#methods.bayes.base.net_distribution.BaseNetDistribution.set_mean_params","title":"<code>set_mean_params()</code>","text":"<p>Set unbaised estimation parameters of model from distribution and set it. So it sets model that would be returned at means.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def set_mean_params(self) -&gt; None:\n    \"\"\"\n    Set unbaised estimation parameters of model from distribution and\n    set it. So it sets model that would be returned at means.\n    \"\"\"\n    for param_name, dist in self.weight_distribution.items():\n        pt = dist.mean\n        # pt = torch.nn.Parameter(pt.to_sparse())\n        set_attr(self.base_module, param_name.split(\".\"), pt)\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/","title":"Variance Parameter Distributions","text":""},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist","title":"<code>LogUniformVarDist</code>","text":"<p>               Bases: <code>ParamDist</code></p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>class LogUniformVarDist(ParamDist):\n    arg_constraints = {\n        \"param_mus\": constraints.real,\n        \"param_std_log\": constraints.real,\n        \"scale_mus\": constraints.real,\n        \"scale_alphas_log\": constraints.real,\n    }\n\n    @classmethod\n    def from_parameter(cls, p: nn.Parameter) -&gt; \"LogUniformVarDist\":\n        \"\"\"\n        Default initialization of LogUniformVarDist forom parameters of nn.Module\n\n        Args:\n            p (nn.Parameter): paramaters for which ParamDist should be created.\n        \"\"\"\n        param_mus = nn.Parameter(p, requires_grad=True)\n        param_std_log = nn.Parameter(\n            torch.log(torch.Tensor(p.shape).uniform_(1e-8, 1e-2)), requires_grad=True\n        )  # (0, 0.01)\n        scale_mus = nn.Parameter(torch.ones_like(p), requires_grad=True)\n        scale_alphas_log = nn.Parameter(\n            torch.Tensor(p.shape).uniform_(-4, -2), requires_grad=True\n        )  # (-4, -2)\n        return LogUniformVarDist(param_mus, param_std_log, scale_mus, scale_alphas_log)\n\n    def __init__(\n        self,\n        param_mus: torch.Tensor,\n        param_std_log: torch.Tensor,\n        scale_mus: torch.Tensor,\n        scale_alphas_log: torch.Tensor,\n        validate_args: Optional[bool] =None,\n    ):\n        r\"\"\"_summary_\n\n        Args:\n            param_mus: $\\mu$ parameter of distribution\n            param_std_log: $\\log(\\sigma)$ parameter of distribution\n            scale_mus: $\\mu$ parameter scale of distribution\n            scale_alphas_log: $\\alpha$ parameter scale of distribution\n            validate_args: alias fo validate_args of torch.distributions.sistribution\n        \"\"\"\n        self.param_mus: nn.Parameter = nn.Parameter(param_mus)\n        r\"\"\"$\\mu$ parameter of distribution\"\"\"\n        self.param_std_log: nn.Parameter = nn.Parameter(param_std_log)\n        r\"\"\"$\\log(\\sigma))$ parameter of distribution\"\"\"\n        self.scale_mus: nn.Parameter = nn.Parameter(scale_mus)\n        r\"\"\"$\\mu$ parameter scale of distribution\"\"\"\n        self.scale_alphas_log: nn.Parameter = nn.Parameter(scale_alphas_log)\n        r\"\"\"$\\alpha$ parameter scale of distribution\"\"\"\n\n        # (\n        #     self.param_mus,\n        #     self.param_std_log,\n        #     self.scale_mus,\n        #     self.scale_alphas_log,\n        # ) = broadcast_all(\n        #     self.param_mus, self.param_std_log, self.scale_mus, self.scale_alphas_log\n        # )\n\n        batch_shape = self.param_mus.size()\n        super().__init__(batch_shape, validate_args=validate_args)\n\n    def get_params(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"\n        Return all parameters that should be registered as named parameters of nn.Module.\n        {\"param_mus\", \"param_std_log\", \"scale_mus\", \"scale_alphas_log\"}\n\n        Returns:\n            dict[str, nn.Parameter]: parameters that should be registered as named parameters of nn.Module\n        \"\"\"\n        return {\n            \"param_mus\": self.param_mus,\n            \"param_std_log\": self.param_std_log,\n            \"scale_mus\": self.scale_mus,\n            \"scale_alphas_log\": self.scale_alphas_log,\n        }\n\n    @property\n    def map(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return MAP(if we speaks about posterior distribution) or MLE estimation of parameters\n\n        Returns:\n            torch.Tensor: MAP estimation of parameters\n        \"\"\"\n        return self.scale_mus * self.param_mus\n\n    @property\n    def mean(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return mean estimation of parameters\n\n        Returns:\n            torch.Tensor: mean value of parameters\n        \"\"\"\n        return self.scale_mus * self.param_mus\n\n    @property\n    def variance(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return variance of parameters\n\n        Returns:\n            torch.Tensor: variance of parameters\n        \"\"\"\n        raise NotImplementedError(\"Variance suppossed to not be used, use log_z_tes instead\")\n\n    def log_prob(self, weights) -&gt; torch.Tensor:\n        \"\"\"\n        Return logarithm probability at weights\n\n        Returns:\n            torch.Tensor: logarithm probability at weights\n        \"\"\"\n        raise NotImplementedError(\"Probability sampling is not implemented yet\")\n\n    def log_z_test(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return logarithm of z-test statistic. For numerical stability it is\n        -self.scale_alphas_log. This value is compared with threshold to\n        consider should be parameter pruned or not.\n\n        Returns:\n            torch.Tensor: logarithm of z-test statistic\n        \"\"\"\n        return -self.scale_alphas_log\n\n    def rsample(self, sample_shape: _size = torch.Size()) -&gt; torch.Tensor:\n        \"\"\"\n        Returns parameters sampled using reparametrization trick, so they could be used for\n        gradient estimation\n\n        Returns:\n            torch.Tensor: sampled parameters\n        \"\"\"\n        shape = self._extended_shape(sample_shape)\n        param_epsilons = _standard_normal(\n            shape, dtype=self.param_mus.dtype, device=self.param_mus.device\n        )\n        scale_epsilons = _standard_normal(\n            shape, dtype=self.scale_mus.dtype, device=self.scale_mus.device\n        )\n        # calculate sample using reparametrization\n        scale_sample = self.scale_mus + scale_epsilons * (self.scale_mus) * torch.sqrt(\n            torch.exp(self.scale_alphas_log)\n        )\n        param_sample = scale_sample * (\n            self.param_mus + param_epsilons * torch.exp(self.param_std_log)\n        )\n        return param_sample\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.map","title":"<code>map: torch.Tensor</code>  <code>property</code>","text":"<p>Return MAP(if we speaks about posterior distribution) or MLE estimation of parameters</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: MAP estimation of parameters</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.mean","title":"<code>mean: torch.Tensor</code>  <code>property</code>","text":"<p>Return mean estimation of parameters</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: mean value of parameters</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.param_mus","title":"<code>param_mus: nn.Parameter = nn.Parameter(param_mus)</code>  <code>instance-attribute</code>","text":"<p>\\(\\mu\\) parameter of distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.param_std_log","title":"<code>param_std_log: nn.Parameter = nn.Parameter(param_std_log)</code>  <code>instance-attribute</code>","text":"<p>\\(\\log(\\sigma))\\) parameter of distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.scale_alphas_log","title":"<code>scale_alphas_log: nn.Parameter = nn.Parameter(scale_alphas_log)</code>  <code>instance-attribute</code>","text":"<p>\\(\\alpha\\) parameter scale of distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.scale_mus","title":"<code>scale_mus: nn.Parameter = nn.Parameter(scale_mus)</code>  <code>instance-attribute</code>","text":"<p>\\(\\mu\\) parameter scale of distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.variance","title":"<code>variance: torch.Tensor</code>  <code>property</code>","text":"<p>Return variance of parameters</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: variance of parameters</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.__init__","title":"<code>__init__(param_mus, param_std_log, scale_mus, scale_alphas_log, validate_args=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>param_mus</code> <code>Tensor</code> <p>\\(\\mu\\) parameter of distribution</p> required <code>param_std_log</code> <code>Tensor</code> <p>\\(\\log(\\sigma)\\) parameter of distribution</p> required <code>scale_mus</code> <code>Tensor</code> <p>\\(\\mu\\) parameter scale of distribution</p> required <code>scale_alphas_log</code> <code>Tensor</code> <p>\\(\\alpha\\) parameter scale of distribution</p> required <code>validate_args</code> <code>Optional[bool]</code> <p>alias fo validate_args of torch.distributions.sistribution</p> <code>None</code> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def __init__(\n    self,\n    param_mus: torch.Tensor,\n    param_std_log: torch.Tensor,\n    scale_mus: torch.Tensor,\n    scale_alphas_log: torch.Tensor,\n    validate_args: Optional[bool] =None,\n):\n    r\"\"\"_summary_\n\n    Args:\n        param_mus: $\\mu$ parameter of distribution\n        param_std_log: $\\log(\\sigma)$ parameter of distribution\n        scale_mus: $\\mu$ parameter scale of distribution\n        scale_alphas_log: $\\alpha$ parameter scale of distribution\n        validate_args: alias fo validate_args of torch.distributions.sistribution\n    \"\"\"\n    self.param_mus: nn.Parameter = nn.Parameter(param_mus)\n    r\"\"\"$\\mu$ parameter of distribution\"\"\"\n    self.param_std_log: nn.Parameter = nn.Parameter(param_std_log)\n    r\"\"\"$\\log(\\sigma))$ parameter of distribution\"\"\"\n    self.scale_mus: nn.Parameter = nn.Parameter(scale_mus)\n    r\"\"\"$\\mu$ parameter scale of distribution\"\"\"\n    self.scale_alphas_log: nn.Parameter = nn.Parameter(scale_alphas_log)\n    r\"\"\"$\\alpha$ parameter scale of distribution\"\"\"\n\n    # (\n    #     self.param_mus,\n    #     self.param_std_log,\n    #     self.scale_mus,\n    #     self.scale_alphas_log,\n    # ) = broadcast_all(\n    #     self.param_mus, self.param_std_log, self.scale_mus, self.scale_alphas_log\n    # )\n\n    batch_shape = self.param_mus.size()\n    super().__init__(batch_shape, validate_args=validate_args)\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.from_parameter","title":"<code>from_parameter(p)</code>  <code>classmethod</code>","text":"<p>Default initialization of LogUniformVarDist forom parameters of nn.Module</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Parameter</code> <p>paramaters for which ParamDist should be created.</p> required Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>@classmethod\ndef from_parameter(cls, p: nn.Parameter) -&gt; \"LogUniformVarDist\":\n    \"\"\"\n    Default initialization of LogUniformVarDist forom parameters of nn.Module\n\n    Args:\n        p (nn.Parameter): paramaters for which ParamDist should be created.\n    \"\"\"\n    param_mus = nn.Parameter(p, requires_grad=True)\n    param_std_log = nn.Parameter(\n        torch.log(torch.Tensor(p.shape).uniform_(1e-8, 1e-2)), requires_grad=True\n    )  # (0, 0.01)\n    scale_mus = nn.Parameter(torch.ones_like(p), requires_grad=True)\n    scale_alphas_log = nn.Parameter(\n        torch.Tensor(p.shape).uniform_(-4, -2), requires_grad=True\n    )  # (-4, -2)\n    return LogUniformVarDist(param_mus, param_std_log, scale_mus, scale_alphas_log)\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.get_params","title":"<code>get_params()</code>","text":"<p>Return all parameters that should be registered as named parameters of nn.Module.</p> <p>Returns:</p> Type Description <code>dict[str, Parameter]</code> <p>dict[str, nn.Parameter]: parameters that should be registered as named parameters of nn.Module</p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def get_params(self) -&gt; dict[str, nn.Parameter]:\n    \"\"\"\n    Return all parameters that should be registered as named parameters of nn.Module.\n    {\"param_mus\", \"param_std_log\", \"scale_mus\", \"scale_alphas_log\"}\n\n    Returns:\n        dict[str, nn.Parameter]: parameters that should be registered as named parameters of nn.Module\n    \"\"\"\n    return {\n        \"param_mus\": self.param_mus,\n        \"param_std_log\": self.param_std_log,\n        \"scale_mus\": self.scale_mus,\n        \"scale_alphas_log\": self.scale_alphas_log,\n    }\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.log_prob","title":"<code>log_prob(weights)</code>","text":"<p>Return logarithm probability at weights</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: logarithm probability at weights</p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def log_prob(self, weights) -&gt; torch.Tensor:\n    \"\"\"\n    Return logarithm probability at weights\n\n    Returns:\n        torch.Tensor: logarithm probability at weights\n    \"\"\"\n    raise NotImplementedError(\"Probability sampling is not implemented yet\")\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.log_z_test","title":"<code>log_z_test()</code>","text":"<p>Return logarithm of z-test statistic. For numerical stability it is -self.scale_alphas_log. This value is compared with threshold to consider should be parameter pruned or not.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: logarithm of z-test statistic</p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def log_z_test(self) -&gt; torch.Tensor:\n    \"\"\"\n    Return logarithm of z-test statistic. For numerical stability it is\n    -self.scale_alphas_log. This value is compared with threshold to\n    consider should be parameter pruned or not.\n\n    Returns:\n        torch.Tensor: logarithm of z-test statistic\n    \"\"\"\n    return -self.scale_alphas_log\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.LogUniformVarDist.rsample","title":"<code>rsample(sample_shape=torch.Size())</code>","text":"<p>Returns parameters sampled using reparametrization trick, so they could be used for gradient estimation</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: sampled parameters</p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def rsample(self, sample_shape: _size = torch.Size()) -&gt; torch.Tensor:\n    \"\"\"\n    Returns parameters sampled using reparametrization trick, so they could be used for\n    gradient estimation\n\n    Returns:\n        torch.Tensor: sampled parameters\n    \"\"\"\n    shape = self._extended_shape(sample_shape)\n    param_epsilons = _standard_normal(\n        shape, dtype=self.param_mus.dtype, device=self.param_mus.device\n    )\n    scale_epsilons = _standard_normal(\n        shape, dtype=self.scale_mus.dtype, device=self.scale_mus.device\n    )\n    # calculate sample using reparametrization\n    scale_sample = self.scale_mus + scale_epsilons * (self.scale_mus) * torch.sqrt(\n        torch.exp(self.scale_alphas_log)\n    )\n    param_sample = scale_sample * (\n        self.param_mus + param_epsilons * torch.exp(self.param_std_log)\n    )\n    return param_sample\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist","title":"<code>NormalReparametrizedDist</code>","text":"<p>               Bases: <code>Normal</code>, <code>ParamDist</code></p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>class NormalReparametrizedDist(D.Normal, ParamDist):\n    @classmethod\n    def from_parameter(self, p: nn.Parameter) -&gt; \"NormalReparametrizedDist\":\n        \"\"\"\n        Default initialization of NormalReparametrizedDist forom parameters of nn.Module\n\n        Args:\n            p (nn.Parameter): paramaters for which ParamDist should be created.\n        \"\"\"\n        loc = nn.Parameter(p, requires_grad=True)\n        # scale4softplus = nn.Parameter(p.new(p.size()).rand_(), requires_grad=True)\n        scale4softplus = nn.Parameter(\n            torch.Tensor(p.shape).uniform_(-4, -2), requires_grad=True\n        )\n        return NormalReparametrizedDist(loc, scale4softplus)\n\n    def __init__(self, loc, log_scale, validate_args=None) -&gt; None:\n        r\"\"\"_summary_\n\n        Args:\n            loc (torch.Tensor): $\\mu$ parameter of normal distribution\n            log_scale (torch.Tensor): $\\log(\\sigma)$ parameter of distribution\n        \"\"\"\n\n        loc, log_scale = broadcast_all(loc, log_scale)\n\n        self.loc = nn.Parameter(loc)\n        \"\"\"$\\mu$ parameter of normal distribution\"\"\"\n        self._scale = nn.Parameter(log_scale)\n        \"\"\"$\\log(\\sigma)$ parameter of distribution\"\"\"\n\n        if isinstance(loc, Number) and isinstance(log_scale, Number):\n            batch_shape = torch.Size()\n        else:\n            batch_shape = self.loc.size()\n        D.Distribution.__init__(self, batch_shape, validate_args=validate_args)\n\n    @property\n    def scale(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return scale parameter of normal distribution\n\n        Returns:\n            torch.Tensor: scale parameter of normal distribution\n        \"\"\"\n        return F.softplus(self._scale)\n\n    @property\n    def log_scale(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return log-scale parameter of normal distribution\n\n        Returns:\n            torch.Tensor: log-scale parameter of normal distribution\n        \"\"\"\n        return torch.log(self.scale)\n\n    def get_params(self) -&gt; dict[str, nn.Parameter]:\n        \"\"\"\n        Return all parameters that should be registered as named parameters of nn.Module.\n        {\"loc\", \"scale_\"}\n\n        Returns:\n            dict[str, nn.Parameter]: parameters that should be registered as named parameters of nn.Module\n        \"\"\"\n        return {\"loc\": self.loc, \"scale\": self._scale}\n\n    # Legacy\n    # def rsample(self, sample_shape: _size = torch.Size()) -&gt; torch.Tensor:\n    #     shape = self._extended_shape(sample_shape)\n    #     eps = _standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)\n    #     w = self.loc + eps * self.scale\n    #     return w\n\n    def log_z_test(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return logarithm of z-test statistic. This value is compared with threshold to\n        consider should be parameter pruned or not.\n\n        Returns:\n            torch.Tensor: logarithm of z-test statistic\n        \"\"\"\n        return torch.log(torch.abs(self.mean)) - torch.log(self.variance)\n\n    @property\n    def map(self) -&gt; torch.Tensor:\n        \"\"\"\n        Return MAP(if we speaks about posterior distribution) or MLE estimation of parameters\n\n        Returns:\n            torch.Tensor: MAP estimation of parameters\n        \"\"\"\n        return self.loc\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.loc","title":"<code>loc = nn.Parameter(loc)</code>  <code>instance-attribute</code>","text":"<p>\\(\\mu\\) parameter of normal distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.log_scale","title":"<code>log_scale: torch.Tensor</code>  <code>property</code>","text":"<p>Return log-scale parameter of normal distribution</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: log-scale parameter of normal distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.map","title":"<code>map: torch.Tensor</code>  <code>property</code>","text":"<p>Return MAP(if we speaks about posterior distribution) or MLE estimation of parameters</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: MAP estimation of parameters</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.scale","title":"<code>scale: torch.Tensor</code>  <code>property</code>","text":"<p>Return scale parameter of normal distribution</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: scale parameter of normal distribution</p>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.__init__","title":"<code>__init__(loc, log_scale, validate_args=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>Tensor</code> <p>\\(\\mu\\) parameter of normal distribution</p> required <code>log_scale</code> <code>Tensor</code> <p>\\(\\log(\\sigma)\\) parameter of distribution</p> required Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def __init__(self, loc, log_scale, validate_args=None) -&gt; None:\n    r\"\"\"_summary_\n\n    Args:\n        loc (torch.Tensor): $\\mu$ parameter of normal distribution\n        log_scale (torch.Tensor): $\\log(\\sigma)$ parameter of distribution\n    \"\"\"\n\n    loc, log_scale = broadcast_all(loc, log_scale)\n\n    self.loc = nn.Parameter(loc)\n    \"\"\"$\\mu$ parameter of normal distribution\"\"\"\n    self._scale = nn.Parameter(log_scale)\n    \"\"\"$\\log(\\sigma)$ parameter of distribution\"\"\"\n\n    if isinstance(loc, Number) and isinstance(log_scale, Number):\n        batch_shape = torch.Size()\n    else:\n        batch_shape = self.loc.size()\n    D.Distribution.__init__(self, batch_shape, validate_args=validate_args)\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.from_parameter","title":"<code>from_parameter(p)</code>  <code>classmethod</code>","text":"<p>Default initialization of NormalReparametrizedDist forom parameters of nn.Module</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Parameter</code> <p>paramaters for which ParamDist should be created.</p> required Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>@classmethod\ndef from_parameter(self, p: nn.Parameter) -&gt; \"NormalReparametrizedDist\":\n    \"\"\"\n    Default initialization of NormalReparametrizedDist forom parameters of nn.Module\n\n    Args:\n        p (nn.Parameter): paramaters for which ParamDist should be created.\n    \"\"\"\n    loc = nn.Parameter(p, requires_grad=True)\n    # scale4softplus = nn.Parameter(p.new(p.size()).rand_(), requires_grad=True)\n    scale4softplus = nn.Parameter(\n        torch.Tensor(p.shape).uniform_(-4, -2), requires_grad=True\n    )\n    return NormalReparametrizedDist(loc, scale4softplus)\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.get_params","title":"<code>get_params()</code>","text":"<p>Return all parameters that should be registered as named parameters of nn.Module.</p> <p>Returns:</p> Type Description <code>dict[str, Parameter]</code> <p>dict[str, nn.Parameter]: parameters that should be registered as named parameters of nn.Module</p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def get_params(self) -&gt; dict[str, nn.Parameter]:\n    \"\"\"\n    Return all parameters that should be registered as named parameters of nn.Module.\n    {\"loc\", \"scale_\"}\n\n    Returns:\n        dict[str, nn.Parameter]: parameters that should be registered as named parameters of nn.Module\n    \"\"\"\n    return {\"loc\": self.loc, \"scale\": self._scale}\n</code></pre>"},{"location":"reference/methods/bayes/variational/distribution/#methods.bayes.variational.distribution.NormalReparametrizedDist.log_z_test","title":"<code>log_z_test()</code>","text":"<p>Return logarithm of z-test statistic. This value is compared with threshold to consider should be parameter pruned or not.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: logarithm of z-test statistic</p> Source code in <code>src/methods/bayes/variational/distribution.py</code> <pre><code>def log_z_test(self) -&gt; torch.Tensor:\n    \"\"\"\n    Return logarithm of z-test statistic. This value is compared with threshold to\n    consider should be parameter pruned or not.\n\n    Returns:\n        torch.Tensor: logarithm of z-test statistic\n    \"\"\"\n    return torch.log(torch.abs(self.mean)) - torch.log(self.variance)\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/","title":"Variance Bayessian Loss","text":""},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.LogUniformVarKLLoss","title":"<code>LogUniformVarKLLoss</code>","text":"<p>               Bases: <code>VarDistLoss</code></p> <p>KL loss between factorized variational distribution and LogUniform prior.  Works only with modules with LogUniformVarDist posterior</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>class LogUniformVarKLLoss(VarDistLoss):\n    \"\"\"KL loss between factorized variational distribution and LogUniform prior. \n    Works only with modules with LogUniformVarDist posterior\"\"\"\n    def __init__(self):\n        \"\"\"_summary_\"\"\"\n        super().__init__()\n\n    def forward(\n        self, posterior: dict[str, LogUniformVarDist], **kwargs\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes KL loss between factorized variational distribution and LogUniform prior\n\n        Args:\n            posterior (dict[str, LogUniformVarDist]): factorized normal variational distribution\n                    with hidden variable that is used with LogUniform prior\n        \"\"\"\n        k1 = torch.tensor(0.63576)\n        k2 = torch.tensor(1.87320)\n        k3 = torch.tensor(1.48695)\n        KL_z = torch.tensor(0)\n        for dist in posterior.values():\n            KL_z_element = (\n                k1 * F.sigmoid(k2 + k3 * dist.scale_alphas_log)\n                - 0.5 * F.softplus(-dist.scale_alphas_log)\n                - k1\n            )\n            KL_z = KL_z + KL_z_element.sum()\n        KL_w = torch.tensor(0)\n        for dist in posterior.values():\n            KL_w_element = 0.5 * (\n                torch.log(1 / torch.exp(dist.param_std_log) ** 2)\n                + torch.exp(dist.param_std_log) ** 2\n                + dist.param_mus**2\n                - 1\n            )\n            KL_w = KL_w + KL_w_element.sum()\n\n        return -KL_z + KL_w\n\n    def aggregate(\n        self, fit_losses: list, dist_losses: list, beta: float\n    ) -&gt; VarDistLoss.AggregationResult:\n        \"\"\"\n        This method aggregate dist_lossed and fit_losses for whole\n        sampled parameters.\n\n        Args:\n            fit_losses (list):\n                    list of data loss of each sample\n            dist_losses (list):  {param_name: ParamDist}\n                    list of distance loss of each sample\n            beta:  {param_name: ParamDist}\n                    sacle parameter of distance loss\n        Returns:\n            VarDistLoss.AggregationResult: Aggretion result for whole samples\n        \"\"\"\n        fit_loss = torch.mean(torch.stack(fit_losses))\n        dist_loss = torch.stack(dist_losses)[0]\n        total_loss = fit_loss + beta * dist_loss\n        return VarDistLoss.AggregationResult(total_loss, fit_loss, dist_loss)\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.LogUniformVarKLLoss.__init__","title":"<code>__init__()</code>","text":"<p>summary</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def __init__(self):\n    \"\"\"_summary_\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.LogUniformVarKLLoss.aggregate","title":"<code>aggregate(fit_losses, dist_losses, beta)</code>","text":"<p>This method aggregate dist_lossed and fit_losses for whole sampled parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fit_losses</code> <code>list</code> <pre><code>list of data loss of each sample\n</code></pre> required <code>dist_losses</code> <code>list</code> <p>{param_name: ParamDist}     list of distance loss of each sample</p> required <code>beta</code> <code>float</code> <p>{param_name: ParamDist}     sacle parameter of distance loss</p> required <p>Returns:     VarDistLoss.AggregationResult: Aggretion result for whole samples</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def aggregate(\n    self, fit_losses: list, dist_losses: list, beta: float\n) -&gt; VarDistLoss.AggregationResult:\n    \"\"\"\n    This method aggregate dist_lossed and fit_losses for whole\n    sampled parameters.\n\n    Args:\n        fit_losses (list):\n                list of data loss of each sample\n        dist_losses (list):  {param_name: ParamDist}\n                list of distance loss of each sample\n        beta:  {param_name: ParamDist}\n                sacle parameter of distance loss\n    Returns:\n        VarDistLoss.AggregationResult: Aggretion result for whole samples\n    \"\"\"\n    fit_loss = torch.mean(torch.stack(fit_losses))\n    dist_loss = torch.stack(dist_losses)[0]\n    total_loss = fit_loss + beta * dist_loss\n    return VarDistLoss.AggregationResult(total_loss, fit_loss, dist_loss)\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.LogUniformVarKLLoss.forward","title":"<code>forward(posterior, **kwargs)</code>","text":"<p>Computes KL loss between factorized variational distribution and LogUniform prior</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>dict[str, LogUniformVarDist]</code> <p>factorized normal variational distribution     with hidden variable that is used with LogUniform prior</p> required Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def forward(\n    self, posterior: dict[str, LogUniformVarDist], **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes KL loss between factorized variational distribution and LogUniform prior\n\n    Args:\n        posterior (dict[str, LogUniformVarDist]): factorized normal variational distribution\n                with hidden variable that is used with LogUniform prior\n    \"\"\"\n    k1 = torch.tensor(0.63576)\n    k2 = torch.tensor(1.87320)\n    k3 = torch.tensor(1.48695)\n    KL_z = torch.tensor(0)\n    for dist in posterior.values():\n        KL_z_element = (\n            k1 * F.sigmoid(k2 + k3 * dist.scale_alphas_log)\n            - 0.5 * F.softplus(-dist.scale_alphas_log)\n            - k1\n        )\n        KL_z = KL_z + KL_z_element.sum()\n    KL_w = torch.tensor(0)\n    for dist in posterior.values():\n        KL_w_element = 0.5 * (\n            torch.log(1 / torch.exp(dist.param_std_log) ** 2)\n            + torch.exp(dist.param_std_log) ** 2\n            + dist.param_mus**2\n            - 1\n        )\n        KL_w = KL_w + KL_w_element.sum()\n\n    return -KL_z + KL_w\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.NormVarKLLoss","title":"<code>NormVarKLLoss</code>","text":"<p>               Bases: <code>VarDistLoss</code></p> <p>KL loss between factorized normals.  Works only with modules with NormalReparametrizedDist posterior</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>class NormVarKLLoss(VarDistLoss):\n    \"\"\"KL loss between factorized normals. \n    Works only with modules with NormalReparametrizedDist posterior\"\"\"\n    def __init__(self):\n        \"\"\"_summary_\"\"\"\n        super().__init__()\n    def forward(\n        self, posterior: dict[str, NormalReparametrizedDist], **kwargs\n    ) -&gt; torch.Tensor:\n        r\"\"\"\n        Computes KL loss between factorized normals\n\n        Args:\n            posterior_params (nn.ParameterList): factorized normal variational distribution\n            prior_parmeter (Optional[nn.ParameterList]): assumed fixed N($\\mu$, $\\sigma$) for all paramteres.\n                As it is possible to analitically find optimal ($\\mu$, $\\sigma$), this parameter is ignored here.\n        \"\"\"\n        # optimal \\alpha parameters\n        # sigma_opt here is the squared sigma\n        mu_opt = 0\n        sigma_opt = 0\n        # number of posterior random variables = num of parameters\n        n_params = 0\n\n        # compute mu\n        for param in posterior.loc:\n            mu_opt += param.sum()\n            n_params += param.numel()\n        mu_opt /= n_params\n\n        # compute sigma in two steps\n        for param in posterior.loc:\n            sigma_opt += torch.sum((param - mu_opt) ** 2) / n_params\n        for param in posterior.log_scale:\n            sigma_opt += torch.exp(param).sum() / n_params\n\n        # compute kl-loss between posterior and prior in two steps\n        kl_loss = 0\n        for param in posterior.loc:\n            kl_loss += torch.sum(0.5 * (1 / sigma_opt) * (param - mu_opt) ** 2)\n        for param in posterior.log_scale:\n            kl_loss += -0.5 * param.sum() + param.numel() * 0.5 * torch.log(sigma_opt)\n            kl_loss += torch.sum(-0.5 + 0.5 * torch.exp(param) / sigma_opt)\n\n        return kl_loss\n\n    def aggregate(\n        self, fit_losses: list, dist_losses: list, beta: float\n    ) -&gt; VarDistLoss.AggregationResult:\n        \"\"\"\n        This method aggregate dist_lossed and fit_losses for whole\n        sampled parameters.\n\n        Args:\n            fit_losses (list):\n                    list of data loss of each sample\n            dist_losses (list):  {param_name: ParamDist}\n                    list of distance loss of each sample\n            beta:  {param_name: ParamDist}\n                    sacle parameter of distance loss\n        Returns:\n            VarDistLoss.AggregationResult: Aggretion result for whole samples\n        \"\"\"\n        fit_loss = torch.mean(torch.stack(fit_losses))\n        dist_loss = torch.stack(dist_losses)[0]\n        total_loss = fit_loss + beta * dist_loss\n        return VarDistLoss.AggregationResult(total_loss, fit_loss, dist_loss)\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.NormVarKLLoss.__init__","title":"<code>__init__()</code>","text":"<p>summary</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def __init__(self):\n    \"\"\"_summary_\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.NormVarKLLoss.aggregate","title":"<code>aggregate(fit_losses, dist_losses, beta)</code>","text":"<p>This method aggregate dist_lossed and fit_losses for whole sampled parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fit_losses</code> <code>list</code> <pre><code>list of data loss of each sample\n</code></pre> required <code>dist_losses</code> <code>list</code> <p>{param_name: ParamDist}     list of distance loss of each sample</p> required <code>beta</code> <code>float</code> <p>{param_name: ParamDist}     sacle parameter of distance loss</p> required <p>Returns:     VarDistLoss.AggregationResult: Aggretion result for whole samples</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def aggregate(\n    self, fit_losses: list, dist_losses: list, beta: float\n) -&gt; VarDistLoss.AggregationResult:\n    \"\"\"\n    This method aggregate dist_lossed and fit_losses for whole\n    sampled parameters.\n\n    Args:\n        fit_losses (list):\n                list of data loss of each sample\n        dist_losses (list):  {param_name: ParamDist}\n                list of distance loss of each sample\n        beta:  {param_name: ParamDist}\n                sacle parameter of distance loss\n    Returns:\n        VarDistLoss.AggregationResult: Aggretion result for whole samples\n    \"\"\"\n    fit_loss = torch.mean(torch.stack(fit_losses))\n    dist_loss = torch.stack(dist_losses)[0]\n    total_loss = fit_loss + beta * dist_loss\n    return VarDistLoss.AggregationResult(total_loss, fit_loss, dist_loss)\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.NormVarKLLoss.forward","title":"<code>forward(posterior, **kwargs)</code>","text":"<p>Computes KL loss between factorized normals</p> <p>Parameters:</p> Name Type Description Default <code>posterior_params</code> <code>ParameterList</code> <p>factorized normal variational distribution</p> required <code>prior_parmeter</code> <code>Optional[ParameterList]</code> <p>assumed fixed N(\\(\\mu\\), \\(\\sigma\\)) for all paramteres. As it is possible to analitically find optimal (\\(\\mu\\), \\(\\sigma\\)), this parameter is ignored here.</p> required Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def forward(\n    self, posterior: dict[str, NormalReparametrizedDist], **kwargs\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes KL loss between factorized normals\n\n    Args:\n        posterior_params (nn.ParameterList): factorized normal variational distribution\n        prior_parmeter (Optional[nn.ParameterList]): assumed fixed N($\\mu$, $\\sigma$) for all paramteres.\n            As it is possible to analitically find optimal ($\\mu$, $\\sigma$), this parameter is ignored here.\n    \"\"\"\n    # optimal \\alpha parameters\n    # sigma_opt here is the squared sigma\n    mu_opt = 0\n    sigma_opt = 0\n    # number of posterior random variables = num of parameters\n    n_params = 0\n\n    # compute mu\n    for param in posterior.loc:\n        mu_opt += param.sum()\n        n_params += param.numel()\n    mu_opt /= n_params\n\n    # compute sigma in two steps\n    for param in posterior.loc:\n        sigma_opt += torch.sum((param - mu_opt) ** 2) / n_params\n    for param in posterior.log_scale:\n        sigma_opt += torch.exp(param).sum() / n_params\n\n    # compute kl-loss between posterior and prior in two steps\n    kl_loss = 0\n    for param in posterior.loc:\n        kl_loss += torch.sum(0.5 * (1 / sigma_opt) * (param - mu_opt) ** 2)\n    for param in posterior.log_scale:\n        kl_loss += -0.5 * param.sum() + param.numel() * 0.5 * torch.log(sigma_opt)\n        kl_loss += torch.sum(-0.5 + 0.5 * torch.exp(param) / sigma_opt)\n\n    return kl_loss\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarDistLoss","title":"<code>VarDistLoss</code>","text":"<p>               Bases: <code>BaseLoss</code></p> <p>Abstract class for Distribution losses. Your distribution loss should be computed using prior and posterior classes and parameters, sampled from posterior.</p> <p>In forward method loss should realize logic of loss for one sampled weights.</p> <p>In aggregate method loss aggregate the data losses and distribution losses for samples.</p> <p>Aggregation returns <code>VarDistLoss.AggregationResult</code></p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>class VarDistLoss(BaseLoss):\n    \"\"\"\n    Abstract class for Distribution losses. Your distribution loss should\n    be computed using prior and posterior classes and parameters, sampled from posterior.\n\n    In forward method loss should realize logic of loss for one sampled weights.\n\n    In aggregate method loss aggregate the data losses and distribution losses for samples.\n\n    Aggregation returns `VarDistLoss.AggregationResult`\n\n    \"\"\"\n\n    @dataclass\n    class AggregationResult:\n        total_loss: torch.Tensor\n        fit_loss: torch.Tensor\n        dist_loss: torch.Tensor\n\n    def __init__(self):\n        \"\"\"_summary_\"\"\"\n        super().__init__()\n\n    @abstractmethod\n    def forward(\n        self,\n        *,\n        param_sample_dict: Dict[str, nn.Parameter],\n        posterior: Dict[str, ParamDist],\n        prior: Dict[str, ParamDist],\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        This method computes loss for one sampled parameters.\n\n        Args:\n            param_sample_dict: {param_name: nn.Parameter}\n                    sampled parameters on network.\n            posterior:  {param_name: ParamDist}\n                    posterior distribution of net parameters.\n            prior:  {param_name: ParamDist}\n                    prior distribution of net parameters.\n\n        Returns:\n            torch.Tensor: distanse loss for one sampled parameters\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def aggregate(\n        self, fit_losses: list, dist_losses: list, beta: float\n    ) -&gt; AggregationResult:\n        \"\"\"\n        This method aggregate dist_lossed and fit_losses for whole\n        sampled parameters.\n\n        Args:\n            fit_losses (list):\n                    list of data loss of each sample\n            dist_losses (list):  {param_name: ParamDist}\n                    list of distance loss of each sample\n            beta:  {param_name: ParamDist}\n                    sacle parameter of distance loss\n        Returns:\n            AggregationResult: Aggretion result for whole samples\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarDistLoss.__init__","title":"<code>__init__()</code>","text":"<p>summary</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def __init__(self):\n    \"\"\"_summary_\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarDistLoss.aggregate","title":"<code>aggregate(fit_losses, dist_losses, beta)</code>  <code>abstractmethod</code>","text":"<p>This method aggregate dist_lossed and fit_losses for whole sampled parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fit_losses</code> <code>list</code> <pre><code>list of data loss of each sample\n</code></pre> required <code>dist_losses</code> <code>list</code> <p>{param_name: ParamDist}     list of distance loss of each sample</p> required <code>beta</code> <code>float</code> <p>{param_name: ParamDist}     sacle parameter of distance loss</p> required <p>Returns:     AggregationResult: Aggretion result for whole samples</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>@abstractmethod\ndef aggregate(\n    self, fit_losses: list, dist_losses: list, beta: float\n) -&gt; AggregationResult:\n    \"\"\"\n    This method aggregate dist_lossed and fit_losses for whole\n    sampled parameters.\n\n    Args:\n        fit_losses (list):\n                list of data loss of each sample\n        dist_losses (list):  {param_name: ParamDist}\n                list of distance loss of each sample\n        beta:  {param_name: ParamDist}\n                sacle parameter of distance loss\n    Returns:\n        AggregationResult: Aggretion result for whole samples\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarDistLoss.forward","title":"<code>forward(*, param_sample_dict, posterior, prior)</code>  <code>abstractmethod</code>","text":"<p>This method computes loss for one sampled parameters.</p> <p>Parameters:</p> Name Type Description Default <code>param_sample_dict</code> <code>Dict[str, Parameter]</code> <p>{param_name: nn.Parameter}     sampled parameters on network.</p> required <code>posterior</code> <code>Dict[str, ParamDist]</code> <p>{param_name: ParamDist}     posterior distribution of net parameters.</p> required <code>prior</code> <code>Dict[str, ParamDist]</code> <p>{param_name: ParamDist}     prior distribution of net parameters.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: distanse loss for one sampled parameters</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    *,\n    param_sample_dict: Dict[str, nn.Parameter],\n    posterior: Dict[str, ParamDist],\n    prior: Dict[str, ParamDist],\n) -&gt; torch.Tensor:\n    \"\"\"\n    This method computes loss for one sampled parameters.\n\n    Args:\n        param_sample_dict: {param_name: nn.Parameter}\n                sampled parameters on network.\n        posterior:  {param_name: ParamDist}\n                posterior distribution of net parameters.\n        prior:  {param_name: ParamDist}\n                prior distribution of net parameters.\n\n    Returns:\n        torch.Tensor: distanse loss for one sampled parameters\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarRenuiLoss","title":"<code>VarRenuiLoss</code>","text":"<p>               Bases: <code>VarDistLoss</code></p> <p>Loss with Renui divergence. https://arxiv.org/pdf/1602.02311v1</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>class VarRenuiLoss(VarDistLoss):\n    \"\"\"\n    Loss with Renui divergence.\n    https://arxiv.org/pdf/1602.02311v1\n    \"\"\"\n\n    def __init__(self, alpha: float = -1, aggregation: str = \"weighting\"):\n        \"\"\"_summary_\n\n        Args:\n            alpha (float): alpha\n            aggregation (str): aggregation\n        \"\"\"\n        assert aggregation in [\"weighting\", \"sample\"]\n        self.alpha = alpha\n        self.aggregation = aggregation\n        super().__init__()\n\n    def forward(\n        self,\n        param_sample_dict: dict[str, nn.Parameter],\n        posterior: dict[str, ParamDist],\n        prior: dict[str, ParamDist],\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        This method computes loss for one sampled parameters.\n\n        Args:\n            param_sample_dict: {param_name: nn.Parameter}\n                    sampled parameters on network.\n            posterior:  {param_name: ParamDist}\n                    posterior distribution of net parameters.\n            prior:  {param_name: ParamDist}\n                    prior distribution of net parameters.\n\n        Returns:\n            torch.Tensor: distanse loss for one sampled parameters\n        \"\"\"\n        prior_likelihood: torch.tensor = 0.0\n        posterior_likelihood: torch.tensor = 0.0\n\n        for parameter, posterior_distr, prior_distr in zip(\n            param_sample_dict.values(), posterior.values(), prior.values()\n        ):\n            prior_likelihood += prior_distr.log_prob(parameter).sum()\n            posterior_likelihood += posterior_distr.log_prob(parameter).sum()\n            # if key == \"fc2.bias\":\n            # print(posterior_distr.loc)\n            # print(prior_likelihood)\n\n        return prior_likelihood - posterior_likelihood\n\n    def aggregate(self, fit_losses, dist_losses, beta_param) -&gt; torch.Tensor:\n        \"\"\"\n        This method aggregate dist_lossed and fit_losses for whole\n        sampled parameters.\n\n        Args:\n            fit_losses (list):\n                    list of data loss of each sample\n            dist_losses (list):  {param_name: ParamDist}\n                    list of distance loss of each sample\n            beta:  {param_name: ParamDist}\n                    sacle parameter of distance loss\n        Returns:\n            VarDistLoss.AggregationResult: Aggretion result for whole samples\n        \"\"\"\n        fit_losses = torch.stack(fit_losses)\n        dist_losses = torch.stack(dist_losses)\n\n        stat_tensor = -fit_losses + beta_param * dist_losses\n\n        if self.aggregation == \"sample\":\n            positions = F.gumbel_softmax(\n                ((1 - self.alpha) * stat_tensor.detach()), hard=True, dim=0\n            )\n        elif self.aggregation == \"weighting\":\n            positions = F.softmax(\n                ((1 - self.alpha) * stat_tensor.detach()), dim=0\n            ).detach()\n        else:\n            raise ValueError()\n\n        loss = -(positions * (stat_tensor)).sum()\n\n        # print(loss)\n        return VarDistLoss.AggregationResult(\n            total_loss=loss, fit_loss=fit_losses.mean(), dist_loss=-dist_losses.mean()\n        )\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarRenuiLoss.__init__","title":"<code>__init__(alpha=-1, aggregation='weighting')</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>alpha</p> <code>-1</code> <code>aggregation</code> <code>str</code> <p>aggregation</p> <code>'weighting'</code> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def __init__(self, alpha: float = -1, aggregation: str = \"weighting\"):\n    \"\"\"_summary_\n\n    Args:\n        alpha (float): alpha\n        aggregation (str): aggregation\n    \"\"\"\n    assert aggregation in [\"weighting\", \"sample\"]\n    self.alpha = alpha\n    self.aggregation = aggregation\n    super().__init__()\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarRenuiLoss.aggregate","title":"<code>aggregate(fit_losses, dist_losses, beta_param)</code>","text":"<p>This method aggregate dist_lossed and fit_losses for whole sampled parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fit_losses</code> <code>list</code> <pre><code>list of data loss of each sample\n</code></pre> required <code>dist_losses</code> <code>list</code> <p>{param_name: ParamDist}     list of distance loss of each sample</p> required <code>beta</code> <p>{param_name: ParamDist}     sacle parameter of distance loss</p> required <p>Returns:     VarDistLoss.AggregationResult: Aggretion result for whole samples</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def aggregate(self, fit_losses, dist_losses, beta_param) -&gt; torch.Tensor:\n    \"\"\"\n    This method aggregate dist_lossed and fit_losses for whole\n    sampled parameters.\n\n    Args:\n        fit_losses (list):\n                list of data loss of each sample\n        dist_losses (list):  {param_name: ParamDist}\n                list of distance loss of each sample\n        beta:  {param_name: ParamDist}\n                sacle parameter of distance loss\n    Returns:\n        VarDistLoss.AggregationResult: Aggretion result for whole samples\n    \"\"\"\n    fit_losses = torch.stack(fit_losses)\n    dist_losses = torch.stack(dist_losses)\n\n    stat_tensor = -fit_losses + beta_param * dist_losses\n\n    if self.aggregation == \"sample\":\n        positions = F.gumbel_softmax(\n            ((1 - self.alpha) * stat_tensor.detach()), hard=True, dim=0\n        )\n    elif self.aggregation == \"weighting\":\n        positions = F.softmax(\n            ((1 - self.alpha) * stat_tensor.detach()), dim=0\n        ).detach()\n    else:\n        raise ValueError()\n\n    loss = -(positions * (stat_tensor)).sum()\n\n    # print(loss)\n    return VarDistLoss.AggregationResult(\n        total_loss=loss, fit_loss=fit_losses.mean(), dist_loss=-dist_losses.mean()\n    )\n</code></pre>"},{"location":"reference/methods/bayes/variational/loss/#methods.bayes.variational.optimization.VarRenuiLoss.forward","title":"<code>forward(param_sample_dict, posterior, prior)</code>","text":"<p>This method computes loss for one sampled parameters.</p> <p>Parameters:</p> Name Type Description Default <code>param_sample_dict</code> <code>dict[str, Parameter]</code> <p>{param_name: nn.Parameter}     sampled parameters on network.</p> required <code>posterior</code> <code>dict[str, ParamDist]</code> <p>{param_name: ParamDist}     posterior distribution of net parameters.</p> required <code>prior</code> <code>dict[str, ParamDist]</code> <p>{param_name: ParamDist}     prior distribution of net parameters.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: distanse loss for one sampled parameters</p> Source code in <code>src/methods/bayes/variational/optimization.py</code> <pre><code>def forward(\n    self,\n    param_sample_dict: dict[str, nn.Parameter],\n    posterior: dict[str, ParamDist],\n    prior: dict[str, ParamDist],\n) -&gt; torch.Tensor:\n    \"\"\"\n    This method computes loss for one sampled parameters.\n\n    Args:\n        param_sample_dict: {param_name: nn.Parameter}\n                sampled parameters on network.\n        posterior:  {param_name: ParamDist}\n                posterior distribution of net parameters.\n        prior:  {param_name: ParamDist}\n                prior distribution of net parameters.\n\n    Returns:\n        torch.Tensor: distanse loss for one sampled parameters\n    \"\"\"\n    prior_likelihood: torch.tensor = 0.0\n    posterior_likelihood: torch.tensor = 0.0\n\n    for parameter, posterior_distr, prior_distr in zip(\n        param_sample_dict.values(), posterior.values(), prior.values()\n    ):\n        prior_likelihood += prior_distr.log_prob(parameter).sum()\n        posterior_likelihood += posterior_distr.log_prob(parameter).sum()\n        # if key == \"fc2.bias\":\n        # print(posterior_distr.loc)\n        # print(prior_likelihood)\n\n    return prior_likelihood - posterior_likelihood\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/","title":"Variance Bayessian NN","text":""},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.BaseBayesVarLayer","title":"<code>BaseBayesVarLayer</code>","text":"<p>               Bases: <code>BayesLayer</code></p> <p>Base Envelope for nn.Modules with some prior and posterior distributions as the variational distibution on paramters. This module should be used as parent class for all variational methods.</p> Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>class BaseBayesVarLayer(BayesLayer):\n    \"\"\"Base Envelope for nn.Modules with some prior and posterior\n    distributions as the variational distibution on paramters. This module should be used\n    as parent class for all variational methods.\n    \"\"\"\n\n    is_posterior_trainable = True\n\n    def flush_weights(self) -&gt; None:\n        \"\"\"\n        This method simply zeros all weights that will be calculated by this layer and\n        set them as tensors, so it will work properly when layer is initialized.\n        \"\"\"\n        for name, p in list(self.base_module.named_parameters()):\n            del_attr(self.base_module, name.split(\".\"))\n            set_attr(self.base_module, name.split(\".\"), torch.zeros_like(p))\n\n    def __init__(self, module: nn.Module) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            module (nn.Module): custom Module layer which is going to be converted to BaseBayesVarLayer\n        \"\"\"\n        super().__init__(module=module)\n        self.flush_weights()\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.BaseBayesVarLayer.__init__","title":"<code>__init__(module)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>custom Module layer which is going to be converted to BaseBayesVarLayer</p> required Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>def __init__(self, module: nn.Module) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        module (nn.Module): custom Module layer which is going to be converted to BaseBayesVarLayer\n    \"\"\"\n    super().__init__(module=module)\n    self.flush_weights()\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.BaseBayesVarLayer.flush_weights","title":"<code>flush_weights()</code>","text":"<p>This method simply zeros all weights that will be calculated by this layer and set them as tensors, so it will work properly when layer is initialized.</p> Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>def flush_weights(self) -&gt; None:\n    \"\"\"\n    This method simply zeros all weights that will be calculated by this layer and\n    set them as tensors, so it will work properly when layer is initialized.\n    \"\"\"\n    for name, p in list(self.base_module.named_parameters()):\n        del_attr(self.base_module, name.split(\".\"))\n        set_attr(self.base_module, name.split(\".\"), torch.zeros_like(p))\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.LogUniformVarLayer","title":"<code>LogUniformVarLayer</code>","text":"<p>               Bases: <code>BaseBayesVarLayer</code></p> <p>Envelope for nn.Modules with the same LogUniform prior on all scalar paramters and factorized normal distributions as the variational distibution on paramters. The prior is not required here as its optimal form can be computed analytically.</p> Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>class LogUniformVarLayer(BaseBayesVarLayer):\n    \"\"\"Envelope for nn.Modules with the same LogUniform prior on all scalar paramters and factorized normal\n    distributions as the variational distibution on paramters. The prior is not required here as\n    its optimal form can be computed analytically.\n    \"\"\"\n\n    def __init__(self, module: nn.Module) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            module (nn.Module): custom Module layer which is going to be converted to LogUniformVarLayer\n        \"\"\"\n        self.posterior_distribution_cls = LogUniformVarDist\n        self.prior_distribution_cls = None\n        self.is_prior_trainable = False\n        super().__init__(module)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.LogUniformVarLayer.__init__","title":"<code>__init__(module)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>custom Module layer which is going to be converted to LogUniformVarLayer</p> required Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>def __init__(self, module: nn.Module) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        module (nn.Module): custom Module layer which is going to be converted to LogUniformVarLayer\n    \"\"\"\n    self.posterior_distribution_cls = LogUniformVarDist\n    self.prior_distribution_cls = None\n    self.is_prior_trainable = False\n    super().__init__(module)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.NormalVarBayesLayer","title":"<code>NormalVarBayesLayer</code>","text":"<p>               Bases: <code>BaseBayesVarLayer</code></p> <p>Envelope for nn.Modules with the same normal prior on all scalar paramters and factorized normal distributions as the variational distibution on paramters. The prior is not required here as its optimal form can be computed analytically.</p> Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>class NormalVarBayesLayer(BaseBayesVarLayer):\n    \"\"\"Envelope for nn.Modules with the same normal prior on all scalar paramters and factorized normal\n    distributions as the variational distibution on paramters. The prior is not required here as\n    its optimal form can be computed analytically.\n    \"\"\"\n\n    def __init__(self, module: nn.Module) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            module (nn.Module): custom Module layer which is going to be converted to NormalVarBayesLayer\n        \"\"\"\n        self.posterior_distribution_cls = NormalReparametrizedDist\n        self.prior_distribution_cls = None\n        self.is_prior_trainable = False\n        super().__init__(module)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.NormalVarBayesLayer.__init__","title":"<code>__init__(module)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>custom Module layer which is going to be converted to NormalVarBayesLayer</p> required Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>def __init__(self, module: nn.Module) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        module (nn.Module): custom Module layer which is going to be converted to NormalVarBayesLayer\n    \"\"\"\n    self.posterior_distribution_cls = NormalReparametrizedDist\n    self.prior_distribution_cls = None\n    self.is_prior_trainable = False\n    super().__init__(module)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.VarBayesNet","title":"<code>VarBayesNet</code>","text":"<p>               Bases: <code>BaseBayesNet</code></p> <p>The whole net that contains all layers that should be tranfomed to bayesian modules. This net is used for variational bayesian methods.</p> Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>class VarBayesNet(BaseBayesNet):\n    \"\"\"The whole net that contains all layers that should be tranfomed to bayesian modules.\n    This net is used for variational bayesian methods.\n    \"\"\"\n\n    def __init__(self, base_module: nn.Module, module_dict: dict[str, nn.Module]):\n        \"\"\"_summary_\n\n        Args:\n            base_module (nn.Module): custom Module which is going to have some BayesModule as submodules\n            module_dict (dict[str, nn.Module]): all submodules of the base_module supposed to be trained. This\n                may be nn.Module or BayesModule. Such division is required because base_module is not\n                registred as Module in this class.\n        \"\"\"\n        super().__init__(base_module, module_dict)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net/#methods.bayes.variational.net.VarBayesNet.__init__","title":"<code>__init__(base_module, module_dict)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>custom Module which is going to have some BayesModule as submodules</p> required <code>module_dict</code> <code>dict[str, Module]</code> <p>all submodules of the base_module supposed to be trained. This may be nn.Module or BayesModule. Such division is required because base_module is not registred as Module in this class.</p> required Source code in <code>src/methods/bayes/variational/net.py</code> <pre><code>def __init__(self, base_module: nn.Module, module_dict: dict[str, nn.Module]):\n    \"\"\"_summary_\n\n    Args:\n        base_module (nn.Module): custom Module which is going to have some BayesModule as submodules\n        module_dict (dict[str, nn.Module]): all submodules of the base_module supposed to be trained. This\n            may be nn.Module or BayesModule. Such division is required because base_module is not\n            registred as Module in this class.\n    \"\"\"\n    super().__init__(base_module, module_dict)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net_distribution/","title":"Variance Network Distributions","text":""},{"location":"reference/methods/bayes/variational/net_distribution/#methods.bayes.variational.net_distribution.VarBayesModuleNetDistribution","title":"<code>VarBayesModuleNetDistribution</code>","text":"<p>               Bases: <code>BaseNetDistribution</code></p> <p>Class for distribution of variance nets. This class sees nets as elements of distribution. It helps sample nets from this distribution or estimate statistics of distribution. For this purpose it have base module architecture and distribution of parameters for each of it weights.</p> Source code in <code>src/methods/bayes/variational/net_distribution.py</code> <pre><code>class VarBayesModuleNetDistribution(BaseNetDistribution):\n    \"\"\"\n    Class for distribution of variance nets. This class sees nets as elements of distribution.\n    It helps sample nets from this distribution or estimate statistics of distribution.\n    For this purpose it have base module architecture and distribution of parameters for\n    each of it weights.\n    \"\"\"\n    def __init__(self, base_module: nn.Module, weight_distribution: dict[str, ParamDist]) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            base_module (nn.Module): custom module layer which is going to be converted to BayesModule\n            weight_distribution (dict[str, ParamDist]): posteror distribution for each parameter of moudule\n        \"\"\"\n        super().__init__(base_module=base_module, weight_distribution=weight_distribution)\n</code></pre>"},{"location":"reference/methods/bayes/variational/net_distribution/#methods.bayes.variational.net_distribution.VarBayesModuleNetDistribution.__init__","title":"<code>__init__(base_module, weight_distribution)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>custom module layer which is going to be converted to BayesModule</p> required <code>weight_distribution</code> <code>dict[str, ParamDist]</code> <p>posteror distribution for each parameter of moudule</p> required Source code in <code>src/methods/bayes/variational/net_distribution.py</code> <pre><code>def __init__(self, base_module: nn.Module, weight_distribution: dict[str, ParamDist]) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        base_module (nn.Module): custom module layer which is going to be converted to BayesModule\n        weight_distribution (dict[str, ParamDist]): posteror distribution for each parameter of moudule\n    \"\"\"\n    super().__init__(base_module=base_module, weight_distribution=weight_distribution)\n</code></pre>"},{"location":"reference/methods/pruner/base/","title":"Base Pruner","text":""},{"location":"reference/methods/pruner/base/#basenetdistributionpruner","title":"<code>BaseNetDistributionPruner</code>","text":"<p>Base pruner for NetDistribution. It will prune all weights wjoch have high probaility of being 0.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>class BaseNetDistributionPruner:\n    \"\"\"\n    Base pruner for NetDistribution. It will prune all weights wjoch have high probaility of being 0.\n    \"\"\"\n\n    def __init__(self, net_distribution: BaseNetDistribution) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            net_distribution (dict[str, ParamDist]): posteror distribution for net which deside how probable zero value is\n        \"\"\"\n        self.net_distribution = net_distribution\n        self.dropout_mask_dict: dict[str, nn.Parameter] = {}\n        for name_dist, dist in self.net_distribution.weight_distribution.items():\n            self.dropout_mask_dict[name_dist] = nn.Parameter(\n                torch.ones_like(dist.sample())\n            )\n\n    def prune(self, threshold: float | dict[str, float]) -&gt; None:\n        \"\"\"\n        Prune all weights which is prune estimation (log_z_test) is lower than threshold.\n        \"\"\"\n        for weight_name in self.net_distribution.weight_distribution:\n            weight_threshold = threshold\n            if isinstance(weight_threshold, dict):\n                weight_threshold = weight_threshold[weight_name]\n            self.prune_weight(weight_name, weight_threshold)\n\n    def prune_weight(self, weight_name: str, threshold: float) -&gt; None:\n        \"\"\"\n        Prune weight if its prune estimation (log_z_test) is lower than threshold.\n        \"\"\"\n        self.set_weight_dropout_mask(weight_name, threshold)\n        pt = get_attr(self.net_distribution.base_module, weight_name.split(\".\"))\n        pt = pt * self.dropout_mask_dict[weight_name]\n        pt = nn.Parameter(pt)\n        set_attr(self.net_distribution.base_module, weight_name.split(\".\"), pt)\n\n    def set_weight_dropout_mask(self, weight_name: str, threshold: float) -&gt; None:\n        \"\"\"\n        Set weight's dropout mask if its prune estimation (log_z_test) is lower than threshold.\n        \"\"\"\n        dist = self.net_distribution.weight_distribution[weight_name]\n        self.dropout_mask_dict[weight_name].data = 1.0 * (\n            dist.log_z_test() &gt;= threshold\n        )\n\n    def prune_stats(self) -&gt; int:\n        \"\"\"\n        Get number of pruned parameters.\n\n        Returns:\n            int: number of pruned parameters\n        \"\"\"\n        prune_cnt = 0\n        for dropout in self.dropout_mask_dict.values():\n            prune_cnt += (1 - dropout).sum()\n        return prune_cnt\n\n    def total_params(self) -&gt; int:\n        \"\"\"\n        Get total number of parameters.\n\n        Returns:\n            int: total number of parameter\n        \"\"\"\n        out = sum(p.numel() for p in self.dropout_mask_dict.values())\n        return out\n</code></pre>"},{"location":"reference/methods/pruner/base/#methods.bayes.base.net_distribution.BaseNetDistributionPruner.__init__","title":"<code>__init__(net_distribution)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>net_distribution</code> <code>dict[str, ParamDist]</code> <p>posteror distribution for net which deside how probable zero value is</p> required Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def __init__(self, net_distribution: BaseNetDistribution) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        net_distribution (dict[str, ParamDist]): posteror distribution for net which deside how probable zero value is\n    \"\"\"\n    self.net_distribution = net_distribution\n    self.dropout_mask_dict: dict[str, nn.Parameter] = {}\n    for name_dist, dist in self.net_distribution.weight_distribution.items():\n        self.dropout_mask_dict[name_dist] = nn.Parameter(\n            torch.ones_like(dist.sample())\n        )\n</code></pre>"},{"location":"reference/methods/pruner/base/#methods.bayes.base.net_distribution.BaseNetDistributionPruner.prune","title":"<code>prune(threshold)</code>","text":"<p>Prune all weights which is prune estimation (log_z_test) is lower than threshold.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def prune(self, threshold: float | dict[str, float]) -&gt; None:\n    \"\"\"\n    Prune all weights which is prune estimation (log_z_test) is lower than threshold.\n    \"\"\"\n    for weight_name in self.net_distribution.weight_distribution:\n        weight_threshold = threshold\n        if isinstance(weight_threshold, dict):\n            weight_threshold = weight_threshold[weight_name]\n        self.prune_weight(weight_name, weight_threshold)\n</code></pre>"},{"location":"reference/methods/pruner/base/#methods.bayes.base.net_distribution.BaseNetDistributionPruner.prune_stats","title":"<code>prune_stats()</code>","text":"<p>Get number of pruned parameters.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of pruned parameters</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def prune_stats(self) -&gt; int:\n    \"\"\"\n    Get number of pruned parameters.\n\n    Returns:\n        int: number of pruned parameters\n    \"\"\"\n    prune_cnt = 0\n    for dropout in self.dropout_mask_dict.values():\n        prune_cnt += (1 - dropout).sum()\n    return prune_cnt\n</code></pre>"},{"location":"reference/methods/pruner/base/#methods.bayes.base.net_distribution.BaseNetDistributionPruner.prune_weight","title":"<code>prune_weight(weight_name, threshold)</code>","text":"<p>Prune weight if its prune estimation (log_z_test) is lower than threshold.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def prune_weight(self, weight_name: str, threshold: float) -&gt; None:\n    \"\"\"\n    Prune weight if its prune estimation (log_z_test) is lower than threshold.\n    \"\"\"\n    self.set_weight_dropout_mask(weight_name, threshold)\n    pt = get_attr(self.net_distribution.base_module, weight_name.split(\".\"))\n    pt = pt * self.dropout_mask_dict[weight_name]\n    pt = nn.Parameter(pt)\n    set_attr(self.net_distribution.base_module, weight_name.split(\".\"), pt)\n</code></pre>"},{"location":"reference/methods/pruner/base/#methods.bayes.base.net_distribution.BaseNetDistributionPruner.set_weight_dropout_mask","title":"<code>set_weight_dropout_mask(weight_name, threshold)</code>","text":"<p>Set weight's dropout mask if its prune estimation (log_z_test) is lower than threshold.</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def set_weight_dropout_mask(self, weight_name: str, threshold: float) -&gt; None:\n    \"\"\"\n    Set weight's dropout mask if its prune estimation (log_z_test) is lower than threshold.\n    \"\"\"\n    dist = self.net_distribution.weight_distribution[weight_name]\n    self.dropout_mask_dict[weight_name].data = 1.0 * (\n        dist.log_z_test() &gt;= threshold\n    )\n</code></pre>"},{"location":"reference/methods/pruner/base/#methods.bayes.base.net_distribution.BaseNetDistributionPruner.total_params","title":"<code>total_params()</code>","text":"<p>Get total number of parameters.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>total number of parameter</p> Source code in <code>src/methods/bayes/base/net_distribution.py</code> <pre><code>def total_params(self) -&gt; int:\n    \"\"\"\n    Get total number of parameters.\n\n    Returns:\n        int: total number of parameter\n    \"\"\"\n    out = sum(p.numel() for p in self.dropout_mask_dict.values())\n    return out\n</code></pre>"},{"location":"reference/methods/trainer/base/","title":"Base Trainer","text":""},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer","title":"<code>BaseBayesTrainer</code>","text":"<p>               Bases: <code>Generic[ModelT]</code>, <code>ABC</code></p> <p>Abstcract trainer class that is used to provide simple interface to train  bayesian modules. Could be used to create your trainers.</p> Source code in <code>src/methods/bayes/base/trainer.py</code> <pre><code>class BaseBayesTrainer(Generic[ModelT], ABC):\n    \"\"\"Abstcract trainer class that is used to provide simple interface to train \n    bayesian modules. Could be used to create your trainers.\"\"\"\n    def __init__(\n        self,\n        params: TrainerParams,\n        report_chain: Optional[ReportChain],\n        train_dataset: Iterable,\n        eval_dataset: Iterable,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            params (TrainerParams): trianing params that is used to fine-tune training\n            report_chain (Optional[ReportChain]): All callback that should be return by each epoch\n            train_dataset (Iterable): Dataset on which model should be trained\n            eval_dataset (Iterable): Dataset on which epoch of training model should be evaluated\n        \"\"\"\n        self.params = params\n        \"\"\"Storing any trianing params that is used to fine-tune training\"\"\"\n        self.report_chain = report_chain\n        \"\"\"All callback that should be return by each epoch\"\"\"\n        self.train_dataset = train_dataset\n        \"\"\"Dataset on which model should be trained\"\"\"\n        self.eval_dataset = eval_dataset\n        \"\"\"Dataset on which epoch of training model should be evaluated\"\"\"\n\n    @abstractmethod\n    def train(self, *args, **kwargs) -&gt; BaseNetDistribution:\n        \"\"\"It simply train provided model with tarin parameters that is stores in params\n\n        Returns:\n            BaseNetDistribution: Distribution of nets that could be used\n            to sample models or getting map estimation (the most probable) by result of train.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer.eval_dataset","title":"<code>eval_dataset = eval_dataset</code>  <code>instance-attribute</code>","text":"<p>Dataset on which epoch of training model should be evaluated</p>"},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":"<p>Storing any trianing params that is used to fine-tune training</p>"},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer.report_chain","title":"<code>report_chain = report_chain</code>  <code>instance-attribute</code>","text":"<p>All callback that should be return by each epoch</p>"},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer.train_dataset","title":"<code>train_dataset = train_dataset</code>  <code>instance-attribute</code>","text":"<p>Dataset on which model should be trained</p>"},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer.__init__","title":"<code>__init__(params, report_chain, train_dataset, eval_dataset)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TrainerParams</code> <p>trianing params that is used to fine-tune training</p> required <code>report_chain</code> <code>Optional[ReportChain]</code> <p>All callback that should be return by each epoch</p> required <code>train_dataset</code> <code>Iterable</code> <p>Dataset on which model should be trained</p> required <code>eval_dataset</code> <code>Iterable</code> <p>Dataset on which epoch of training model should be evaluated</p> required Source code in <code>src/methods/bayes/base/trainer.py</code> <pre><code>def __init__(\n    self,\n    params: TrainerParams,\n    report_chain: Optional[ReportChain],\n    train_dataset: Iterable,\n    eval_dataset: Iterable,\n):\n    \"\"\"_summary_\n\n    Args:\n        params (TrainerParams): trianing params that is used to fine-tune training\n        report_chain (Optional[ReportChain]): All callback that should be return by each epoch\n        train_dataset (Iterable): Dataset on which model should be trained\n        eval_dataset (Iterable): Dataset on which epoch of training model should be evaluated\n    \"\"\"\n    self.params = params\n    \"\"\"Storing any trianing params that is used to fine-tune training\"\"\"\n    self.report_chain = report_chain\n    \"\"\"All callback that should be return by each epoch\"\"\"\n    self.train_dataset = train_dataset\n    \"\"\"Dataset on which model should be trained\"\"\"\n    self.eval_dataset = eval_dataset\n    \"\"\"Dataset on which epoch of training model should be evaluated\"\"\"\n</code></pre>"},{"location":"reference/methods/trainer/base/#methods.bayes.base.trainer.BaseBayesTrainer.train","title":"<code>train(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>It simply train provided model with tarin parameters that is stores in params</p> <p>Returns:</p> Name Type Description <code>BaseNetDistribution</code> <code>BaseNetDistribution</code> <p>Distribution of nets that could be used</p> <code>BaseNetDistribution</code> <p>to sample models or getting map estimation (the most probable) by result of train.</p> Source code in <code>src/methods/bayes/base/trainer.py</code> <pre><code>@abstractmethod\ndef train(self, *args, **kwargs) -&gt; BaseNetDistribution:\n    \"\"\"It simply train provided model with tarin parameters that is stores in params\n\n    Returns:\n        BaseNetDistribution: Distribution of nets that could be used\n        to sample models or getting map estimation (the most probable) by result of train.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/trainer/variational/","title":"Variational Trainer","text":""},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler","title":"<code>Beta_Scheduler</code>","text":"<p>Abstract class for beta scheduler a scale parameter between Distance loss and Data loss, the higher beta value is the more important Distance loss is. It is recommended to start with small value (&lt; 0.1) and increase it through learning.</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>class Beta_Scheduler:\n    \"\"\"\n    Abstract class for beta scheduler a scale parameter between\n    Distance loss and Data loss, the higher beta value is the more important\n    Distance loss is. It is recommended to start with small value (&lt; 0.1) and\n    increase it through learning.\n    \"\"\"\n\n    def __init__(self, beta: float, ref: Union['Beta_Scheduler', 'VarTrainerParams'] = None, *args, **kwargs) -&gt; None:\n        \"\"\"_summary_\n\n        Args:\n            beta (float): initial beta value\n            ref (Beta_Scheduler| VarTrainerParams): reference to trainer parameters which contains beta attribute \n                or another Beta_Shelduer\n        \"\"\"\n        self.ref = self\n        self.beta = beta\n        if ref is not None:\n            self.ref = ref\n            \"Refernce to trainer parameters which contain beta attribute\"\n            self.beta = ref.beta\n            \"Beta value\"\n\n    def step(self, loss) -&gt; None:\n        \"\"\"\n        Abstract class for beta scheduler a scale parameter between\n        Distance loss and Data loss, the higher this value is the more important\n        Distance loss is. It is recommended to start with small value (&lt; 0.1) and\n        increase it through learning.\n        \"\"\"\n        ...\n\n    def __float__(self) -&gt; None:\n        return self.ref.beta\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler.__init__","title":"<code>__init__(beta, ref=None, *args, **kwargs)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>initial beta value</p> required <code>ref</code> <code>Beta_Scheduler | VarTrainerParams</code> <p>reference to trainer parameters which contains beta attribute  or another Beta_Shelduer</p> <code>None</code> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def __init__(self, beta: float, ref: Union['Beta_Scheduler', 'VarTrainerParams'] = None, *args, **kwargs) -&gt; None:\n    \"\"\"_summary_\n\n    Args:\n        beta (float): initial beta value\n        ref (Beta_Scheduler| VarTrainerParams): reference to trainer parameters which contains beta attribute \n            or another Beta_Shelduer\n    \"\"\"\n    self.ref = self\n    self.beta = beta\n    if ref is not None:\n        self.ref = ref\n        \"Refernce to trainer parameters which contain beta attribute\"\n        self.beta = ref.beta\n        \"Beta value\"\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler.step","title":"<code>step(loss)</code>","text":"<p>Abstract class for beta scheduler a scale parameter between Distance loss and Data loss, the higher this value is the more important Distance loss is. It is recommended to start with small value (&lt; 0.1) and increase it through learning.</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def step(self, loss) -&gt; None:\n    \"\"\"\n    Abstract class for beta scheduler a scale parameter between\n    Distance loss and Data loss, the higher this value is the more important\n    Distance loss is. It is recommended to start with small value (&lt; 0.1) and\n    increase it through learning.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato","title":"<code>Beta_Scheduler_Plato</code>","text":"<p>               Bases: <code>Beta_Scheduler</code></p> <p>Class for plato beta scheduler a scale parameter between Distance loss and Data loss, the higher this value is the more important Distance loss is. It is recommended to start with small value (&lt; 0.1) and increase it through learning. It increase it when target loss stops improving fo more then patience steps. Use ref to specify trainer parameters which contain beta attribute in other way you should assign it manually</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>class Beta_Scheduler_Plato(Beta_Scheduler):\n    \"\"\"\n    Class for plato beta scheduler a scale parameter between\n    Distance loss and Data loss, the higher this value is the more important\n    Distance loss is. It is recommended to start with small value (&lt; 0.1) and\n    increase it through learning. It increase it when target loss stops\n    improving fo more then patience steps. Use ref to specify trainer parameters\n    which contain beta attribute in other way you should assign it manually\n    \"\"\"\n\n    def __init__(\n        self,\n        beta: float = 1e-2,\n        alpha: float = 1e-1,\n        patience: int = 10,\n        is_min: bool = True,\n        threshold: float = 0.01,\n        eps: float = 1e-08,\n        max_beta: float = 1.0,\n        min_beta: float = 1e-9,\n        ref=None,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            beta (float): initial beta value\n            alpha (float): factor of beta value by which it multiplied\n            patience (int): Number of steps of loss non-improvement which schelduer should tolerate \n                before changing beta value\n            is_min (bool): Is it minimiztion problem. So method would consider that loss\n                stops improving when it starts maximizing\n            threshold (float): Algorithm consider loss stops imporving if the next loss is \n                more(for minimiztion) then min loss more then threshold\n            eps (float): Mininum change of beta. If delta is less there will be no change\n            max_beta (float): Beta would be maxcliped to this value. To work properly\n                ref should reference to trainer parameter\n            min_beta (float): Beta would be mincliped to this value. To work properly\n                ref should reference to trainer parameter\n            ref (Beta_Scheduler| VarTrainerParams): reference to trainer parameters which contains beta attribute \n                or another Beta_Shelduer\n        \"\"\"\n        super().__init__(beta, ref)\n        self.cnt_upward = 0\n        self.prev_opt = None\n        self.patience = patience\n        \"\"\"Number of steps of loss non-improvement which schelduer should tolerate \n        before changing beta value\"\"\"\n        self.alpha = alpha\n        \"\"\"Factor by which beta value changing\"\"\"\n        self.is_min = is_min\n        \"\"\"Is it minimiztion problem. So method would consider that loss\n        stops improving&lt; when it starts maximizing\"\"\"\n        self.max_beta = max_beta\n        \"\"\"Beta would be maxcliped to this value. To work properly\n        ref should reference to trainer parameter\"\"\"\n        self.min_beta = min_beta\n        \"\"\"Beta would be mincliped to this value. To work properly\n        ref should reference to trainer parameter\"\"\"\n        self.threshold = threshold\n        \"\"\"Algorithm consider loss stops imporving if the next loss is \n        more(for minimiztion) then min loss more then threshold\"\"\"\n        self.eps = eps\n        \"\"\"Mininum change of beta. If delta is less there will be no change\"\"\"\n\n    def step(self, loss):\n        if (self.prev_opt is not None) and (\n            loss &gt; self.prev_opt - abs(self.prev_opt) * self.threshold\n        ):\n            self.cnt_upward += 1\n        else:\n            self.cnt_upward = 0\n        if (self.cnt_upward &lt; self.patience) ^ (self.is_min):\n            new_beta = self.alpha * self.ref.beta\n            if abs(new_beta - self.ref.beta) &gt; self.eps:\n                new_beta = min(new_beta, self.max_beta)\n                new_beta = max(new_beta, self.min_beta)\n                self.ref.beta = new_beta\n        if self.prev_opt is None:\n            self.prev_opt = loss\n        elif self.is_min:\n            self.prev_opt = min(self.prev_opt, loss)\n        else:\n            self.prev_opt = max(self.prev_opt, loss)\n\n    def __float__(self):\n        return self.ref.beta\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.alpha","title":"<code>alpha = alpha</code>  <code>instance-attribute</code>","text":"<p>Factor by which beta value changing</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":"<p>Mininum change of beta. If delta is less there will be no change</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.is_min","title":"<code>is_min = is_min</code>  <code>instance-attribute</code>","text":"<p>Is it minimiztion problem. So method would consider that loss stops improving&lt; when it starts maximizing</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.max_beta","title":"<code>max_beta = max_beta</code>  <code>instance-attribute</code>","text":"<p>Beta would be maxcliped to this value. To work properly ref should reference to trainer parameter</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.min_beta","title":"<code>min_beta = min_beta</code>  <code>instance-attribute</code>","text":"<p>Beta would be mincliped to this value. To work properly ref should reference to trainer parameter</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.patience","title":"<code>patience = patience</code>  <code>instance-attribute</code>","text":"<p>Number of steps of loss non-improvement which schelduer should tolerate  before changing beta value</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":"<p>Algorithm consider loss stops imporving if the next loss is  more(for minimiztion) then min loss more then threshold</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.Beta_Scheduler_Plato.__init__","title":"<code>__init__(beta=0.01, alpha=0.1, patience=10, is_min=True, threshold=0.01, eps=1e-08, max_beta=1.0, min_beta=1e-09, ref=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>initial beta value</p> <code>0.01</code> <code>alpha</code> <code>float</code> <p>factor of beta value by which it multiplied</p> <code>0.1</code> <code>patience</code> <code>int</code> <p>Number of steps of loss non-improvement which schelduer should tolerate  before changing beta value</p> <code>10</code> <code>is_min</code> <code>bool</code> <p>Is it minimiztion problem. So method would consider that loss stops improving when it starts maximizing</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Algorithm consider loss stops imporving if the next loss is  more(for minimiztion) then min loss more then threshold</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Mininum change of beta. If delta is less there will be no change</p> <code>1e-08</code> <code>max_beta</code> <code>float</code> <p>Beta would be maxcliped to this value. To work properly ref should reference to trainer parameter</p> <code>1.0</code> <code>min_beta</code> <code>float</code> <p>Beta would be mincliped to this value. To work properly ref should reference to trainer parameter</p> <code>1e-09</code> <code>ref</code> <code>Beta_Scheduler | VarTrainerParams</code> <p>reference to trainer parameters which contains beta attribute  or another Beta_Shelduer</p> <code>None</code> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def __init__(\n    self,\n    beta: float = 1e-2,\n    alpha: float = 1e-1,\n    patience: int = 10,\n    is_min: bool = True,\n    threshold: float = 0.01,\n    eps: float = 1e-08,\n    max_beta: float = 1.0,\n    min_beta: float = 1e-9,\n    ref=None,\n):\n    \"\"\"_summary_\n\n    Args:\n        beta (float): initial beta value\n        alpha (float): factor of beta value by which it multiplied\n        patience (int): Number of steps of loss non-improvement which schelduer should tolerate \n            before changing beta value\n        is_min (bool): Is it minimiztion problem. So method would consider that loss\n            stops improving when it starts maximizing\n        threshold (float): Algorithm consider loss stops imporving if the next loss is \n            more(for minimiztion) then min loss more then threshold\n        eps (float): Mininum change of beta. If delta is less there will be no change\n        max_beta (float): Beta would be maxcliped to this value. To work properly\n            ref should reference to trainer parameter\n        min_beta (float): Beta would be mincliped to this value. To work properly\n            ref should reference to trainer parameter\n        ref (Beta_Scheduler| VarTrainerParams): reference to trainer parameters which contains beta attribute \n            or another Beta_Shelduer\n    \"\"\"\n    super().__init__(beta, ref)\n    self.cnt_upward = 0\n    self.prev_opt = None\n    self.patience = patience\n    \"\"\"Number of steps of loss non-improvement which schelduer should tolerate \n    before changing beta value\"\"\"\n    self.alpha = alpha\n    \"\"\"Factor by which beta value changing\"\"\"\n    self.is_min = is_min\n    \"\"\"Is it minimiztion problem. So method would consider that loss\n    stops improving&lt; when it starts maximizing\"\"\"\n    self.max_beta = max_beta\n    \"\"\"Beta would be maxcliped to this value. To work properly\n    ref should reference to trainer parameter\"\"\"\n    self.min_beta = min_beta\n    \"\"\"Beta would be mincliped to this value. To work properly\n    ref should reference to trainer parameter\"\"\"\n    self.threshold = threshold\n    \"\"\"Algorithm consider loss stops imporving if the next loss is \n    more(for minimiztion) then min loss more then threshold\"\"\"\n    self.eps = eps\n    \"\"\"Mininum change of beta. If delta is less there will be no change\"\"\"\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLoss","title":"<code>CallbackLoss</code>","text":"<p>Abstract class for additional losses that should be calculated each train step</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>class CallbackLoss:\n    \"\"\"Abstract class for additional losses that should\n    be calculated each train step\"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None: ...\n\n    def __call__(self):\n        \"\"\"Function should return aggregated loss that is\n        calculed through whole states and return it\n        \"\"\"\n        ...\n\n    def step(self, *args, **kwargs) -&gt; None:\n        \"\"\"Method should calculate train step loss\n        using information that train step provided\"\"\"\n        ...\n\n    def zero(self) -&gt; None:\n        \"\"\"Method should resets values to initial\"\"\"\n        ...\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLoss.__call__","title":"<code>__call__()</code>","text":"<p>Function should return aggregated loss that is calculed through whole states and return it</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def __call__(self):\n    \"\"\"Function should return aggregated loss that is\n    calculed through whole states and return it\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLoss.step","title":"<code>step(*args, **kwargs)</code>","text":"<p>Method should calculate train step loss using information that train step provided</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def step(self, *args, **kwargs) -&gt; None:\n    \"\"\"Method should calculate train step loss\n    using information that train step provided\"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLoss.zero","title":"<code>zero()</code>","text":"<p>Method should resets values to initial</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def zero(self) -&gt; None:\n    \"\"\"Method should resets values to initial\"\"\"\n    ...\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLossAccuracy","title":"<code>CallbackLossAccuracy</code>","text":"<p>               Bases: <code>CallbackLoss</code></p> <p>Class for accuracy losses for classification problem to add them in callback</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>class CallbackLossAccuracy(CallbackLoss):\n    \"\"\"Class for accuracy losses for classification problem\n    to add them in callback\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.zero()\n\n    def __call__(self) -&gt; float:\n        \"\"\"Function returns mean accuracy for whole\n        train steps.\n\n        Returns:\n            float: mean accuracy\n        \"\"\"\n        return self.sum_acc / self.samples\n\n    def step(self, output, label) -&gt; None:\n        \"\"\"Method should calculate accuracy for train\n\n        Args:\n            output (torch.tensor): predicted logits for each class\n            label (torch.tensor): validatation labels for each object\n        \"\"\"\n        _, predicted = torch.max(output.data, 1)\n        self.sum_acc += (predicted == label).sum().item() / label.size(0)\n        self.samples += 1\n\n    def zero(self) -&gt; None:\n        \"\"\"Method resets values to initial\"\"\"\n        self.sum_acc = 0\n        self.samples = 0\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLossAccuracy.__call__","title":"<code>__call__()</code>","text":"<p>Function returns mean accuracy for whole train steps.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>mean accuracy</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def __call__(self) -&gt; float:\n    \"\"\"Function returns mean accuracy for whole\n    train steps.\n\n    Returns:\n        float: mean accuracy\n    \"\"\"\n    return self.sum_acc / self.samples\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLossAccuracy.step","title":"<code>step(output, label)</code>","text":"<p>Method should calculate accuracy for train</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>tensor</code> <p>predicted logits for each class</p> required <code>label</code> <code>tensor</code> <p>validatation labels for each object</p> required Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def step(self, output, label) -&gt; None:\n    \"\"\"Method should calculate accuracy for train\n\n    Args:\n        output (torch.tensor): predicted logits for each class\n        label (torch.tensor): validatation labels for each object\n    \"\"\"\n    _, predicted = torch.max(output.data, 1)\n    self.sum_acc += (predicted == label).sum().item() / label.size(0)\n    self.samples += 1\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.CallbackLossAccuracy.zero","title":"<code>zero()</code>","text":"<p>Method resets values to initial</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def zero(self) -&gt; None:\n    \"\"\"Method resets values to initial\"\"\"\n    self.sum_acc = 0\n    self.samples = 0\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer","title":"<code>VarBayesTrainer</code>","text":"<p>               Bases: <code>BaseBayesTrainer[VarBayesNet]</code></p> <p>Trainer that is used for all variational methods all it parameters are stored in params(VarTrainerParams) attribute</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>class VarBayesTrainer(BaseBayesTrainer[VarBayesNet]):\n    \"\"\"Trainer that is used for all variational methods all it parameters are stored\n    in params(VarTrainerParams) attribute\"\"\"\n\n    @dataclass\n    class EvalResult:\n        val_loss: float\n        fit_loss: float\n        dist_loss: float\n        cnt_prune_parameters: int\n        cnt_params: int\n        custom_losses: dict[str, Any]\n\n    @dataclass\n    class TrainResult:\n        total_loss: float\n        fit_loss: float\n        dist_loss: float\n\n    def __init__(\n        self,\n        params: VarTrainerParams,\n        report_chain: Optional[ReportChain],\n        train_dataset: Iterable,\n        eval_dataset: Iterable,\n        post_train_step_func: Union[\n            None, list[Callable[[BaseBayesTrainer, TrainResult], None]]\n        ] = None,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            params (TrainerParams): trianing params that is used to fine-tune training\n            report_chain (Optional[ReportChain]): All callback that should be return by each epoch\n            train_dataset (Iterable): Dataset on which model should be trained\n            eval_dataset (Iterable): Dataset on which epoch of training model should be evaluated\n            post_train_step_func (Union[None, list[Callable[[BaseBayesTrainer, TrainResult], None]]):\n                functions that should be executed after each train step\n        ] \n        \"\"\"\n        super().__init__(params, report_chain, train_dataset, eval_dataset)\n\n        # B8006\n        if post_train_step_func is None:\n            post_train_step_func = []\n\n        self.post_train_step_func = post_train_step_func\n\n    def train(self, model: VarBayesNet) -&gt; VarBayesModuleNetDistribution:\n        \"\"\"It simply train provided model with tarin parameters that is stores in params\n\n        Args:\n            model (VarBayesModuleNet): Any variational bayesian model that should be trained\n\n        Returns:\n            VarBayesModuleNetDistribution: Distribution of variational nets that could be used\n            to sample models or getting map estimation (the most probable) by result of train.\n        \"\"\"\n        losses = []\n        val_losses = []\n        # Train the model\n        for epoch in tqdm(range(self.params.num_epochs)):\n            if self.params.callback_losses is not None:\n                for custom_loss in self.params.callback_losses.values():\n                    custom_loss.zero()\n            train_num_batch = 0\n            train_loss = 0\n            train_dist_loss = 0\n            train_fit_loss = 0\n            for _, (objects, labels) in enumerate(self.train_dataset):\n                train_output = self.train_step(model, objects, labels)\n                self.__post_train_step(train_output)\n                losses.append(train_output.total_loss.item())\n                train_loss += train_output.total_loss\n                train_dist_loss += train_output.dist_loss\n                train_fit_loss += train_output.fit_loss\n                train_num_batch += 1\n            train_loss /= train_num_batch\n            train_dist_loss /= train_num_batch\n            train_fit_loss /= train_num_batch\n\n            # Save model\n            if epoch % 10 == 0:\n                torch.save(model.state_dict(), \"model.pt\")\n\n            # Train step callback\n            callback_dict = {\n                \"num_epoch\": epoch + 1,\n                \"total_num_epoch\": self.params.num_epochs,\n                \"total_loss\": train_loss,\n                \"kl_loss\": train_dist_loss,\n                \"fit_loss\": train_fit_loss,\n            }\n            if self.params.callback_losses is not None:\n                for loss_name, custom_loss in self.params.callback_losses.items():\n                    callback_dict[loss_name] = custom_loss()\n\n            # Eval Step\n            eval_result = self.eval(model, self.eval_dataset)\n            val_losses.append(eval_result.val_loss)\n            # Eval step callback\n            callback_dict.update({\"val_total_loss\": eval_result.val_loss})\n            if self.params.callback_losses is not None:\n                for loss_name, custom_loss in self.params.callback_losses.items():\n                    callback_dict[\"val_\" + loss_name] = custom_loss()\n\n            # Some additional information to callback\n            callback_dict.update(\n                {\n                    \"cnt_prune_parameters\": eval_result.cnt_prune_parameters,\n                    \"cnt_params\": eval_result.cnt_params,\n                    \"beta\": self.params.beta,\n                    \"losses\": losses,\n                    \"val_losses\": val_losses,\n                }\n            )\n\n            # if is not None let's callback\n            if isinstance(self.report_chain, ReportChain):\n                self.report_chain.report(callback_dict)\n        return VarBayesModuleNetDistribution(model.base_module, model.posterior)\n\n    def train_step(self, model: VarBayesNet, objects, labels) -&gt; TrainResult:\n        \"\"\"\n        Train step for specific batch\n        \"\"\"\n        device = model.device\n        # Forward pass\n        objects = objects.to(device)\n        labels = labels.to(device)\n        fit_loss_total = 0\n        dist_losses = []\n        fit_losses = []\n        for _ in range(self.params.num_samples):\n            param_sample_dict = model.sample()\n            outputs = model(objects)\n            fit_losses.append(self.params.fit_loss(outputs, labels))\n            dist_losses.append(\n                self.params.dist_loss(\n                    param_sample_dict=param_sample_dict,\n                    posterior=model.posterior,\n                    prior=model.prior,\n                )\n            )\n\n            if self.params.callback_losses is not None:\n                for custom_loss in self.params.callback_losses.values():\n                    custom_loss.step(outputs, labels)\n        aggregation_output = self.params.dist_loss.aggregate(\n            fit_losses, dist_losses, float(self.params.beta)\n        )\n        total_loss, fit_loss_total, dist_loss_total = (\n            aggregation_output.total_loss,\n            aggregation_output.fit_loss,\n            aggregation_output.dist_loss,\n        )\n        # Backward pass and optimization\n        self.params.optimizer.zero_grad()\n        total_loss.backward()\n        self.params.optimizer.step()\n\n        # print(dict(model.named_parameters()))\n        \"\"\"\n        for name, params in dict(model.named_parameters()).items():\n            print(name)\n            print(params.grad)\n        \"\"\"\n\n        return VarBayesTrainer.TrainResult(total_loss, fit_loss_total, dist_loss_total)\n\n    def __post_train_step(self, train_result: TrainResult) -&gt; None:\n        \"\"\"Functions that should exectuted after each taraining\"\"\"\n        for func in self.post_train_step_func:\n            func(self, train_result)\n\n    def eval(\n        self, model: VarBayesNet, eval_dataset\n    ) -&gt; \"VarBayesTrainer.EvalResult\":\n        \"\"\"Evalute model on dataset using stored train parameters\n        Args:\n            model (VarBayesModuleNet): Variational bayesian model that should be evaulted\n            eval_dataset: datatest on which model should be evaluted\n        Returns:\n            VarBayesTrainer.EvalResult: Evaluation result that is stored in VarBayesTrainer.EvalResult format\"\"\"\n        # Evaluate the model on the validation set\n        device = model.device\n        if self.params.callback_losses is not None:\n            for custom_loss in self.params.callback_losses.values():\n                custom_loss.zero()\n        net_distributon = VarBayesModuleNetDistribution(\n            model.base_module, model.posterior\n        )\n        net_distributon_pruner = BaseNetDistributionPruner(net_distributon)\n        with torch.no_grad():\n            batches = 0\n            dist_losses = []\n            fit_losses = []\n            for objects, labels in eval_dataset:\n                labels = labels.to(device)\n                objects = objects.to(device)\n                # Sampling model's parameters to evaluate distance between variational and prior\n                for _ in range(self.params.num_samples):\n                    param_sample_dict = net_distributon.sample_params()\n                    dist_losses.append(\n                        self.params.dist_loss(\n                            param_sample_dict=param_sample_dict,\n                            posterior=model.posterior,\n                            prior=model.prior,\n                        )\n                    )\n\n                # Set model parameters to map and prune it\n                net_distributon.set_map_params()\n                net_distributon_pruner.prune(self.params.prune_threshold)\n                # get basic model for evaluation\n                eval_model = net_distributon.get_model()\n                outputs = eval_model(objects)\n                for _ in range(self.params.num_samples):\n                    fit_losses.append(self.params.fit_loss(outputs, labels))\n\n                batches += 1\n            aggregation_output = self.params.dist_loss.aggregate(\n                fit_losses, dist_losses, self.params.beta\n            )\n            # calculate fit loss based on mean and standard deviation of output\n            aggregation_output = self.params.dist_loss.aggregate(\n                fit_losses, dist_losses, self.params.beta\n            )\n            val_total_loss, fit_loss_total, dist_loss_total = (\n                aggregation_output.total_loss,\n                aggregation_output.fit_loss,\n                aggregation_output.dist_loss,\n            )\n            if self.params.callback_losses is not None:\n                for custom_loss in self.params.callback_losses.values():\n                    custom_loss.step(outputs, labels)\n            cnt_prune_parameters = net_distributon_pruner.prune_stats()\n            cnt_params = net_distributon_pruner.total_params()\n        custom_losses = {}\n        if self.params.callback_losses is not None:\n            for loss_name, custom_loss in self.params.callback_losses.items():\n                custom_losses[\"val_\" + loss_name] = custom_loss()\n        out = VarBayesTrainer.EvalResult(\n            val_total_loss,\n            fit_loss_total,\n            dist_loss_total,\n            cnt_prune_parameters,\n            cnt_params,\n            custom_losses,\n        )\n        return out\n\n    def eval_thresholds(\n        self, model: VarBayesNet, thresholds: List[float]\n    ) -&gt; List[\"VarBayesTrainer.EvalResult\"]:\n        \"\"\"Simillar to eval() but evaluate for a list of prune threshold\n\n        Args:\n            model (VarBayesModuleNet): Variational bayesian model that should be evaulted\n            thresholds (List[float]): list of prune thresholds on which model should be evaluted\n\n        Returns:\n            List[VarBayesTrainer.EvalResult]: Evaluation result that is stored in VarBayesTrainer.EvalResult format\"\"\"\n        old_thr = self.params.prune_threshold\n\n        eval_resilts = []\n        for thr in thresholds:\n            self.params.prune_threshold = thr\n            tmp_eval = self.eval(model, self.eval_dataset)\n            eval_resilts.append(tmp_eval)\n\n        self.params.prune_threshold = old_thr\n\n        return eval_resilts\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer.__init__","title":"<code>__init__(params, report_chain, train_dataset, eval_dataset, post_train_step_func=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>TrainerParams</code> <p>trianing params that is used to fine-tune training</p> required <code>report_chain</code> <code>Optional[ReportChain]</code> <p>All callback that should be return by each epoch</p> required <code>train_dataset</code> <code>Iterable</code> <p>Dataset on which model should be trained</p> required <code>eval_dataset</code> <code>Iterable</code> <p>Dataset on which epoch of training model should be evaluated</p> required <code>post_train_step_func</code> <code>Union[None, list[Callable[[BaseBayesTrainer, TrainResult], None]]</code> <p>functions that should be executed after each train step</p> <code>None</code> <p>]</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def __init__(\n    self,\n    params: VarTrainerParams,\n    report_chain: Optional[ReportChain],\n    train_dataset: Iterable,\n    eval_dataset: Iterable,\n    post_train_step_func: Union[\n        None, list[Callable[[BaseBayesTrainer, TrainResult], None]]\n    ] = None,\n):\n    \"\"\"_summary_\n\n    Args:\n        params (TrainerParams): trianing params that is used to fine-tune training\n        report_chain (Optional[ReportChain]): All callback that should be return by each epoch\n        train_dataset (Iterable): Dataset on which model should be trained\n        eval_dataset (Iterable): Dataset on which epoch of training model should be evaluated\n        post_train_step_func (Union[None, list[Callable[[BaseBayesTrainer, TrainResult], None]]):\n            functions that should be executed after each train step\n    ] \n    \"\"\"\n    super().__init__(params, report_chain, train_dataset, eval_dataset)\n\n    # B8006\n    if post_train_step_func is None:\n        post_train_step_func = []\n\n    self.post_train_step_func = post_train_step_func\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer.__post_train_step","title":"<code>__post_train_step(train_result)</code>","text":"<p>Functions that should exectuted after each taraining</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def __post_train_step(self, train_result: TrainResult) -&gt; None:\n    \"\"\"Functions that should exectuted after each taraining\"\"\"\n    for func in self.post_train_step_func:\n        func(self, train_result)\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer.eval","title":"<code>eval(model, eval_dataset)</code>","text":"<p>Evalute model on dataset using stored train parameters Args:     model (VarBayesModuleNet): Variational bayesian model that should be evaulted     eval_dataset: datatest on which model should be evaluted Returns:     VarBayesTrainer.EvalResult: Evaluation result that is stored in VarBayesTrainer.EvalResult format</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def eval(\n    self, model: VarBayesNet, eval_dataset\n) -&gt; \"VarBayesTrainer.EvalResult\":\n    \"\"\"Evalute model on dataset using stored train parameters\n    Args:\n        model (VarBayesModuleNet): Variational bayesian model that should be evaulted\n        eval_dataset: datatest on which model should be evaluted\n    Returns:\n        VarBayesTrainer.EvalResult: Evaluation result that is stored in VarBayesTrainer.EvalResult format\"\"\"\n    # Evaluate the model on the validation set\n    device = model.device\n    if self.params.callback_losses is not None:\n        for custom_loss in self.params.callback_losses.values():\n            custom_loss.zero()\n    net_distributon = VarBayesModuleNetDistribution(\n        model.base_module, model.posterior\n    )\n    net_distributon_pruner = BaseNetDistributionPruner(net_distributon)\n    with torch.no_grad():\n        batches = 0\n        dist_losses = []\n        fit_losses = []\n        for objects, labels in eval_dataset:\n            labels = labels.to(device)\n            objects = objects.to(device)\n            # Sampling model's parameters to evaluate distance between variational and prior\n            for _ in range(self.params.num_samples):\n                param_sample_dict = net_distributon.sample_params()\n                dist_losses.append(\n                    self.params.dist_loss(\n                        param_sample_dict=param_sample_dict,\n                        posterior=model.posterior,\n                        prior=model.prior,\n                    )\n                )\n\n            # Set model parameters to map and prune it\n            net_distributon.set_map_params()\n            net_distributon_pruner.prune(self.params.prune_threshold)\n            # get basic model for evaluation\n            eval_model = net_distributon.get_model()\n            outputs = eval_model(objects)\n            for _ in range(self.params.num_samples):\n                fit_losses.append(self.params.fit_loss(outputs, labels))\n\n            batches += 1\n        aggregation_output = self.params.dist_loss.aggregate(\n            fit_losses, dist_losses, self.params.beta\n        )\n        # calculate fit loss based on mean and standard deviation of output\n        aggregation_output = self.params.dist_loss.aggregate(\n            fit_losses, dist_losses, self.params.beta\n        )\n        val_total_loss, fit_loss_total, dist_loss_total = (\n            aggregation_output.total_loss,\n            aggregation_output.fit_loss,\n            aggregation_output.dist_loss,\n        )\n        if self.params.callback_losses is not None:\n            for custom_loss in self.params.callback_losses.values():\n                custom_loss.step(outputs, labels)\n        cnt_prune_parameters = net_distributon_pruner.prune_stats()\n        cnt_params = net_distributon_pruner.total_params()\n    custom_losses = {}\n    if self.params.callback_losses is not None:\n        for loss_name, custom_loss in self.params.callback_losses.items():\n            custom_losses[\"val_\" + loss_name] = custom_loss()\n    out = VarBayesTrainer.EvalResult(\n        val_total_loss,\n        fit_loss_total,\n        dist_loss_total,\n        cnt_prune_parameters,\n        cnt_params,\n        custom_losses,\n    )\n    return out\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer.eval_thresholds","title":"<code>eval_thresholds(model, thresholds)</code>","text":"<p>Simillar to eval() but evaluate for a list of prune threshold</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>VarBayesModuleNet</code> <p>Variational bayesian model that should be evaulted</p> required <code>thresholds</code> <code>List[float]</code> <p>list of prune thresholds on which model should be evaluted</p> required <p>Returns:</p> Type Description <code>List[EvalResult]</code> <p>List[VarBayesTrainer.EvalResult]: Evaluation result that is stored in VarBayesTrainer.EvalResult format</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def eval_thresholds(\n    self, model: VarBayesNet, thresholds: List[float]\n) -&gt; List[\"VarBayesTrainer.EvalResult\"]:\n    \"\"\"Simillar to eval() but evaluate for a list of prune threshold\n\n    Args:\n        model (VarBayesModuleNet): Variational bayesian model that should be evaulted\n        thresholds (List[float]): list of prune thresholds on which model should be evaluted\n\n    Returns:\n        List[VarBayesTrainer.EvalResult]: Evaluation result that is stored in VarBayesTrainer.EvalResult format\"\"\"\n    old_thr = self.params.prune_threshold\n\n    eval_resilts = []\n    for thr in thresholds:\n        self.params.prune_threshold = thr\n        tmp_eval = self.eval(model, self.eval_dataset)\n        eval_resilts.append(tmp_eval)\n\n    self.params.prune_threshold = old_thr\n\n    return eval_resilts\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer.train","title":"<code>train(model)</code>","text":"<p>It simply train provided model with tarin parameters that is stores in params</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>VarBayesModuleNet</code> <p>Any variational bayesian model that should be trained</p> required <p>Returns:</p> Name Type Description <code>VarBayesModuleNetDistribution</code> <code>VarBayesModuleNetDistribution</code> <p>Distribution of variational nets that could be used</p> <code>VarBayesModuleNetDistribution</code> <p>to sample models or getting map estimation (the most probable) by result of train.</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def train(self, model: VarBayesNet) -&gt; VarBayesModuleNetDistribution:\n    \"\"\"It simply train provided model with tarin parameters that is stores in params\n\n    Args:\n        model (VarBayesModuleNet): Any variational bayesian model that should be trained\n\n    Returns:\n        VarBayesModuleNetDistribution: Distribution of variational nets that could be used\n        to sample models or getting map estimation (the most probable) by result of train.\n    \"\"\"\n    losses = []\n    val_losses = []\n    # Train the model\n    for epoch in tqdm(range(self.params.num_epochs)):\n        if self.params.callback_losses is not None:\n            for custom_loss in self.params.callback_losses.values():\n                custom_loss.zero()\n        train_num_batch = 0\n        train_loss = 0\n        train_dist_loss = 0\n        train_fit_loss = 0\n        for _, (objects, labels) in enumerate(self.train_dataset):\n            train_output = self.train_step(model, objects, labels)\n            self.__post_train_step(train_output)\n            losses.append(train_output.total_loss.item())\n            train_loss += train_output.total_loss\n            train_dist_loss += train_output.dist_loss\n            train_fit_loss += train_output.fit_loss\n            train_num_batch += 1\n        train_loss /= train_num_batch\n        train_dist_loss /= train_num_batch\n        train_fit_loss /= train_num_batch\n\n        # Save model\n        if epoch % 10 == 0:\n            torch.save(model.state_dict(), \"model.pt\")\n\n        # Train step callback\n        callback_dict = {\n            \"num_epoch\": epoch + 1,\n            \"total_num_epoch\": self.params.num_epochs,\n            \"total_loss\": train_loss,\n            \"kl_loss\": train_dist_loss,\n            \"fit_loss\": train_fit_loss,\n        }\n        if self.params.callback_losses is not None:\n            for loss_name, custom_loss in self.params.callback_losses.items():\n                callback_dict[loss_name] = custom_loss()\n\n        # Eval Step\n        eval_result = self.eval(model, self.eval_dataset)\n        val_losses.append(eval_result.val_loss)\n        # Eval step callback\n        callback_dict.update({\"val_total_loss\": eval_result.val_loss})\n        if self.params.callback_losses is not None:\n            for loss_name, custom_loss in self.params.callback_losses.items():\n                callback_dict[\"val_\" + loss_name] = custom_loss()\n\n        # Some additional information to callback\n        callback_dict.update(\n            {\n                \"cnt_prune_parameters\": eval_result.cnt_prune_parameters,\n                \"cnt_params\": eval_result.cnt_params,\n                \"beta\": self.params.beta,\n                \"losses\": losses,\n                \"val_losses\": val_losses,\n            }\n        )\n\n        # if is not None let's callback\n        if isinstance(self.report_chain, ReportChain):\n            self.report_chain.report(callback_dict)\n    return VarBayesModuleNetDistribution(model.base_module, model.posterior)\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarBayesTrainer.train_step","title":"<code>train_step(model, objects, labels)</code>","text":"<p>Train step for specific batch</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>def train_step(self, model: VarBayesNet, objects, labels) -&gt; TrainResult:\n    \"\"\"\n    Train step for specific batch\n    \"\"\"\n    device = model.device\n    # Forward pass\n    objects = objects.to(device)\n    labels = labels.to(device)\n    fit_loss_total = 0\n    dist_losses = []\n    fit_losses = []\n    for _ in range(self.params.num_samples):\n        param_sample_dict = model.sample()\n        outputs = model(objects)\n        fit_losses.append(self.params.fit_loss(outputs, labels))\n        dist_losses.append(\n            self.params.dist_loss(\n                param_sample_dict=param_sample_dict,\n                posterior=model.posterior,\n                prior=model.prior,\n            )\n        )\n\n        if self.params.callback_losses is not None:\n            for custom_loss in self.params.callback_losses.values():\n                custom_loss.step(outputs, labels)\n    aggregation_output = self.params.dist_loss.aggregate(\n        fit_losses, dist_losses, float(self.params.beta)\n    )\n    total_loss, fit_loss_total, dist_loss_total = (\n        aggregation_output.total_loss,\n        aggregation_output.fit_loss,\n        aggregation_output.dist_loss,\n    )\n    # Backward pass and optimization\n    self.params.optimizer.zero_grad()\n    total_loss.backward()\n    self.params.optimizer.step()\n\n    # print(dict(model.named_parameters()))\n    \"\"\"\n    for name, params in dict(model.named_parameters()).items():\n        print(name)\n        print(params.grad)\n    \"\"\"\n\n    return VarBayesTrainer.TrainResult(total_loss, fit_loss_total, dist_loss_total)\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams","title":"<code>VarTrainerParams</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainerParams</code></p> <p>Class for VarBayesTrainer parameters</p> Source code in <code>src/methods/bayes/variational/trainer.py</code> <pre><code>@dataclass\nclass VarTrainerParams(TrainerParams):\n    \"\"\"Class for VarBayesTrainer parameters\"\"\"\n\n    fit_loss: Callable\n    \"\"\"Loss for data of non-bayesian model. There could be used \n    any usual loss that is appropiated for this model and task.\"\"\"\n    dist_loss: VarDistLoss\n    \"\"\"Loss for distributions of bayesian-model. This loss set up method\n    that you are choose to use. Select it carefully as not all \n    losses and distribution are compatible\"\"\"\n    num_samples: int\n    \"\"\"Number of samples that are used for estimation of losses.\n    Increasing it lowers variance and improves learning in cost of computaion time\"\"\"\n    prune_threshold: float = -2.2\n    \"\"\"Threshold by which parameters are pruned. The lower it is the more are pruned.\n    Could be any real number.\"\"\"\n    beta: float = 0.02\n    \"\"\"Beta is scale factor betwenn distance loss and data loss. The higher beta value \n    is the more important distance loss is. It is recommended to start with \n    small value (&lt; 0.1) and increase it through learning.\"\"\"\n    callback_losses: Optional[dict[CallbackLoss]] = None\n    \"\"\"All additional losses that should be added to callback\"\"\"\n</code></pre>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams.beta","title":"<code>beta: float = 0.02</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Beta is scale factor betwenn distance loss and data loss. The higher beta value  is the more important distance loss is. It is recommended to start with  small value (&lt; 0.1) and increase it through learning.</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams.callback_losses","title":"<code>callback_losses: Optional[dict[CallbackLoss]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>All additional losses that should be added to callback</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams.dist_loss","title":"<code>dist_loss: VarDistLoss</code>  <code>instance-attribute</code>","text":"<p>Loss for distributions of bayesian-model. This loss set up method that you are choose to use. Select it carefully as not all  losses and distribution are compatible</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams.fit_loss","title":"<code>fit_loss: Callable</code>  <code>instance-attribute</code>","text":"<p>Loss for data of non-bayesian model. There could be used  any usual loss that is appropiated for this model and task.</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams.num_samples","title":"<code>num_samples: int</code>  <code>instance-attribute</code>","text":"<p>Number of samples that are used for estimation of losses. Increasing it lowers variance and improves learning in cost of computaion time</p>"},{"location":"reference/methods/trainer/variational/#methods.bayes.variational.trainer.VarTrainerParams.prune_threshold","title":"<code>prune_threshold: float = -2.2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Threshold by which parameters are pruned. The lower it is the more are pruned. Could be any real number.</p>"},{"location":"user_guide/guide/","title":"User Guide","text":""},{"location":"user_guide/guide/#training-a-bayesian-model-based-on-the-pytorch-model","title":"Training a Bayesian model based on the PyTorch model","text":"<p>The library is powered by pythorch. So first we need to create a model from pythorch that we want to train. <pre><code>import torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport torch.nn as nn \nimport torch.optim as optim \nimport torch.nn.functional as F\nimport sys\n</code></pre></p> <p>Create a simple classifier, which will be our base model, which we want to train and prune.</p> <pre><code>class Classifier(nn.Module): \n    def __init__(self, classes: int = 10): \n        super().__init__() \n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) \n        self.pool = nn.MaxPool2d(2, 2) \n        #self.dropout1 = nn.Dropout2d(0.25) \n        #self.dropout2 = nn.Dropout2d(0.5) \n        self.fc1 = nn.Linear(64 * 7 * 7, 128) \n        self.fc2 = nn.Linear(128, classes) \n\n    def forward(self, x): \n        x = self.pool(F.relu(self.conv1(x))) \n        #x = self.dropout1(x) \n        x = self.pool(F.relu(self.conv2(x))) \n        #x = self.dropout2(x) \n        x = x.view(-1, 64 * 7 * 7) \n        x = F.relu(self.fc1(x)) \n        x = self.fc2(x) \n        return x\n</code></pre> <p>Load the MNIST dataset on which we want to train our classifier.</p> <pre><code>test_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Next, we need to turn the model into a Bayesian model. To do this, we need to wrap in BayesLayer (LogUniformVarLayer) the layers for which we want to apply Bayesian learning. And also BayesNet (VarBayesNet), which stores all Bayesian layers and the original network. To select a specific learning method, we need to select BaseLoss(LogUniformVarKLLoss), which knows how to work with the selected layers.</p>"},{"location":"user_guide/guide/#creating-a-bayesian-model-based-on-nnmodule","title":"Creating a Bayesian model based on nn.Module","text":"<pre><code>from src.methods.bayes.variational.net import LogUniformVarLayer, VarBayesNet #\u041f\u0435\u0440\u0432\u044b\u043c \u043c\u043e\u0434\u0443\u043b\u043e\u0435\u043c \u043c\u044b \u043e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u0442\u0435 \u0441\u043b\u043e\u0438 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u044b\u043c\u0438, \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0443\u043b\u044c \u044d\u0442\u043e \u0441\u0430\u043c\u0430 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0430\u044f \u0441\u0435\u0442\u044c\nfrom src.methods.bayes.variational.optimization import LogUniformVarKLLoss #\u042d\u0442\u043e \u043b\u043e\u0441\u0441 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0442\u0438\u043f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u0412\u0441\u0435\u0433\u0434\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043b\u043e\u0441\u0441, \u043d\u043e \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0435\u0433\u043e \u043d\u0435\u0442\n</code></pre> <p>The first thing we'll do is create our base model</p> <pre><code>module = Classifier()\n</code></pre> <p>Next, we turn some layers into Bayesian using LogUniformVarLayer. And create a list of all layers nn.ModuleList([layer1, layer2, ...]) that we want to train (including layers that are not Bayesian). Note that it is possible to wrap the whole network and pass a list consisting only of it.</p> <pre><code>var_module1 = LogUniformVarLayer(module.conv1)\nbayes_model = VarBayesNet(module, nn.ModuleDict({'conv1': var_module1}))\n</code></pre>"},{"location":"user_guide/guide/#example-of-a-training-step","title":"Example of a training step","text":"<p>Let's see what the learning step looks like for the network.</p> <pre><code>optimizer = optim.Adam(bayes_model.parameters(), lr=1e-3)\n</code></pre> <p>In general, it is no different from a regular step, we just need to correctly aggregate losses from several samples on one step.</p> <pre><code>#get one sample\n#========\nimage, label = test_dataset[10]\ny = bayes_model(torch.ones_like(image))\nkl_loss = LogUniformVarKLLoss()\n#========\n\n#list of fit_loss for each sample (we have one sample)\nfit_loss = [y.sum()]\n #list of dist_loss for each sample (we have one sample)\ndist_loss = [kl_loss(posterior = bayes_model.posterior, prior = bayes_model.prior, param_sample_dict = bayes_model.weights)]\nbeta = 0.1 # scale factor betwenn dist_loss and data_loss\n#aggregation result is stored in total_loss attribute, all others are provided for statistic of traininghow important each part is\naggregation_result = kl_loss.aggregate(fit_loss, dist_loss, beta) \nout = aggregation_result.total_loss # calculated loss for one step\n#optimizer step\noptimizer.zero_grad() \nout.backward() \noptimizer.step() \n</code></pre> <p>You can create a network allocation simply from the allocation to parameters and the base network.</p> <pre><code>net_distributon = VarBayesModuleNetDistribution(bayes_model.base_module, bayes_model.posterior)\n#This is a pruner that zeros the weights depending on the density of the distribution at 00\nnet_distributon_pruner = BaseNetDistributionPruner(net_distributon)\n#Here we set the MAP model weights  \nnet_distributon.set_map_params()\n#Prune based on a certain threshold\nnet_distributon_pruner.prune(1.9)\n#get basic model for evaluation\neval_model = net_distributon.get_model()\n</code></pre> <p>We got a model with the same architecture as the original one.</p> <pre><code>print(eval_model.conv1.weight)\n</code></pre> <pre><code>print(bayes_model.state_dict())\n</code></pre> <p>Forward is done on the last saved sample. Note that we do not copy the data anywhere, and the model is not encapsulated. Therefore, in order to unlink them, they must be copied.</p> <pre><code>print(bayes_model(torch.zeros_like(image)))\n#print(bayes_model(torch.zeros_like(image), sample = False))\nprint(module(torch.zeros_like(image)))\n</code></pre> <p>It is recommended to use a GPU for training.</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n</code></pre>"},{"location":"user_guide/guide/#training-with-the-built-in-trainer","title":"Training with the built-in trainer","text":"<p>Next we import several modules for training</p> <p>The trainer itself, Trainer Parameters, Planner beta (ratio between normal and Bayesian loss), and a callback for accuracy metrics. <pre><code>from src.methods.bayes.variational.trainer import VarBayesTrainer, VarTrainerParams, Beta_Scheduler_Plato, CallbackLossAccuracy\n</code></pre></p> <p>List of callbacks</p> <pre><code>from src.methods.report.base import ReportChain\n</code></pre> <p>And for some callback example. This callback module just outputs each step data from the trainer.</p> <pre><code>from src.methods.report.variational import VarBaseReport \n</code></pre> <p>Initialize the trainer. You don't have to write your own trainer. There is already a ready-made one for all variation methods.</p> <p>Set the training parameters first.</p> <pre><code>BATCH_SIZE=1000\nEPOCHS=4000\nLR = 1e-3 #5e-4\n# Split the training set into training and validation sets \nVAL_PERCENT = 0.2 # percentage of the data used for validation \nSAMPLES = 10\nBETA = 0.01 #5e-5 #0.01\nBETA_FAC = 5e-1\nPRUNE = 1.9#1.99, 2.1, 1.9\nPLATO_TOL = 20\n\ntrain_params = VarTrainerParams(EPOCHS, optimizer,fit_loss, kl_loss, SAMPLES, PRUNE, BETA, {'accuracy': CallbackLossAccuracy()})\n</code></pre> <p>Then we create a Bayesian network based on the one layer of usual one</p> <pre><code>base_module = Classifier()\nvar_module1 = LogUniformVarLayer(base_module.conv1)\n# First argument is the base network, second is a list of all layers (where the right ones are Bayesian)\nmodel = VarBayesNet(base_module, {'conv1': var_module1})\n</code></pre> <p>Then we create a Bayesian network based on the whole net <pre><code>base_module = Classifier()\nvar_module = LogUniformVarLayer(base_module)\n# First argument is the base network, second is a list of all layers (where the right ones are Bayesian)\nmodel = VarBayesNet(base_module, {'': var_module})\n</code></pre> Select the optimizer we want to use for the task</p> <pre><code>optimizer = optim.Adam(model.parameters(), lr=LR)\n</code></pre> <p>Select the loss we want to use. It should be compatible with the module we are using. But note that not all modules are compatible with all losses, some losses are specific to certain modules.</p> <pre><code># The first lot is a normal data lot, the second lot is a Bayesian model lot\nfit_loss = nn.CrossEntropyLoss() \nkl_loss = LogUniformVarKLLoss()\n</code></pre> <p>For stability of training it is recommended to use planarofschik beta</p> <pre><code>#Use the planner for the proportionality coefficient between fit_loss and kl_loss\nbeta = Beta_Scheduler_Plato(BETA, BETA_FAC, PLATO_TOL)\nbeta_KL = Beta_Scheduler_Plato(beta.beta, 1 / BETA_FAC, PLATO_TOL, ref = beta, threshold=1e-4)\n\n\n#This function will be executed after each coach step, \n#so we need to make a step in the planner and change the corresponding coefficient.\ndef post_train_step(trainer: VarTrainerParams, train_result: VarBayesTrainer.TrainResult):\n    beta.step(train_result.fit_loss)\n    beta_KL.step(train_result.dist_loss)\n    trainer.params.beta = float(beta)\n</code></pre> <p>Initialize training and validation dataset</p> <pre><code>#print(model.base_module.state_dict().keys())\nval_size    = int(VAL_PERCENT * len(train_dataset)) \ntrain_size  = len(train_dataset) - val_size \n\nt_dataset, v_dataset = torch.utils.data.random_split(train_dataset,  \n                                                        [train_size,  \n                                                            val_size]) \n\n#Create DataLoaders for the training and validation sets \ntrain_loader = torch.utils.data.DataLoader(t_dataset,  \n                                        batch_size=BATCH_SIZE,  \n                                        shuffle=True, \n                                        pin_memory=True) \neval_loader = torch.utils.data.DataLoader(v_dataset,  \n                                        batch_size=BATCH_SIZE,  \n                                        shuffle=False, \n                                        pin_memory=True) \n</code></pre> <p>All nn.Module methods are safely applied to Bayesian models, including the fact that they can be transferred to another device quite easily</p> <pre><code>model.to(device) \n</code></pre> <p>Once we have created the Bayesian network, defined the loss and set the dataset we can start training, using the built-in trainer.</p> <pre><code>#If we want to make the beta fixed, we need to remove the [post_train_step] argument.\n#trainer = VarBayesTrainer(train_params, ReportChain([VarBaseReport()]), train_loader, eval_loader, [post_train_step])\ntrainer = VarBayesTrainer(train_params, ReportChain([VarBaseReport()]), train_loader, eval_loader)\ntrainer.train(model)\n</code></pre> <p>You can also save these models in their entirety to disk      </p> <pre><code>torch.save(model.state_dict(), 'model_bayes.pt' )\n</code></pre> <p>And boot from the disk <pre><code>model.load_state_dict(torch.load('model_bayes.pt'))\nimage1, label1 = test_dataset[10]\nimage2, label2 = test_dataset[11]\nmodel(image1)\n</code></pre></p> <p>You can also use the eval() function of the trainer to evaluate the model on a validation dataset</p> <pre><code>val_loss = 0.0\nval_acc = 0.0\nPRUNE = 1.0\ntest_loader = torch.utils.data.DataLoader(test_dataset,  \n                                         batch_size=BATCH_SIZE,  \n                                         shuffle=False, \n                                         pin_memory=True) \nkl_loss = LogUniformVarKLLoss()\ntrainer.params.prune_threshold = PRUNE\ntest_result = trainer.eval(model, test_loader)\nacc = test_result.custom_losses['val_accuracy']\nprint(f'Loss:{test_result.val_loss}, KL Loss: {test_result.dist_loss}, FitLoss: {test_result.fit_loss}, Accuracy {acc}, Prune parameters: {test_result.cnt_prune_parameters}/{test_result.cnt_params}')\n</code></pre>"},{"location":"user_guide/guide/#pruning","title":"Pruning.","text":"<p>For model pruning, it is recommended to use some sort of deterministic model estimation.  In the example, the pruning is first performed on the -1.0 value and then the MAP estimation of the parameters is set.</p> <pre><code>model.to(device=device)\nmodel.prune({'threshold': -1.0})\nmodel.set_map_params()\n</code></pre> <p>This model can then be used as a deterministic model</p> <pre><code>image, label = test_dataset[100]\nplt.imshow(image.permute(1, 2, 0), cmap=\"gray\")\nprint(\"Label:\", label)\n</code></pre> <pre><code>Label: 5\n</code></pre> <p></p> <pre><code>torch.max(model(image.cuda()).data, 1)\n</code></pre> <pre><code>torch.return_types.max(\nvalues=tensor([2.1405], device='cuda:0'),\nindices=tensor([5], device='cuda:0'))\n</code></pre>"},{"location":"variational/examples/main/main/","title":"Variational KL","text":""},{"location":"variational/examples/main/main/#create-a-variational-bayesian-model-based-on-an-existing-one-from-pytorch","title":"Create a variational Bayesian model based on an existing one from Pytorch","text":"<pre><code>import torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport torch.nn as nn \nimport torch.optim as optim \nimport torch.nn.functional as F\nimport sys\n</code></pre> <p>Add the path to our library to the environment variable</p> <pre><code>sys.path.insert(0, \"../\")\n</code></pre> <pre><code>print(sys.path)\n</code></pre> <pre><code>['../', '/home/sasha/BMM/bayes_deep_compression/examples', '/home/sasha/anaconda3/lib/python312.zip', '/home/sasha/anaconda3/lib/python3.12', '/home/sasha/anaconda3/lib/python3.12/lib-dynload', '', '/home/sasha/anaconda3/lib/python3.12/site-packages', '/home/sasha/anaconda3/lib/python3.12/site-packages/setuptools/_vendor', '/tmp/tmpfzoa1i3d']\n</code></pre> <p>Create a simple classifier, which will be our base model, which we want to train and enforce</p> <pre><code>class Classifier(nn.Module): \n    def __init__(self, classes: int = 10): \n        super().__init__() \n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) \n        self.pool = nn.MaxPool2d(2, 2) \n        #self.dropout1 = nn.Dropout2d(0.25) \n        #self.dropout2 = nn.Dropout2d(0.5) \n        self.fc1 = nn.Linear(64 * 7 * 7, 128) \n        self.fc2 = nn.Linear(128, classes) \n\n    def forward(self, x): \n        x = self.pool(F.relu(self.conv1(x))) \n        #x = self.dropout1(x) \n        x = self.pool(F.relu(self.conv2(x))) \n        #x = self.dropout2(x) \n        x = x.view(-1, 64 * 7 * 7) \n        x = F.relu(self.fc1(x)) \n        x = self.fc2(x) \n        return x\n</code></pre> <p>Let's see how the distributions in our library work (they do not have to be imported to work and train the Bayesian model).</p> <p>The first type is the usual distributions on numbers. Let's import the one we use in our model.</p> <pre><code>from src.methods.bayes.variational.distribution import LogUniformVarDist\n</code></pre> <p>The first kind is our familiar distributions on numbers.</p> <pre><code>from src.methods.bayes.variational.net_distribution import VarBayesModuleNetDistribution #\u041d\u0435\u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043e\u043d\u043e \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043e \u0432 \u043d\u0430\u0448\u0443 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nfrom src.methods.bayes.base.net_distribution import BaseNetDistributionPruner #\u0422\u0430\u043a\u0436\u0435 \u043d\u0435 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043d\u043e \u043d\u0443\u0436\u0435\u043d, \u0435\u0441\u043b\u0438 \u0432\u044b \u0445\u043e\u0442\u0438\u0442\u0435 \u0437\u0430\u043f\u0440\u0443\u043d\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c\n</code></pre> <p>We can initialize the distribution weights simply from the model parameters. This uses the recommended initialization of distribution parameters</p> <pre><code>p = nn.Parameter(torch.tensor([0.0, 1.0])) \nLogUniformVarDist.from_parameter(p)\n</code></pre> <pre><code>LogUniformVarDist(param_mus: torch.Size([2]), param_std_log: torch.Size([2]), scale_mus: torch.Size([2]), scale_alphas_log: torch.Size([2]))\n</code></pre> <p>These three modules are the core modules for Bayesian learning</p> <pre><code>from src.methods.bayes.variational.net import LogUniformVarLayer, VarBayesNet #\u041f\u0435\u0440\u0432\u044b\u043c \u043c\u043e\u0434\u0443\u043b\u043e\u0435\u043c \u043c\u044b \u043e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u0442\u0435 \u0441\u043b\u043e\u0438 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u044b\u043c\u0438, \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0443\u043b\u044c \u044d\u0442\u043e \u0441\u0430\u043c\u0430 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0430\u044f \u0441\u0435\u0442\u044c\nfrom src.methods.bayes.variational.optimization import LogUniformVarKLLoss #\u042d\u0442\u043e \u043b\u043e\u0441\u0441 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0442\u0438\u043f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u0412\u0441\u0435\u0433\u0434\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043b\u043e\u0441\u0441, \u043d\u043e \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0435\u0433\u043e \u043d\u0435\u0442\n</code></pre> <p>Load the MNIST dataset on which we want to train our classifier</p> <pre><code>test_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre>"},{"location":"variational/examples/main/main/#example-of-creating-a-bayesian-model","title":"Example of creating a Bayesian model","text":"<p>Let's take a closer look at how to create a Bayesian network</p> <p>First of all, let's create our base model</p> <pre><code>module = Classifier()\n</code></pre> <p>Next, we turn some layers into Bayesian using LogUniformVarBayesModule. And create a list of all layers nn.ModuleList([layer1, layer2, ...]) that we want to train (including layers that are not Bayesian). Note that it is possible to wrap the whole network and pass a list consisting only of it.</p> <pre><code>#bayes_model = BayesModule(module)\n#var_module = LogUniformVarBayesModule(module)\nvar_module1 = LogUniformVarLayer(module.conv1)\n#bayes_model = VarBayesModuleNet(module, nn.ModuleList([var_module]))#First argument is the base network, second is a list of all layers (where the right ones are Bayesian)\nbayes_model = VarBayesNet(module, nn.ModuleDict({'conv1': var_module1}))\n</code></pre> <p>Let's see which parameters base module contains. As you can see it contain only weights for deterministic layers <pre><code>list(module.named_parameters())\n</code></pre>     [('conv2.weight',       Parameter containing:       tensor([[[[ 0.0234,  0.0521, -0.0416],                 [-0.0067,  0.0452,  0.0165],                 [-0.0186,  0.0080, -0.0282]],</p> <pre><code>           [[-0.0410, -0.0424,  0.0147],\n            [-0.0014,  0.0093, -0.0366],\n            [ 0.0582,  0.0491, -0.0511]],\n\n           [[-0.0524,  0.0151,  0.0347],\n            [ 0.0564, -0.0440,  0.0343],\n            [ 0.0427,  0.0040, -0.0588]],\n\n           ...,\n\n ('fc1.weight',\n  Parameter containing:\n  tensor([[-1.9572e-03,  5.5105e-03,  1.2907e-02,  ...,  9.3362e-04,\n            6.5410e-03,  1.2940e-02],\n          [-1.3658e-02,  1.1704e-02, -1.0554e-02,  ...,  9.2351e-03,\n            5.5499e-03,  9.6728e-03],\n          [ 7.4363e-03, -1.3107e-02,  5.7782e-03,  ..., -7.1625e-03,\n           -1.7367e-03, -1.0198e-02],\n          ...,\n          [-4.1796e-03, -9.8072e-03,  3.3602e-05,  ..., -1.6692e-02,\n           -1.3552e-02,  1.0872e-02],\n          [-1.0710e-03,  8.8952e-04, -9.1831e-03,  ..., -1.2799e-02,\n           -1.1214e-02, -2.0703e-04],\n          [-1.3680e-03,  1.8639e-03, -1.3678e-02,  ..., -1.7131e-02,\n           -3.2144e-03,  1.5104e-02]], requires_grad=True)),\n ('fc1.bias',\n  Parameter containing:\n  tensor([ 4.4785e-03,  3.5242e-03, -3.3225e-03,  1.6817e-02,  1.3938e-02,\n           1.4219e-02,  1.6224e-02, -2.3135e-03,  7.8831e-04, -4.9901e-03,\n          -2.9009e-03, -5.3911e-03,  2.5817e-05, -1.0779e-02,  6.1594e-04,\n           6.7744e-03,  6.4704e-03, -1.4375e-02, -3.4684e-04,  6.6257e-03,\n           8.6150e-03, -3.0658e-03,  7.4117e-03, -1.2159e-03,  1.2959e-02,\n           1.3157e-02, -3.3168e-03,  1.7615e-02, -1.2785e-02, -1.3743e-02,\n           7.0512e-03, -2.6989e-03,  1.1465e-02, -1.6236e-02, -1.4229e-02,\n          -3.0829e-04,  1.6266e-02,  7.7983e-03,  1.4834e-02,  1.6881e-03,\n          -1.4324e-02, -5.2661e-03,  1.7644e-02, -8.4930e-03,  6.4189e-03,\n          -4.1196e-03, -6.5473e-03, -1.5205e-02,  3.9125e-04,  1.7055e-02,\n          -1.8167e-03,  1.0101e-02,  1.5026e-02, -4.0402e-03, -6.2205e-03,\n          -4.7132e-03,  9.1314e-03, -9.3026e-03,  1.5071e-02, -1.2423e-02,\n          -1.2544e-02, -2.9424e-03,  9.4202e-03,  1.1231e-02,  2.3752e-05,\n           9.4760e-04,  8.7380e-03, -9.9631e-03, -8.7878e-03,  1.2486e-02,\n           3.5142e-03, -1.0424e-02,  3.9431e-03,  5.0228e-03,  5.4710e-03,\n          -8.6429e-04, -1.6535e-02,  1.5351e-02, -2.1290e-03,  8.6930e-03,\n           1.1497e-02,  1.5888e-02, -1.7183e-02,  8.6403e-03, -1.2504e-02,\n           1.4148e-02,  6.4226e-03,  8.1229e-03, -1.4210e-02,  6.0145e-03,\n          -1.1026e-03,  1.4578e-02,  4.2843e-03,  1.3727e-02,  1.1653e-02,\n          -8.0806e-03, -6.8637e-03,  1.5679e-03,  1.1133e-02, -1.4571e-02,\n          -5.8287e-03,  3.7694e-04, -1.5597e-02,  3.9533e-03, -1.1086e-03,\n          -1.7687e-02, -2.0691e-03,  7.8452e-03, -7.3790e-03, -1.1317e-02,\n          -2.5809e-03,  1.4929e-02,  4.0213e-03,  1.1033e-02,  7.0566e-03,\n           5.7826e-03, -1.3471e-02, -6.8864e-03, -4.2468e-03,  1.4285e-02,\n           2.0096e-03, -1.2925e-02,  2.0085e-03,  1.2672e-02, -5.6027e-03,\n           1.1251e-02, -1.1526e-02, -1.2023e-02], requires_grad=True)),\n ('fc2.weight',\n  Parameter containing:\n  tensor([[-0.0131, -0.0487, -0.0635,  ..., -0.0152, -0.0267,  0.0358],\n          [ 0.0727,  0.0434, -0.0356,  ...,  0.0618, -0.0324,  0.0476],\n          [-0.0554, -0.0653, -0.0443,  ...,  0.0300,  0.0828, -0.0215],\n          ...,\n          [ 0.0412,  0.0263, -0.0159,  ..., -0.0541,  0.0620,  0.0866],\n          [ 0.0701,  0.0316,  0.0735,  ..., -0.0793,  0.0681, -0.0710],\n          [ 0.0116, -0.0273,  0.0488,  ...,  0.0707,  0.0734, -0.0759]],\n         requires_grad=True)),\n ('fc2.bias',\n  Parameter containing:\n  tensor([ 0.0721,  0.0219, -0.0776, -0.0522, -0.0775, -0.0507,  0.0066, -0.0190,\n           0.0268,  0.0247], requires_grad=True))]\n</code></pre> <p>Let's take a look at the resulting network structure</p> <pre><code>print(bayes_model)\n</code></pre> <pre><code>VarBayesNet(\n  (base_module): Classifier(\n    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (fc1): Linear(in_features=3136, out_features=128, bias=True)\n    (fc2): Linear(in_features=128, out_features=10, bias=True)\n  )\n  (module_dict): ModuleDict(\n    (conv1): LogUniformVarLayer(\n      (posterior_params): ParameterList(\n          (0): Object of type: ParameterDict\n          (1): Object of type: ParameterDict\n        (0): ParameterDict(\n            (param_mus): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n            (param_std_log): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n            (scale_mus): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n        )\n        (1): ParameterDict(\n            (param_mus): Parameter containing: [torch.FloatTensor of size 32]\n            (param_std_log): Parameter containing: [torch.FloatTensor of size 32]\n            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 32]\n            (scale_mus): Parameter containing: [torch.FloatTensor of size 32]\n        )\n      )\n      (prior_params): ParameterList()\n    )\n  )\n  (module_list): ModuleList(\n    (0): LogUniformVarLayer(\n      (posterior_params): ParameterList(\n          (0): Object of type: ParameterDict\n          (1): Object of type: ParameterDict\n        (0): ParameterDict(\n            (param_mus): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n            (param_std_log): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n            (scale_mus): Parameter containing: [torch.FloatTensor of size 32x1x3x3]\n        )\n        (1): ParameterDict(\n            (param_mus): Parameter containing: [torch.FloatTensor of size 32]\n            (param_std_log): Parameter containing: [torch.FloatTensor of size 32]\n            (scale_alphas_log): Parameter containing: [torch.FloatTensor of size 32]\n            (scale_mus): Parameter containing: [torch.FloatTensor of size 32]\n        )\n      )\n      (prior_params): ParameterList()\n    )\n  )\n)\n</code></pre> <p>The selected network has no prior on parameters</p> <pre><code>bayes_model.prior\n</code></pre> <pre><code>{'weight': None, 'bias': None, 'conv1.weight': None, 'conv1.bias': None}\n</code></pre> <p>Let's see what the learning step looks like for the network</p> <pre><code>optimizer = optim.Adam(bayes_model.parameters(), lr=1e-3)\n</code></pre> <p>In general, it is no different from a regular step, we just need to correctly aggregate losses from several samples on one step</p> <pre><code>#get one sample\n#========\nimage, label = test_dataset[10]\ny = bayes_model(torch.ones_like(image))\nkl_loss = LogUniformVarKLLoss()\n#========\n\n#list of fit_loss for each sample (we have one sample)\nfit_loss = [y.sum()]\n #list of dist_loss for each sample (we have one sample)\ndist_loss = [kl_loss(posterior = bayes_model.posterior, prior = bayes_model.prior, param_sample_dict = bayes_model.weights)]\nbeta = 0.1 # scale factor betwenn dist_loss and data_loss\n#aggregation result is stored in total_loss attribute, all others are provided for statistic of traininghow important each part is\naggregation_result = kl_loss.aggregate(fit_loss, dist_loss, beta) \nout = aggregation_result.total_loss # calculated loss for one step\n#optimizer step\noptimizer.zero_grad() \nout.backward() \noptimizer.step() \n</code></pre> <p>You can create a network allocation simply from the allocation to parameters and the base network</p> <pre><code>net_distributon = VarBayesModuleNetDistribution(bayes_model.base_module, bayes_model.posterior)\n#\u042d\u0442\u043e \u043f\u0440\u0443\u043d\u0435\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u043d\u0443\u043b\u044f\u0435\u0442 \u0432\u0435\u0441\u0430 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438 0\nnet_distributon_pruner = BaseNetDistributionPruner(net_distributon)\n#\u0417\u0434\u0435\u0441\u044c \u043c\u044b \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0432\u0435\u0441\u0430 \u043c\u043e\u0434\u0435\u043b\u0438  \nnet_distributon.set_map_params()\n#\u041f\u0440\u0443\u043d\u0438\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u043e\u0433\u0430\nnet_distributon_pruner.prune(1.9)\n#get basic model for evaluation\neval_model = net_distributon.get_model()\n</code></pre> <p>We got a model with the same architecture as the original one</p> <pre><code>print(eval_model.conv1.weight)\n</code></pre> <pre><code>Parameter containing:\ntensor([[[[-0.2201, -0.1363,  0.0124],\n          [ 0.2032, -0.1667,  0.0023],\n          [ 0.0774,  0.2318, -0.1995]]],\n\n\n        [[[ 0.0459, -0.1258,  0.0935],\n          [-0.1269,  0.0314, -0.0996],\n          [-0.0275, -0.2574,  0.1112]]],\n\n\n        ...\n\n\n        [[[ 0.1589,  0.2691,  0.0279],\n          [ 0.1339, -0.1604,  0.0495],\n          [-0.2156, -0.0669,  0.1900]]],\n\n\n        [[[ 0.1797, -0.0371, -0.1127],\n          [ 0.3198,  0.0628, -0.2585],\n          [-0.1933,  0.1206, -0.2441]]],\n\n\n        [[[-0.2437, -0.0979,  0.0240],\n          [-0.2082, -0.0979,  0.2999],\n          [ 0.0398, -0.1725, -0.2454]]]], requires_grad=True)\n</code></pre> <pre><code>bayes_model.state_dict()\n</code></pre> <pre><code>OrderedDict([('base_module.conv1.weight',\n              tensor([[[[-0.2201, -0.1363,  0.0124],\n                        [ 0.2032, -0.1667,  0.0023],\n                        [ 0.0774,  0.2318, -0.1995]]],\n\n\n                      [[[ 0.0459, -0.1258,  0.0935],\n                        [-0.1269,  0.0314, -0.0996],\n                        [-0.0275, -0.2574,  0.1112]]],\n\n\n                      [[[ 0.1811,  0.0643,  0.0043],\n                        [-0.0697, -0.3153,  0.1617],\n                        [-0.2736,  0.1742,  0.2239]]],\n\n\n                      [[[-0.0079,  0.1319,  0.1114],\n                        [-0.0743, -0.0305,  0.3261],\n                        [ 0.1474,  0.2353, -0.0412]]],\n\n              ...\n             ('module_list.0.posterior_params.1.param_mus',\n              tensor([ 0.0243,  0.0314,  0.3040, -0.0936, -0.3259,  0.0230, -0.0320,  0.3027,\n                      -0.0815, -0.1082,  0.0233,  0.0705, -0.0668, -0.1233,  0.0172,  0.0904,\n                       0.0125, -0.0182, -0.3289, -0.0261, -0.1676,  0.1954,  0.2275,  0.2331,\n                       0.0743, -0.0876, -0.1143, -0.2752, -0.2665, -0.2323, -0.0370, -0.0340])),\n             ('module_list.0.posterior_params.1.param_std_log',\n              tensor([-7.1025, -5.3064, -4.9560, -4.8693, -5.6517, -5.9971, -5.8863, -6.9338,\n                      -5.7341, -6.2144, -4.6343, -6.0424, -5.1508, -5.0778, -5.0742, -6.0448,\n                      -5.0951, -5.4542, -4.6547, -6.1729, -5.9752, -4.8851, -4.7508, -4.7064,\n                      -4.9404, -5.2045, -5.0169, -5.2111, -5.6868, -8.3506, -6.1773, -5.0066])),\n             ('module_list.0.posterior_params.1.scale_alphas_log',\n              tensor([-2.9490, -3.2520, -3.3946, -3.4951, -3.4896, -3.1572, -2.0109, -3.6435,\n                      -2.2213, -2.4887, -2.2303, -2.1010, -2.7849, -3.4162, -3.8819, -2.5487,\n                      -3.1461, -3.7327, -2.4182, -2.5243, -2.5713, -3.1543, -2.8069, -3.7425,\n                      -3.3890, -3.8608, -3.9808, -2.6061, -2.3058, -3.8742, -2.2982, -2.9821])),\n             ('module_list.0.posterior_params.1.scale_mus',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))])\n</code></pre> <p>Forward is done on the last saved sample. Note that we do not copy the data anywhere, and the model is not encapsulated. Therefore, in order to unlink them, they must be copied</p> <pre><code>print(bayes_model(torch.zeros_like(image)))\n#print(bayes_model(torch.zeros_like(image), sample = False))\nprint(module(torch.zeros_like(image)))\n</code></pre> <pre><code>tensor([[ 0.0305, -0.0716, -0.1202, -0.1129, -0.1449, -0.0620, -0.0262, -0.0984,\n          0.0281, -0.0110]], grad_fn=&lt;AddmmBackward0&gt;)\ntensor([[ 0.0305, -0.0716, -0.1202, -0.1129, -0.1449, -0.0620, -0.0262, -0.0984,\n          0.0281, -0.0110]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>We suggest to run this example on GPU</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n</code></pre> <pre><code>device(type='cuda')\n</code></pre>"},{"location":"variational/examples/main/main/#example-of-training-a-bayesian-model","title":"Example of training a Bayesian model","text":"<p>Next we import several modules for training</p> <pre><code>from src.methods.bayes.variational.trainer import VarBayesTrainer, VarTrainerParams, Beta_Scheduler_Plato, CallbackLossAccuracy #\u0421\u0430\u043c \u0442\u0440\u0435\u043d\u0435\u0440, \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0442\u0440\u0435\u043d\u0435\u0440\u0430, \u041f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a beta(\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u043e\u043e\u044c\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u044b\u0447\u043d\u044b\u043c \u043b\u043e\u0441\u0441\u043e\u043c \u0438 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u043c), \u0438 callback \u0434\u043b\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\nfrom src.methods.report.base import ReportChain #\u042d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u0441\u043f\u0438\u0441\u043e\u043a callback\nfrom src.methods.report.variational import VarBaseReport #\u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c callback \u043f\u0440\u043e\u0441\u0442\u043e \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u043a\u0430\u0436\u0434\u044b\u0439 \u0448\u0430\u0433 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0442 \u0442\u0440\u0435\u043d\u0435\u0440\u0430\n</code></pre> <pre><code>beta = Beta_Scheduler_Plato()\n</code></pre> <pre><code>BATCH_SIZE=1000\nEPOCHS=4000\nLR = 1e-3 #5e-4\n# Split the training set into training and validation sets \nVAL_PERCENT = 0.2 # percentage of the data used for validation \nSAMPLES = 10\nBETA = 0.01 #5e-5 #0.01\nBETA_FAC = 5e-1\nPRUNE = 1.9#1.99, 2.1, 1.9\nPLATO_TOL = 20\n\n\"\"\"\nbase_module = Classifier()\nvar_module = LogUniformVarBayesModule(base_module)\nmodel = VarBayesModuleNet(base_module, nn.ModuleList([var_module]))\n\"\"\"\nbase_module = Classifier()\n#var_module = LogUniformVarLayer(base_module)\nvar_module1 = LogUniformVarLayer(base_module.conv1)\n#bayes_model = VarBayesModuleNet(module, nn.ModuleList([var_module])) #\u041f\u0435\u0440\u0432\u044b\u0439 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442 \u0431\u0430\u0437\u043e\u0432\u0430\u044f \u0441\u0435\u0442\u044c, \u0432\u0442\u043e\u0440\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432 (\u0433\u0434\u0435 \u043d\u0443\u0436\u043d\u044b\u0435 \u0438\u0437 \u043d\u0438\u0445 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u044b\u043c\u0438)\nmodel = VarBayesNet(base_module, {'conv1': var_module1})\n#model = VarBayesNet(base_module, {'': var_module})\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\n#\u041f\u0435\u0440\u0432\u044b\u0439 \u043b\u043e\u0441\u0441 \u044d\u0442\u043e \u043e\u0431\u044b\u0447\u043d\u044b\u0439 \u043b\u043e\u0441\u0441 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435, \u0432\u0442\u043e\u0440\u043e\u0439 \u043b\u043e\u0441\u0441 \u044d\u0442\u043e \u043b\u043e\u0441\u0441 \u0431\u0430\u0439\u0435\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\nfit_loss = nn.CrossEntropyLoss() \nkl_loss = LogUniformVarKLLoss()\n\n#\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0435\u043d\u0442\u0430 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u0435\u0436\u0434\u0443 fit_loss \u0438 kl_loss\nbeta = Beta_Scheduler_Plato(BETA, BETA_FAC, PLATO_TOL)\nbeta_KL = Beta_Scheduler_Plato(beta.beta, 1 / BETA_FAC, PLATO_TOL, ref = beta, threshold=1e-4)\n\n#\u0414\u0430\u043d\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0448\u0430\u0433\u0430 \u0442\u0440\u0435\u043d\u0435\u0440\u0430, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0435\u043d\u043d\u043e \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0448\u0430\u0433 \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a\u0430 \u0438 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0441\u043e\u043e\u0442\u0432\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\ndef post_train_step(trainer: VarTrainerParams, train_result: VarBayesTrainer.TrainResult):\n    beta.step(train_result.fit_loss)\n    beta_KL.step(train_result.dist_loss)\n    trainer.params.beta = float(beta)\n\n#print(model.base_module.state_dict().keys())\nval_size    = int(VAL_PERCENT * len(train_dataset)) \ntrain_size  = len(train_dataset) - val_size \n\nt_dataset, v_dataset = torch.utils.data.random_split(train_dataset,  \n                                                        [train_size,  \n                                                            val_size]) \n\n# Create DataLoaders for the training and validation sets \ntrain_loader = torch.utils.data.DataLoader(t_dataset,  \n                                        batch_size=BATCH_SIZE,  \n                                        shuffle=True, \n                                        pin_memory=True) \neval_loader = torch.utils.data.DataLoader(v_dataset,  \n                                        batch_size=BATCH_SIZE,  \n                                        shuffle=False, \n                                        pin_memory=True) \n\nmodel.to(device) \ntrain_params = VarTrainerParams(EPOCHS, optimizer,fit_loss, kl_loss, SAMPLES, PRUNE, BETA, {'accuracy': CallbackLossAccuracy()})\n#\u0415\u0441\u043b\u0438 \u0445\u043e\u0442\u0438\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0431\u0435\u0442\u0443 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439, \u0442\u043e \u043d\u0443\u043d\u0436\u043e \u0443\u0431\u0440\u0430\u0442\u044c \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442 [post_train_step]\n#trainer = VarBayesTrainer(train_params, ReportChain([VarBaseReport()]), train_loader, eval_loader, [post_train_step])\ntrainer = VarBayesTrainer(train_params, ReportChain([VarBaseReport()]), train_loader, eval_loader)\ntrainer.train(model)\n</code></pre> <pre><code>  0%|          | 1/4000 [00:01&lt;1:41:30,  1.52s/it]\n\nEpoch [1/4000],Loss:25.183677673339844, KL Loss: 2314.0859375. FitLoss: 2.042818784713745,Accuracy:0.4181125,Validation Loss:24.66039276123047,Validation Accuracy:0.673, Prune parameters: 0.0/320,Beta: 0.01\n\n\n  0%|          | 2/4000 [00:02&lt;1:35:21,  1.43s/it]\n\nEpoch [2/4000],Loss:24.183488845825195, KL Loss: 2310.0380859375. FitLoss: 1.0831090211868286,Accuracy:0.7423624999999996,Validation Loss:23.792869567871094,Validation Accuracy:0.787, Prune parameters: 0.0/320,Beta: 0.01\n\n\n  0%|          | 2/4000 [00:03&lt;1:56:51,  1.75s/it]\n</code></pre>"},{"location":"variational/examples/main/main/#evaluate-model-using-trainer","title":"Evaluate model using trainer","text":"<p>Let's look at the quality of the trained model on the validation dataset</p> <pre><code>val_loss = 0.0\nval_acc = 0.0\nPRUNE = 1.0\ntest_loader = torch.utils.data.DataLoader(test_dataset,  \n                                         batch_size=BATCH_SIZE,  \n                                         shuffle=False, \n                                         pin_memory=True) \nkl_loss = LogUniformVarKLLoss()\ntrainer.params.prune_threshold = PRUNE\ntest_result = trainer.eval(model, test_loader)\nacc = test_result.custom_losses['val_accuracy']\nprint(f'Loss:{test_result.val_loss}, KL Loss: {test_result.dist_loss}, FitLoss: {test_result.fit_loss}, Accuracy {acc}, Prune parameters: {test_result.cnt_prune_parameters}/{test_result.cnt_params}')\n</code></pre> <pre><code>Loss:16906.90234375, KL Loss: 1690681.375, FitLoss: 0.09073139727115631, Accuracy 0.98, Prune parameters: 221821.0/421642\n</code></pre>"},{"location":"variational/examples/main/main/#obtaining-a-deterministic-model","title":"Obtaining a deterministic model","text":"<p>Let's lock the model by its built-in method. As a deterministic model we choose the MAP estimator <pre><code>model.to(device=device)\nmodel.prune({'threshold': 1.0})\nmodel.set_map_params()\n</code></pre></p> <p>Let's look at how the model categorizes the numbers</p> <pre><code>image, label = test_dataset[100]\nplt.imshow(image.permute(1, 2, 0), cmap=\"gray\")\nprint(\"Label:\", label)\n</code></pre> <pre><code>Label: 5\n</code></pre> <p></p> <p>It classify it right and deterministic <pre><code>torch.max(model(image.cuda()).data, 1)\n</code></pre></p> <pre><code>torch.return_types.max(\nvalues=tensor([2.1405], device='cuda:0'),\nindices=tensor([5], device='cuda:0'))\n</code></pre>"},{"location":"variational/examples/renyu/renui_example/","title":"Variational Renyu Divergence","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch\nimport torch.nn as nn \nimport torch.optim as optim \nimport torch.nn.functional as F\n</code></pre> <pre><code>class Classifier(nn.Module): \n    def __init__(self, classes: int = 10): \n        super().__init__() \n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) \n        self.pool = nn.MaxPool2d(2, 2) \n        #self.dropout1 = nn.Dropout2d(0.25) \n        #self.dropout2 = nn.Dropout2d(0.5) \n        self.fc1 = nn.Linear(64 * 7 * 7, 128) \n        self.fc2 = nn.Linear(128, classes) \n\n    def forward(self, x): \n        x = self.pool(F.relu(self.conv1(x))) \n        #x = self.dropout1(x) \n        x = self.pool(F.relu(self.conv2(x))) \n        #x = self.dropout2(x) \n        x = x.view(-1, 64 * 7 * 7) \n        x = F.relu(self.fc1(x)) \n        x = self.fc2(x) \n        return x\n</code></pre> <pre><code># imports for model changes\nfrom src.methods.bayes.variational.net import VarBayesNet\nfrom src.methods.bayes.variational.net import NormalVarBayesLayer\nfrom src.methods.bayes.variational.optimization import VarRenuiLoss\n</code></pre> <pre><code># imports for trainer\nfrom src.methods.bayes.variational.trainer import VarBayesTrainer, VarTrainerParams, Beta_Scheduler_Plato, CallbackLossAccuracy\nfrom src.methods.report.base import ReportChain\nfrom src.methods.report.variational import VarBaseReport\n</code></pre> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n</code></pre> <pre><code>device(type='cuda')\n</code></pre> <pre><code>test_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre>"},{"location":"variational/examples/renyu/renui_example/#define-parameters-for-training","title":"Define parameters for training","text":"<pre><code>EPOCHS=5\n\nBATCH_SIZE=1024\nLR = 1e-3 #5e-4\n# Split the training set into training and validation sets \nVAL_PERCENT = 0.2 # percentage of the data used for validation \nSAMPLES = 10\nBETA = 1e-2 #5e-5 #len(train_dataset) *1. / BATCH_SIZE\nBETA_FAC = 5e-1\nPRUNE = -5#1.99, 2.1\nPLATO_TOL = 20\n\n# define a module and make a Bayesian model out of it\nbase_module = Classifier()\nvar_module = NormalVarBayesLayer(base_module)\nmodel = VarBayesNet(base_module, nn.ModuleList([var_module]))\n\n# set an optimizer\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\n# define losses\nfit_loss = nn.CrossEntropyLoss(reduction=\"sum\") \nrenui_loss = VarRenuiLoss()\nbeta = BETA\n\ntrain_params = VarTrainerParams(\n                num_epochs= EPOCHS, \n                optimizer=optimizer, \n                fit_loss=fit_loss, \n                dist_loss= renui_loss,\n                num_samples=SAMPLES,\n                prune_threshold=PRUNE,\n                beta=beta,\n                callback_losses={'accuracy': CallbackLossAccuracy()}\n                )\n</code></pre>"},{"location":"variational/examples/renyu/renui_example/#prepare-dataloader","title":"prepare dataloader","text":"<pre><code>val_size    = int(VAL_PERCENT * len(train_dataset)) \ntrain_size  = len(train_dataset) - val_size \n\nt_dataset, v_dataset = torch.utils.data.random_split(train_dataset,  \n                                                        [train_size,  \n                                                            val_size]) \n\n# Create DataLoaders for the training and validation sets \ntrain_loader = torch.utils.data.DataLoader(t_dataset,  \n                                        batch_size=BATCH_SIZE,  \n                                        shuffle=True, \n                                        pin_memory=True) \n\neval_loader = torch.utils.data.DataLoader(v_dataset,  \n                                        batch_size=BATCH_SIZE,  \n                                        shuffle=False, \n                                        pin_memory=True)\n</code></pre>"},{"location":"variational/examples/renyu/renui_example/#train-model","title":"train model","text":"<pre><code>model.to(device)\n\ntrainer = VarBayesTrainer(train_params, ReportChain([VarBaseReport()]), train_loader, eval_loader)\n\ntrainer.train(model)\n</code></pre> <pre><code>  0%|          | 0/5 [00:00&lt;?, ?it/s]\n\n\nEpoch [1/5],Loss:4226.0458984375, KL Loss: 186235.15625. FitLoss: 2485.269287109375,Accuracy:0.10749605618990384,Validation Loss:4084.62255859375,Validation Accuracy:0.09221311475409837, Prune parameters: 596.0/421642,Beta: 0.01\nEpoch [2/5],Loss:4147.62109375, KL Loss: 186219.140625. FitLoss: 2379.785400390625,Accuracy:0.1213172325721154,Validation Loss:4065.800537109375,Validation Accuracy:0.21413934426229508, Prune parameters: 634.0/421642,Beta: 0.01\nEpoch [3/5],Loss:4097.51220703125, KL Loss: 186340.484375. FitLoss: 2320.719482421875,Accuracy:0.13963153545673082,Validation Loss:4037.357666015625,Validation Accuracy:0.2776639344262295, Prune parameters: 656.0/421642,Beta: 0.01\nEpoch [4/5],Loss:4031.77392578125, KL Loss: 186475.59375. FitLoss: 2275.6953125,Accuracy:0.16670015775240382,Validation Loss:3954.070556640625,Validation Accuracy:0.4354508196721312, Prune parameters: 646.0/421642,Beta: 0.01\nEpoch [5/5],Loss:3908.925537109375, KL Loss: 186752.4375. FitLoss: 2179.834228515625,Accuracy:0.22488356370192303,Validation Loss:3777.31298828125,Validation Accuracy:0.6127049180327869, Prune parameters: 651.0/421642,Beta: 0.01\n\n\n\n\n\n&lt;src.methods.bayes.variational.net_distribution.VarBayesModuleNetDistribution at 0x7fd84c833890&gt;\n</code></pre>"},{"location":"variational/examples/renyu/renui_example/#show-that-trained-model-can-be-pruned-well","title":"Show, that  trained model can be pruned well","text":"<pre><code>thresholds = np.linspace(-5, 5, 10)\nthreshold_results = trainer.eval_thresholds(model, thresholds)\n\n# plot dependence of accuracy on count on non pruned parameter\nx_s = [(a.cnt_prune_parameters / a.cnt_params).cpu().numpy() for a in threshold_results]\nx_s = 1. - np.array(x_s) # now we get partition of non pruned parameters\ny_s = [a.custom_losses[\"val_accuracy\"] for a in threshold_results]\n\nplt.plot(x_s, y_s)\nplt.xlabel(\"Non pruned params, %\")\nplt.ylabel(\"Accuracy, %\")\nplt.title(\"Accuracy on count of nonpruned params\")\nplt.grid()\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}