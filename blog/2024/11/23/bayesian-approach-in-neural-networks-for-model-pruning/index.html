
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.48">
    
    
      
        <title>Bayesian approach in neural networks for model pruning - Bayes Deep Compression</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#bayesian-approach-in-neural-networks-for-model-pruning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Bayes Deep Compression" class="md-header__button md-logo" aria-label="Bayes Deep Compression" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Bayes Deep Compression
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Bayesian approach in neural networks for model pruning
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/intsystems/bayes_deep_compression" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../user_guide/guide/" class="md-tabs__link">
        
  
    
  
  User Guide

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../variational/examples/main/main/" class="md-tabs__link">
          
  
  Examples

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../reference/methods/bayes/base/distribution/" class="md-tabs__link">
          
  
  Reference

        </a>
      </li>
    
  

    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  Blog

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Bayes Deep Compression" class="md-nav__button md-logo" aria-label="Bayes Deep Compression" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Bayes Deep Compression
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/intsystems/bayes_deep_compression" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../user_guide/guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    User Guide
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../variational/examples/main/main/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variational KL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../variational/examples/renyu/renui_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variational Renyu Divergence
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Parameter Distributions
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Parameter Distributions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/base/distribution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Parameter Distributions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/variational/distribution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variance Parameter Distributions
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Bayessian NN
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Bayessian NN
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/base/net/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Bayessian NN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/variational/net/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variance Bayessian NN
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Network Distributions
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            Network Distributions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/base/net_distribution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Network Distributions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/variational/net_distribution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variance Network Distributions
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
        
          
          <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Methods
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            Methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_4_1" id="__nav_4_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4_1">
            <span class="md-nav__icon md-icon"></span>
            Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/trainer/variational/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variational Trainer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_4_2" id="__nav_4_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Pruner
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_4_2">
            <span class="md-nav__icon md-icon"></span>
            Pruner
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/pruner/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Pruner
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
        
          
          <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Bayessian Loss
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_5">
            <span class="md-nav__icon md-icon"></span>
            Bayessian Loss
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/base/loss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Bayessian Loss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../reference/methods/bayes/variational/loss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variance Bayessian Loss
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Blog
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2024/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#intro" class="md-nav__link">
    <span class="md-ellipsis">
      Intro
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#making-neural-network-bayesian" class="md-nav__link">
    <span class="md-ellipsis">
      Making neural network Bayesian
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-bayes-may-be-useful" class="md-nav__link">
    <span class="md-ellipsis">
      Why Bayes may be useful?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variational-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Variational inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variational inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-variational-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Using variational distribution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#choosing-variational-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing variational distribution
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Choosing variational distribution">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reparameterization-trick" class="md-nav__link">
    <span class="md-ellipsis">
      Reparameterization trick
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kl-computation" class="md-nav__link">
    <span class="md-ellipsis">
      KL computation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#posterior-approximation" class="md-nav__link">
    <span class="md-ellipsis">
      Posterior approximation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pruning-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      Pruning strategy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#renyi-divergence" class="md-nav__link">
    <span class="md-ellipsis">
      Renyi-divergence
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Renyi-divergence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#renyi-approximation" class="md-nav__link">
    <span class="md-ellipsis">
      Renyi approximation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kroneker-factorized-laplace" class="md-nav__link">
    <span class="md-ellipsis">
      Kroneker-factorized Laplace
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kroneker-factorized Laplace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pruning-strategy_1" class="md-nav__link">
    <span class="md-ellipsis">
      Pruning strategy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hessian-factorization" class="md-nav__link">
    <span class="md-ellipsis">
      Hessian factorization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://avatars.githubusercontent.com/u/42578877?s=400&u=ff26e03579920d7616ac2ecbe3df72a02384d105&v=4" alt="Kirill Semkin">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          Kirill Semkin
                        
                      </strong>
                      <br>
                      Pharmacist
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2024-11-23 00:00:00+00:00" class="md-ellipsis">November 23, 2024</time>
                      </div>
                    </li>
                    
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              12 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        

  
  


<h1 id="bayesian-approach-in-neural-networks-for-model-pruning">Bayesian approach in neural networks for model pruning</h1>
<p><center>
<img alt="Pruning NN" src="../../../../img/intro_img.png" width="70%" />
</center></p>
<h2 id="intro">Intro</h2>
<p>Using deep learning in solving complex, real-world problems has become quite an engineering routine. But we should never forget about the probabilistic sense inside models and loss minimization. So here, we are going to recall that probabilistic framework and extend it to the <em>Bayesian</em> framework. Such switching will give us pleasant perks but it is not always for free. The main applicition presented in the blog will be neural networks <em>pruning</em> but other directions will be mentioned too.</p>
<p>So we will present 4 Bayesian techniques to envelope any task involving neural networks. If you get interested in the topic and want to use it on practice, you are welcome at our <em>pytoch</em> <a href="https://github.com/intsystems/bayes_deep_compression">library</a>.</p>
<!-- more -->

<h2 id="making-neural-network-bayesian">Making neural network Bayesian</h2>
<p>The usual ML problem setup consists in minimising the loss function <span class="arithmatex">\(L\)</span> between the train targets <span class="arithmatex">\(y\)</span> and parametrical model <span class="arithmatex">\(f_{\mathbf{w}}(\mathbf{x})\)</span>. We assume the model to be some neural network parameterized by <span class="arithmatex">\(\mathbf{w}\)</span>. At the same time, the loss together with the model define data's distribution <span class="arithmatex">\(p(y | \mathbf{x}, \mathbf{w})\)</span>. Minimising the loss w.r.t. <span class="arithmatex">\(\mathbf{w}\)</span> is equivalent to finding maximum likelihood estimation <span class="arithmatex">\(\hat{\mathbf{w}}\)</span> of model's parameters. Finally, to obtain target distribution on the <strong>new</strong> <em>object</em> <span class="arithmatex">\(\hat{\mathbf{x}}\)</span> we just use learnt estimation and have <span class="arithmatex">\(p(y | \hat{\mathbf{x}}, \hat{\mathbf{w}})\)</span>.</p>
<p>The Bayesian approach complements the model with a <em>prior</em> distribution <span class="arithmatex">\(p(\mathbf{w} | \Theta)\)</span> . It is generally parameterized by <em>hyperparameters</em> <span class="arithmatex">\(\Theta\)</span> (but prior can be a fixed distribution as well). This move kind of changes the game because now target distribution for a new object <span class="arithmatex">\(\hat{\mathbf{x}}\)</span> is trickier</p>
<div class="arithmatex">\[
   p(y | \hat{\mathbf{x}}, \Theta) = \int p(\mathbf{w} | y, \mathbf{x}, \Theta) p(y | \hat{\mathbf{x}}, \mathbf{w}) d\mathbf{w} = \mathbb{E}_{\mathbf{w} \sim p(\mathbf{w} | y, \mathbf{x}, \Theta)} [p(y | \hat{\mathbf{x}}, \mathbf{w})] , \label{new_point}\tag{1}
\]</div>
<p>where <span class="arithmatex">\(p(\mathbf{w} | y, \mathbf{x}, \Theta)\)</span> is a <em>posterior</em> distribution of the model's parameters based on the train data. Unfortunately, finding the posterior is typically intractable in case of the NNs. That leads to the intractability of the prediction. Another problem here is how to choose optimal hyperparameters <span class="arithmatex">\(\Theta\)</span> if we don't know them from some prior (expert) knowledge.</p>
<h2 id="why-bayes-may-be-useful">Why Bayes may be useful?</h2>
<p>In spite of the mentioned difficulties, Bayesian framework has a lot to give. You've might already heard that <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)"><em>L2 regularization</em></a> is equal to simple gaussian prior. Furthermore, nets <em>pruning</em> is possible using the very same gaussian or more sparsity-inducing priors (e.g. <a href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace prior</a>).</p>
<p>Some other bayes features:</p>
<ul>
<li>The formula (<span class="arithmatex">\(\ref{new_point}\)</span>) implies <a href="https://en.wikipedia.org/wiki/Ensemble_learning"><em>assembling</em></a> your model to evaluate prediction for a new data point. To see it, you may evaluate expectation with Monte_Carlo samples drawn from posterior. Major benefit here is using more information about true model state in the final prediction. It also prevents models from being <a href="https://docs.giskard.ai/en/latest/knowledge/key_vulnerabilities/overconfidence/index.html"><em>over-confident</em></a>, see illustrative picture under.</li>
</ul>
<figure>
<p><img alt="Confidence shift" src="../../../../img/confidence.jpg" width="70%" /></p>
<figcaption>
<p><em>Confidence shift in distribution after incorporating prior knowledge</em></p>
</figcaption>
</figure>
<ul>
<li>Bayes can be used to perform <a href="https://en.wikipedia.org/wiki/Model_selection"><em>model selection</em></a>, for example see <a href="https://en.wikipedia.org/wiki/Latent_space"><em>hidden state models</em></a> and learning <a href="https://scikit-learn.org/1.5/modules/mixture.html#variational-Bayesian-gaussian-mixture"><em>mixture of gaussians</em></a>.</li>
</ul>
<p>We have provided only a handful of applications, but this might be enough to spark your interest. Now, we will go through methods that will overcome problems with bayessian framework and make it usable.</p>
<h2 id="variational-inference">Variational inference</h2>
<p>One of the ways to add a Bayesian layer of inference to your neural network - <em>variational inference</em> principle. It is built upon special function called <a href="https://en.wikipedia.org/wiki/Bayesian_inference#Formal_explanation"><em>evidence</em></a> which is</p>
<div class="arithmatex">\[
    p(y | \mathbf{x}, \Theta) = \mathbb{E}_{p(\mathbf{w} | y, \mathbf{x}, \Theta)} [p(y | \mathbf{x}, \mathbf{w})].
\]</div>
<p>It's similar to (<span class="arithmatex">\(\ref{new_point}\)</span>) but it has training object <span class="arithmatex">\(\mathbf{x}\)</span> in the left. The function basically indicates how probable the given data is under varying hyperparameters. Maximising this function is a key to finding optimal <span class="arithmatex">\(\Theta\)</span>. But it can also help with the posterior! Following the <a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">ELBO</a> technic, introduce <em>variational</em> distribution <span class="arithmatex">\(q(\mathbf{w} | \phi)\)</span> which is parameterized by <span class="arithmatex">\(\phi\)</span>. This distribution is supposed to approximate the posterior <span class="arithmatex">\(p(\mathbf{w} | y, \mathbf{x}, \Theta)\)</span> (see picture under) and in theory can be anything we want. Then, it can be shown that the following expression is <em>evidence lower bound</em></p>
<div class="arithmatex">\[
   \text{ELBO}(\Theta, \phi) = \mathbb{E}_{\mathbf{w} \sim q(\mathbf{w} | \phi)} [p(y | \mathbf{x}, \mathbf{w})] + \text{KL}(q(\mathbf{w} | \phi) || p(\mathbf{w} | \Theta))
\]</div>
<p>or in terms of the loss and the model</p>
<div class="arithmatex">\[
   \text{ELBO}(\Theta, \phi) = \mathbb{E}_{\mathbf{w} \sim q(\mathbf{w} | \phi)} [L(y, f_{\mathbf{w}}(\mathbf{x}))] + \text{KL}(q(\mathbf{w} | \phi) || p(\mathbf{w} | \Theta))
\]</div>
<p>where <span class="arithmatex">\(\text{KL}(\cdot || \cdot)\)</span> is a <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-divergence</a>. Maximising it by <span class="arithmatex">\(\phi\)</span> and <span class="arithmatex">\(\Theta\)</span> gives us estimation of the optimal hyperparameters and the variational distribution.</p>
<figure>
<p><img alt="Variational distribution" src="../../../../img/var_distr.webp" width="70%" /></p>
<figcaption>
<p><em>ELBO optimization is equal to posterior fitting</em></p>
</figcaption>
</figure>
<h3 id="using-variational-distribution">Using variational distribution</h3>
<p>If you are not interested in hyperparameters, you can just use trained <span class="arithmatex">\(q(\mathbf{w} | \phi^*)\)</span> to get the desired prediction. The (<span class="arithmatex">\(\ref{new_point}\)</span>) can be estimated as</p>
<div class="arithmatex">\[
   p(y | \hat{\mathbf{x}}, \Theta) \approx \mathbb{E}_{\mathbf{w} \sim q(\mathbf{w} | \phi^*)} [p(y | \hat{\mathbf{x}}, \mathbf{w})].
\]</div>
<p>It is usually approximated futher with Monte-Carlo. On the other hand, you can find the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">MAP</a> estimation: <span class="arithmatex">\(\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmax }} q(\mathbf{w} | \phi^*)\)</span>, and simplify it to</p>
<div class="arithmatex">\[
   p(y | \hat{\mathbf{x}}, \Theta) \approx p(y | \hat{\mathbf{x}}, \mathbf{w}^*).
\]</div>
<p>This expression is equal to just using your model <span class="arithmatex">\(f_{\mathbf{w}}(\mathbf{x})\)</span> with <span class="arithmatex">\(\mathbf{w} = \mathbf{w}^*\)</span> for prediction.</p>
<h3 id="choosing-variational-distribution">Choosing variational distribution</h3>
<p>As it was mentioned, there are no limits on <span class="arithmatex">\(q(\mathbf{w} | \phi)\)</span> except to be computable. But to make the whole thing practical and use <em>gradient optimization</em> it should comply with several requirements.</p>
<h4 id="reparameterization-trick"><a href="https://en.wikipedia.org/wiki/Reparameterization_trick">Reparameterization trick</a></h4>
<p>In order to estimate and compute the gradient of the expectation in <span class="arithmatex">\(\text{ELBO}(\Theta, \phi)\)</span> the <span class="arithmatex">\(q\)</span> must "separate" the randomness from <span class="arithmatex">\(\mathbf{w}\)</span>. Namely, introduce some deterministic function <span class="arithmatex">\(h\)</span> parameterized by <span class="arithmatex">\(\phi\)</span> and some random variable <span class="arithmatex">\(\epsilon\)</span>, usually with simple distribution <span class="arithmatex">\(p(\epsilon)\)</span> from which we can sample. Now, randomness of <span class="arithmatex">\(\mathbf{w}\)</span> is expressed through the randomness of <span class="arithmatex">\(\epsilon\)</span></p>
<div class="arithmatex">\[
   \mathbf{w} \sim q(\mathbf{w} | \phi) \Leftrightarrow \mathbf{w} = h(\mathbf{w}, \phi, \epsilon), \ \epsilon \sim p(\epsilon).
\]</div>
<p>This trick enables us to estimate the expectation in ELBO and compute its gradients:</p>
<div class="arithmatex">\[\begin{align}
   \mathbb{E}_{\mathbf{w} \sim q(\mathbf{w} | \phi)} [p(y | \mathbf{x}, \mathbf{w})] &amp;\approx \frac{1}{K} \sum_{i = 1}^K p(y | \mathbf{x}, \mathbf{w}_i) = \frac{1}{K} \sum_{i = 1}^K L(y, f_{\mathbf{w}_i}(\mathbf{x})), \\
   \nabla \mathbb{E}_{\mathbf{w} \sim q(\mathbf{w} | \phi)} [p(y | \mathbf{x}, \mathbf{w})] &amp;\approx \frac{1}{K} \sum_{i = 1}^K \nabla L(y, f_{\mathbf{w}_i}(\mathbf{x})).
\end{align}\]</div>
<p>where <span class="arithmatex">\(\mathbf{w}_i = h(\mathbf{w}, \phi, \epsilon_i)\)</span>, <span class="arithmatex">\(\epsilon_i \sim p(\epsilon)\)</span> and <span class="arithmatex">\(K\)</span> is the number of samples.</p>
<h4 id="kl-computation">KL computation</h4>
<p>To optimise <span class="arithmatex">\(\text{ELBO}(\Theta, \phi)\)</span> it is necessary to compute KL term between <span class="arithmatex">\(q(\mathbf{w} | \phi)\)</span> and <span class="arithmatex">\(p(\mathbf{w} | \Theta)\)</span>  plus its gradients. As the choice of distributions is arbitrary and task-dependent, we do not discuss it further. One general solution here can be Monte-Carlo estimation:</p>
<div class="arithmatex">\[
   \text{KL}(q(\mathbf{w} | \phi) || p(\mathbf{w} | \Theta)) \approx \frac{1}{M} \sum_{i = 1}^M \log \frac{q(\mathbf{w}_i | \phi)}{p(\mathbf{w}_i | \Theta)},
\]</div>
<p>where <span class="arithmatex">\(\mathbf{w}_i \sim q(\mathbf{w} | \phi)\)</span>. Several particular solutions will be given futher.</p>
<h4 id="posterior-approximation">Posterior approximation</h4>
<p>Ideally, the class of variational distributions parametrized by <span class="arithmatex">\(\phi\)</span> should contain <span class="arithmatex">\(p(\mathbf{w} | y, \mathbf{x}, \Theta)\)</span>. If <span class="arithmatex">\(q\)</span> is exactly posterior then the ELBO will be exactly the evidence (not just lower bound)!</p>
<p>Practically, we don't know the exact posterior but we know it up to the normalisation. It is followed from thes Bayesian theorem:</p>
<div class="arithmatex">\[
   p(\mathbf{w} | y, \mathbf{x}, \Theta) \propto p(y | \mathbf{w}, \mathbf{x}) p(\mathbf{w} | \Theta).
\]</div>
<p>This can be a hint for choosing variational distributions class. For example, if you know that <span class="arithmatex">\(p(\mathbf{w} | y, \mathbf{x}, \Theta)\)</span> have some special properties, make sure that functions from <span class="arithmatex">\(q\)</span> class have them too (for example, multimodality, see picture under).</p>
<figure>
<p><img alt="Variational distribtion problem" src="../../../../img/var_dist_problems.png" width="80%" /></p>
<figcaption>
<p><em>Unimodal variational distribution can't fit multimodal posterior</em></p>
</figcaption>
</figure>
<h3 id="pruning-strategy">Pruning strategy</h3>
<p>Finally, let's learn variational pruning technic. Generally, it is based on the probability mass of the distribution <span class="arithmatex">\(q\)</span> in the <span class="arithmatex">\(w_i = 0\)</span> points. Here <span class="arithmatex">\(w_i\)</span> represents each individual weight of the model. If probability of this point is high, the weight is considered to be zero.</p>
<p>For example, if <span class="arithmatex">\(q\)</span> is a factorized gaussian (<a href="https://papers.nips.cc/paper_files/paper/2011/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html">Graves</a>, 2011) meaning <span class="arithmatex">\(w_i \sim \mathcal{N}(w_i | \mu_i, \sigma_i^2)\)</span>, then <span class="arithmatex">\(\log q(w_i = 0) \propto -\dfrac{\mu_i^2}{2 \sigma_i^2}\)</span>. Set upper threshold on this value and we obtain pruning rule for individual weights:</p>
<div class="arithmatex">\[
   \left| \dfrac{\mu_i}{\sigma_i} \right| &lt; \lambda \Rightarrow \text{prune}.
\]</div>
<p>Similar rules can be derived for more tricky distributions like <em>log-uniform</em> or <em>half-Cauchy</em> (<a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf">Christos Louizos et. al.</a>, 2017). The main practical issue here is whatever <span class="arithmatex">\(q\)</span> you choose it should <strong>factorize</strong> weights into small groups. Otherwise it would be impossible to compute marginal distributions on individual weights in reasonable time.</p>
<h2 id="renyi-divergence">Renyi-divergence</h2>
<p>Another Bayesian approach (<a href="https://arxiv.org/abs/1602.02311">Yingzhen Li et. al.</a>, 2016) is actually an extension of the variational inference. The main idea is to substitute KL divergence with the <a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"><em>Renyi</em> divergence</a></p>
<div class="arithmatex">\[
   D_{\alpha} [p || q] = \frac{1}{\alpha - 1} \log \int p(\mathbf{w})^{\alpha} q(\mathbf{w})^{1 - \alpha} d \mathbf{w}.
\]</div>
<p>Formally, <span class="arithmatex">\(\alpha \ge 0\)</span>. But it is possible to set <span class="arithmatex">\(\alpha &lt; 0\)</span> although it won't be <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">divergence</a> mathematically.</p>
<p>Let's denote the new ELBO functional as <span class="arithmatex">\(\text{ELBO}_{\alpha}\)</span>. Also denote by <span class="arithmatex">\(\alpha_+\)</span> any <span class="arithmatex">\(\alpha \in [0, 1]\)</span> and by <span class="arithmatex">\(\alpha_-\)</span> any <span class="arithmatex">\(\alpha &lt; 0\)</span>. The great feature of the Reniu functional is flexibility. It is possible to show that varying <span class="arithmatex">\(\alpha\)</span> we can obtain an old <span class="arithmatex">\(\text{ELBO} = \underset{\alpha \to 1}{\lim} \text{ELBO}_{\alpha}\)</span> or even true evidence <span class="arithmatex">\(p(y | x) = \underset{\alpha \to 0}{\lim} \text{ELBO}_{\alpha}\)</span>. Generally, we have the following relations</p>
<div class="arithmatex">\[
   \text{ELBO} = \underset{\alpha \to 1}{\lim} \text{ELBO}_{\alpha} \le \text{ELBO}_{\alpha_+} \le p(y | x) = \underset{\alpha \to 0}{\lim} \text{ELBO}_{\alpha} \le \text{ELBO}_{\alpha_-}.
\]</div>
<p>Visual example below demonstrates evolution of fitted variational distribution depending on <span class="arithmatex">\(\alpha\)</span>. The task is <a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression">Bayesian linear regression</a>. <strong>Black</strong> line corresponds to the exact posterior. It is multivariate gaussian with covariant components. Chosen variational distribution family is factorized gaussians.</p>
<p>We go from the <font color="cyan"> cyan circle </font> corresponding to <span class="arithmatex">\(\alpha \to \infty\)</span>. It can be referred to as <em>mode-seeking</em> ELBO regime. This gaussian lies exactly inside true gaussian and is perfectly centred around the mode. Then, decreasing <span class="arithmatex">\(\alpha\)</span> to 1 we obtain <font color="violet"> violet circle </font> corresponding to conventional ELBO. After that, we get more inflated <font color="blue"> blue circle </font> with <span class="arithmatex">\(\alpha = 0.5\)</span> and <em>optimal</em> <font color="green"> green circle </font> with <span class="arithmatex">\(\alpha = 0\)</span>. In this case, variational distribution is true marginal distribution of the parameters</p>
<div class="arithmatex">\[
   q (\mathbf{w}) = p(w_1 | y, x) \cdot p(w_2 | y, x)
\]</div>
<p>and delivers true evidence <span class="arithmatex">\(\text{ELBO}_{0} = \text{ELBO}\)</span>.</p>
<p>If <span class="arithmatex">\(\alpha\)</span> is decreased further, we switch to the so called <em>mass-covering</em> regime. The  <font color="red"> red circle </font> corresponds to <span class="arithmatex">\(\alpha \to -\infty\)</span>. As we can see it hasn't inflated over the whole plane but has become a perfect encapsulation of the true posterior. To conclude, fitting models for different <span class="arithmatex">\(\alpha\)</span> is a great way to control distribution of probability mass around the true posterior.</p>
<figure>
<p><img align="center" alt="Approximate posterior" src="../../../../img/renui_posterior.png" /></p>
<figcaption>
<p><em>Evolution of variational distributions for different <span class="arithmatex">\(\alpha\)</span></em></p>
</figcaption>
</figure>
<h3 id="renyi-approximation">Renyi approximation</h3>
<p>Let's rewrite <span class="arithmatex">\(\text{ELBO}_{\alpha}\)</span> in another way</p>
<div class="arithmatex">\[
   \text{ELBO}_{\alpha} = \frac{1}{1 - \alpha} \log \mathbb{E}_{\mathbf{w} \sim q} [\left( \frac{p(y, \mathbf{w} | x)}{q(\mathbf{w})} \right)^{1 - \alpha}]
\]</div>
<p>Unfortunately, this loss is equally intractable for deep learning models as ELBO in variational inference. But key walkarounds towards feasibility stay the same: the reparametrization trick and monte-carlo estimation. Imagine we have <span class="arithmatex">\(\mathbf{w} = h(\mathbf{w}, \phi, \epsilon), \ \epsilon \sim p(\epsilon)\)</span> and <span class="arithmatex">\(K\)</span> samples from this distribution, then estimated loss is</p>
<div class="arithmatex">\[\begin{multline}
   \text{ELBO}_{\alpha} = \frac{1}{1 - \alpha} \log \mathbb{E}_{\epsilon} [\left( \frac{p(y, \mathbf{w} | x)}{q(\mathbf{w})} \right)^{1 - \alpha}] \approx \\ \approx \frac{1}{1 - \alpha} \log \frac{1}{K} \sum_{i = 1}^K [\left( \frac{p(y, \mathbf{w}_i | x)}{q(\mathbf{w}_i)} \right)^{1 - \alpha}] =: \widehat{\text{ELBO}}_{\alpha}.
\end{multline}\]</div>
<p>As <span class="arithmatex">\(p(\epsilon)\)</span> is usually standard normal it is easy to take gradient over this estimation.</p>
<p>The only problem here is that the new loss is actually <em>biased</em>. But as authors of the approach proved this bias <em>monotonously</em> vanishes with more samples. Moreover, with fixed <span class="arithmatex">\(K\)</span> you can play around with <span class="arithmatex">\(\alpha\)</span> as <span class="arithmatex">\(\widehat{\text{ELBO}}_{\alpha}\)</span> is <em>non-decreasing</em> on <span class="arithmatex">\(\alpha\)</span>. Combining earlier facts we can obtain that either <span class="arithmatex">\(\widehat{\text{ELBO}}_{\alpha} &lt; p(y|x)\)</span> for all <span class="arithmatex">\(\alpha\)</span> or their exists <strong>optimal</strong> <span class="arithmatex">\(\alpha_K\)</span> such that <span class="arithmatex">\(\widehat{\text{ELBO}}_{\alpha_K} = p(y|x)\)</span>!</p>
<p>The illustration below shows relations between <span class="arithmatex">\(\text{ELBO}_{\alpha}\)</span> for diffrent <span class="arithmatex">\(\alpha\)</span>/fixed <span class="arithmatex">\(K\)</span> and vice versa. It is denoted here as <span class="arithmatex">\(\mathcal{L}_{\alpha}\)</span>. True evidence is denoted as <span class="arithmatex">\(\log p(x)\)</span>. Conventional elbo here is <span class="arithmatex">\(\mathcal{L}_{VI}\)</span>.</p>
<figure>
<p><img alt="Renyi behavior" src="../../../../img/renui_sampled.png" width="60%" /></p>
<figcaption>
<p><em>Illustration of how Renyi loss is changed under different variations</em>.</p>
</figcaption>
</figure>
<p>So, regarding discussed guarantees the approach is possible to be realized on practise.</p>
<h2 id="kroneker-factorized-laplace">Kroneker-factorized Laplace</h2>
<p>The key advantage of the last Bayesian approach (<a href="https://discovery.ucl.ac.uk/id/eprint/10080902/1/kflaplace.pdf">Hippolyt Ritter et. al.</a>, 2018) is that it is applicable to <strong>trained</strong> NNs with definite <em>layer structure</em>. We assume each layer to be linear transform followed by activation function. Imagine we have no prior for now, only likelihood <span class="arithmatex">\(\log p(y | \mathbf{w}, \mathbf{x})\)</span>. We can use second order approximation around likelihood maximum denoted by <span class="arithmatex">\(\mathbf{w}^*\)</span></p>
<div class="arithmatex">\[
   \log p(y | \mathbf{w}, \mathbf{x}) \approx \log p(y | \mathbf{w}^*, \mathbf{x}) + (\mathbf{w} - \mathbf{w}^*)^{\text{T}} H (\mathbf{w} - \mathbf{w}^*).
\]</div>
<p>Here <span class="arithmatex">\(H\)</span> is a hessian of the likelihood in the maximum point that we don't know. However, we do know <span class="arithmatex">\(\mathbf{w}^*\)</span> (it is our trained model). The approximation gives us normal distribution for net's parameters</p>
<div class="arithmatex">\[
    \mathbf{w} \sim \mathcal{N}(\mathbf{w}^*, H^{-1}).
\]</div>
<p>Actually, the result would be the same even if we had some fixed prior in the beginning. In this case, the likelihood function is substituted for posterior <span class="arithmatex">\(p(\mathbf{w} | y, \mathbf{x})\)</span>. Now <span class="arithmatex">\(\mathbf{w}^*\)</span> is the MAP model, <span class="arithmatex">\(H\)</span> is the hessian of the posterior. Simple example of this transition is incorporation of the L2-regularization.</p>
<p>It must be said that laplace approximation is not a panacea. The picture below illustrates how far from reality it can be. But for now we assume that it's applicable.</p>
<figure>
<p><img alt="Laplace approximation example" src="../../../../img/laplace_modes.png" width="70%" /></p>
<figcaption>
<p><font color="red">Laplace approximation</font> of an <font color="blue">arbitary distribution</font>.</p>
<p>As you can see, it is not always quite accurate.</p>
</figcaption>
</figure>
<h3 id="pruning-strategy_1">Pruning strategy</h3>
<p>If we knew <span class="arithmatex">\(H\)</span> and <span class="arithmatex">\(\mathbf{w}^*\)</span>, pruning could be based on the probability mass in <span class="arithmatex">\(w_i = 0\)</span>.So it is similar to the pruning in the <a href="#variational-inference">variational approach</a>. The major concern here is again factorization into parameters groups, namely by NN's layers. The authors of the approach showed that if we assume layer's independence, the hessian will factorize into a block-diagonal matrix. Therefore we will have independent gaussians for each layer! Computing marginals <span class="arithmatex">\(q(w_i)\)</span> will be absolutely feasible.</p>
<h3 id="hessian-factorization">Hessian factorization</h3>
<p>Let's see what is the hessian's structure on each layer. Denote hessian of the layer <span class="arithmatex">\(\lambda\)</span> as <span class="arithmatex">\(H_{\lambda}\)</span>, then it can be shown that</p>
<div class="arithmatex">\[
   \mathbb{E}[H_{\lambda}] = \mathbb{E}[\mathcal{Q}_{\lambda}] \otimes \mathbb{E}[\mathcal{H}_{\lambda}],
\]</div>
<p>where <span class="arithmatex">\(\mathcal{Q}_{\lambda} = a_{\lambda-1}^{\text{T}} a_{\lambda-1}\)</span> is a covariance of the incoming <em>activations</em> <span class="arithmatex">\(a_{\lambda-1}\)</span> and <span class="arithmatex">\(\mathcal{H}_{\lambda} = \dfrac{\partial^2 L}{\partial h_{\lambda} \partial h_{\lambda}}\)</span> is the hessian of the loss w.r.t. linear <em>pre-activations</em> <span class="arithmatex">\(h_{\lambda}\)</span>.</p>
<p>Expectations here can be estimated by Monte-Carlo. The <span class="arithmatex">\(\mathcal{H}_{\lambda}\)</span> is actually quite heavy to compute but can be estimated using <a href="https://arxiv.org/abs/1706.03662">KFRA</a> or <a href="https://arxiv.org/abs/1503.05671">KFAC</a> algorithms (in terms of the implementation, these are most cumbersome).</p>
<p>Ultimately, the distribution on layer weights is <a href="https://en.wikipedia.org/wiki/Matrix_normal_distribution">matrix normal</a></p>
<div class="arithmatex">\[
   \mathbf{w}_{\lambda} \sim \mathcal{MN}(\mathbf{w}^*_{\lambda}, \mathcal{Q}_{\lambda}^{-1}, \mathcal{H}_{\lambda}).
\]</div>
<h2 id="conclusion">Conclusion</h2>
<p>We have enough mathematics for now <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f60c.svg" title=":relieved:" />. I hope you have enjoyed the concept of Bayesian inference and now understand how it can be useful in applications. Some practical python libraries are <a href="https://github.com/JavierAntoran/Bayesian-Neural-Networks?tab=readme-ov-file#stochastic-gradient-hamiltonian-monte-carlo">Bayesian Neural Networks</a>, <a href="https://pyro.ai/">pyro</a> and our developing library <a href="https://github.com/intsystems/bayes_deep_compression">bayescomp</a>. You can find many examples of using bayes approach in their docs.</p>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tabs", "content.code.copy"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>